<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A Comparison of Least Square, L2-regularization and L1-regularization | Ivan Xiao</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="A Comparison of Least Square, L2-regularization and L1-regularization" />
<meta name="author" content="Ivan Xiao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Engineer. Father. Cat Lover. Guitarist. Ballroom Dancer. Woodworker." />
<meta property="og:description" content="Engineer. Father. Cat Lover. Guitarist. Ballroom Dancer. Woodworker." />
<link rel="canonical" href="https://www.ivansiu.com/2014/05/15/a-comparison-of-least-square-l2-regularization-and-l1-regularization" />
<meta property="og:url" content="https://www.ivansiu.com/2014/05/15/a-comparison-of-least-square-l2-regularization-and-l1-regularization" />
<meta property="og:site_name" content="Ivan Xiao" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2014-05-15T02:32:38+00:00" />
<script type="application/ld+json">
{"url":"https://www.ivansiu.com/2014/05/15/a-comparison-of-least-square-l2-regularization-and-l1-regularization","@type":"BlogPosting","headline":"A Comparison of Least Square, L2-regularization and L1-regularization","dateModified":"2014-05-15T02:32:38+00:00","datePublished":"2014-05-15T02:32:38+00:00","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://www.ivansiu.com/images/xiao.png"},"name":"Ivan Xiao"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.ivansiu.com/2014/05/15/a-comparison-of-least-square-l2-regularization-and-l1-regularization"},"author":{"@type":"Person","name":"Ivan Xiao"},"description":"Engineer. Father. Cat Lover. Guitarist. Ballroom Dancer. Woodworker.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/skins/dark.css">
  <link rel="alternate" type="application/atom+xml" title="Ivan Xiao" href="/feed.xml">
<!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

</head>


  <body class="layout--post  a-comparison-of-least-square-l2-regularization-and-l1-regularization">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/">Home</a></li><li><a href="/archive/">Archive</a></li><li><a href="/categories/">Categories</a></li><li><a href="/tags/">Tags</a></li><li><a href="/search/">Search</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
      <a href="/" class="site-logo" rel="home" title="Ivan Xiao">
        <img src="/images/xiao.png" class="site-logo-img animated fadeInDown" alt="Ivan Xiao">
      </a>
    
    
    
      
        <div class="site-title animated fadeIn"><a href="/">Ivan Xiao</a></div>
      
      <p class="site-description animated fadeIn" itemprop="description">Engineer. Father. Cat Lover. Guitarist. Ballroom Dancer. Woodworker.</p>
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    

    <div class="page-wrapper">
      <header class="page-header">
        
        
          <h1 id="page-title" class="page-title p-name">A Comparison of Least Square, L2-regularization and L1-regularization
</h1>
        
      </header>

      <div class="page-sidebar">
        <div class="page-author h-card p-author"><img src="/images/ivan-memoji.jpg" class="author-avatar u-photo" alt="Ivan Xiao"><div class="author-info"><div class="author-name">
        <span class="p-name">Ivan Xiao</span>
      </div><ul class="author-links"><li class="author-link">
            <a class="u-url" rel="me" href="https://about.me/iveney"><i class="fas fa-external-link-square-alt fa-lg" title="About"></i></a>
          </li><li class="author-link">
            <a class="u-url" rel="me" href="https://twitter.com/ivanzxiao"><i class="fab fa-twitter-square fa-lg" title="Twitter"></i></a>
          </li><li class="author-link">
            <a class="u-url" rel="me" href="https://github.com/iveney"><i class="fab fa-github-square fa-lg" title="GitHub"></i></a>
          </li></ul>

<span class="read-time">3 min read</span>

    <time class="page-date dt-published" datetime="2014-05-15T02:32:38+00:00"><a class="u-url" href="">May 15, 2014</a>
</time>

  </div>
</div>

        
  <h3 class="page-taxonomies-title">Categories</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy"><a class="p-category" href="/categories/#machine-learning" title="Pages filed under Machine Learning">Machine Learning</a></li>
  </ul>


        

      </div>

      <div class="page-content">
        <div class="e-content">
          <p><img src="https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/2014-05-14-a-comparison-of-least-square-l2-regularization-and-l1-regularization/coef.png" alt="Recovered Coefficients by Different Methods" /></p>

<h1 id="problem-setting">Problem Setting</h1>

<p>Ordinary Least Square (OLS), L2-regularization and L1-regularization are
all techniques of finding solutions in a linear system. However,
they serve for different purposes. Recently, L1-regularization gains
much attention due to its ability in finding sparse solutions.
This post demonstrates this by comparing OLS, L2 and L1 regularization.</p>

<!-- more -->

<p>Consider the following linear system:</p>

\[Ax = y\]

<p>where \(A \in \reals^{m \times n}\), \(m\) is the number of rows (observations) and
\(n\) is the number of columns (variable dimension), \(x\) is the variable
coefficients and \(y\) is the response. There are three cases to consider:</p>

<ol>
  <li>\(m=n\). This is the common-seen case. If \(A\) is not degenerate, the solution
is unique.</li>
  <li>\(m&gt;n\). This is called <em>over-determined linear system</em>. There is usually <em>no</em>
solutions, but an <em>approximation</em> can be easily found by minimizing
the <em>residue cost</em> \(\norm{Ax-y}^2_2\) using least square methods, and it has
a nice closed-form solution \(x_{ls}=(A^T A)^{-1} A^T y\). In L2-regularization,
we add a penalize term to minimize the 2-norm of the coefficients. Thus, the
objective becomes:
\(\min_x \norm{Ax-y}^2_2 + \alpha \norm{x}_2\)
where \(\alpha\) is a weight to decide the importance of the regularization.</li>
  <li>\(m&lt;n\). This is called <em>under-determined linear system</em>. There is usually
no solution or infinite solutions. This is where it get interesting:
when we have some prior knowledge in the solution structure, such as
sparsity, we can have a ‘metric’ to find a better solution among a whole bunch.
The objective is thus:
\(\min_x \norm{Ax-y}^2_2 + \alpha \norm{x}_1\)
The optimization technique for the above problem is called <a href="http://www-stat.stanford.edu/~tibs/lasso.html">lasso</a>, and there
is an advanced version called <a href="http://en.wikipedia.org/wiki/Elastic_net_regularization">elastic net</a>, which <a href="http://scikit-learn.org/stable/modules/linear_model.html#elastic-net">combines the L2 and L1
regularization together</a>, hoping to get the advantages of both: L1 regularization
finds sparse solution but introduces a large Mean Square Error (MSE) error,
while L2 is better at minimizing MSE.</li>
</ol>

<h2 id="an-example">An Example</h2>

<p>In the following, we show their performances by solving a simple case.</p>

<figure class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="c1">% regression_ex.m</span>
<span class="c1">% Compare Ordinary Least square (no regularization), L2-reguarlized (Ridge),</span>
<span class="c1">% L1-regualarized (Lasso) regression in finding the sparse coefficient</span>
<span class="c1">% in a underdetermined linear system</span>

<span class="nb">rng</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>  <span class="c1">% for reproducibility</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">50</span><span class="p">;</span>  <span class="c1">% num samples</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200</span><span class="p">;</span> <span class="c1">% num variables, note that n &gt; m</span>

<span class="n">A</span> <span class="o">=</span> <span class="nb">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
<span class="n">x</span> <span class="o">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">nz</span> <span class="o">=</span> <span class="mi">10</span><span class="p">;</span> <span class="c1">% 10 non-zeros variables (sparse)</span>
<span class="n">nz_idx</span> <span class="o">=</span> <span class="nb">randperm</span><span class="p">(</span><span class="n">n</span><span class="p">);</span>
<span class="n">x</span><span class="p">(</span><span class="n">nz_idx</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">nz</span><span class="p">))</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="nb">rand</span><span class="p">(</span><span class="n">nz</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">A</span><span class="o">*</span><span class="n">x</span><span class="p">;</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="nb">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">% add some noise</span>

<span class="c1">% plot original x</span>
<span class="nb">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="nb">bar</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">axis</span> <span class="n">tight</span><span class="p">;</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'Original coefficients'</span><span class="p">);</span>

<span class="c1">% OLS</span>
<span class="n">x_ols</span> <span class="o">=</span> <span class="n">A</span> <span class="p">\</span> <span class="n">y</span><span class="p">;</span>
<span class="nb">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
<span class="nb">bar</span><span class="p">(</span><span class="n">x_ols</span><span class="p">),</span> <span class="nb">axis</span> <span class="n">tight</span><span class="p">;</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'Ordinary Least Square'</span><span class="p">);</span>
<span class="n">y_ols</span> <span class="o">=</span> <span class="n">A</span> <span class="o">*</span> <span class="n">x_ols</span><span class="p">;</span>

<span class="c1">% L2 (Ridge)</span>
<span class="n">x_l2</span> <span class="o">=</span> <span class="n">ridge</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>  <span class="c1">% last parameter = 00 to generate intercept term</span>
<span class="n">b_l2</span> <span class="o">=</span> <span class="n">x_l2</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="n">x_l2</span> <span class="o">=</span> <span class="n">x_l2</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">);</span>
<span class="nb">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">);</span>
<span class="nb">bar</span><span class="p">(</span><span class="n">x_l2</span><span class="p">),</span> <span class="nb">axis</span> <span class="n">tight</span><span class="p">;</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'L2 Regularization'</span><span class="p">);</span>
<span class="n">y_l2</span> <span class="o">=</span> <span class="n">A</span> <span class="o">*</span> <span class="n">x_l2</span> <span class="o">+</span> <span class="n">b_l2</span><span class="p">;</span>

<span class="c1">% L1 (Lasso)</span>
<span class="p">[</span><span class="n">x_l1</span><span class="p">,</span> <span class="n">fitinfo</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">'Lambda'</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">);</span>
<span class="n">b_l1</span> <span class="o">=</span> <span class="n">fitinfo</span><span class="o">.</span><span class="n">Intercept</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="n">y_l1</span> <span class="o">=</span> <span class="n">A</span> <span class="o">*</span> <span class="n">x_l1</span> <span class="o">+</span> <span class="n">b_l1</span><span class="p">;</span>
<span class="nb">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span>
<span class="nb">bar</span><span class="p">(</span><span class="n">x_l1</span><span class="p">),</span> <span class="nb">axis</span> <span class="n">tight</span><span class="p">;</span>
<span class="nb">title</span><span class="p">(</span><span class="s1">'L1 Regularization'</span><span class="p">);</span>

<span class="c1">% L1 (Elastic Net)</span>
<span class="p">[</span><span class="n">x_en</span><span class="p">,</span> <span class="n">fitinfo_en</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">'Lambda'</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">'Alpha'</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">);</span>
<span class="n">b_en</span> <span class="o">=</span> <span class="n">fitinfo_en</span><span class="o">.</span><span class="n">Intercept</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="n">y_en</span> <span class="o">=</span> <span class="n">A</span> <span class="o">*</span> <span class="n">x_en</span> <span class="o">+</span> <span class="n">b_en</span><span class="p">;</span>

<span class="n">MSE_y</span> <span class="o">=</span> <span class="p">[</span><span class="n">mse</span><span class="p">(</span><span class="n">y_ols</span><span class="o">-</span><span class="n">y</span><span class="p">),</span> <span class="n">mse</span><span class="p">(</span><span class="n">y_l2</span><span class="o">-</span><span class="n">y</span><span class="p">),</span> <span class="n">mse</span><span class="p">(</span><span class="n">y_l1</span><span class="o">-</span><span class="n">y</span><span class="p">),</span> <span class="n">mse</span><span class="p">(</span><span class="n">y_en</span><span class="o">-</span><span class="n">y</span><span class="p">)];</span>
<span class="nb">disp</span><span class="p">(</span><span class="s1">'Mean square error: '</span><span class="p">)</span>
<span class="nb">fprintf</span><span class="p">(</span><span class="s1">'%g    '</span><span class="p">,</span> <span class="n">MSE_y</span><span class="p">);</span> <span class="nb">fprintf</span><span class="p">(</span><span class="s1">'\n\n'</span><span class="p">);</span>

<span class="c1">% Plot the recovered coefficients</span>
<span class="nb">figure</span><span class="p">,</span> <span class="nb">hold</span> <span class="n">on</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">x_l1</span><span class="p">,</span> <span class="s1">'b'</span><span class="p">);</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">x_en</span><span class="p">,</span> <span class="s1">'r'</span><span class="p">);</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">'g--'</span><span class="p">);</span>
<span class="nb">legend</span><span class="p">(</span><span class="s1">'Lasso Coef'</span><span class="p">,</span> <span class="s1">'Elastic Net coef'</span><span class="p">,</span> <span class="s1">'Original Coef'</span><span class="p">);</span></code></pre></figure>

<p>Output:</p>

<figure class="highlight"><pre><code class="language-matlab" data-lang="matlab"><span class="n">Mean</span> <span class="n">square</span> <span class="nb">error</span><span class="p">:</span>
<span class="mf">1.81793e-29</span>    <span class="mf">7.93494e-15</span>    <span class="mf">0.0975002</span>    <span class="mf">0.0641214</span></code></pre></figure>

<p>The above code snippets generates an under-determined matrix \(A\), and
a sparse coefficients which has 200 variables but only 10 of them are non-zeros.
Noises are added to the responses. We then run the proposed three methods
to try to recover the coefficients. It then generates two plots:</p>

<ol>
  <li>The first plot is as shown in the top. As we can see, OLS does a very bad job,
though the MSE is minimized to zero.
L2-regularization do find some of the sparks, but there are also lots of non-zeros
introduced. Finally, L1-regularization finds most of the non-zeros correctly
and resembles the original coefficients most.</li>
  <li>The second plot shows how similar the recovered coefficients by lasso and
elastic nets resemble the original coefficients. As we can see, both of them
can recover most parts, while elastic nets contain some small ‘noise’. However,
elastic net yields a slightly better MSE than lasso.</li>
</ol>

<p><img src="https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/2014-05-14-a-comparison-of-least-square-l2-regularization-and-l1-regularization/plot.png" alt="Plot of Coefficients" /></p>

<h2 id="probing-further">Probing Further</h2>

<p>Scikit has some excellent examples on regualarization (<a href="http://scikit-learn.org/stable/modules/linear_model.html">1</a>, <a href="http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html">2</a>). Quora has an
excellent <a href="http://www.quora.com/Machine-Learning/What-is-the-difference-between-L1-and-L2-regularization">discussion</a> on L2 vs L1 regualarization. I found the top three answers
very useful in understanding deeper, especially from the <em>Bayesian regularization paradigm</em> perspective by thinking the regularization as MAP (Maximum A Posteriori)
that adds a Laplacian (L1) or Gaussian (L2) prior to the original objective.</p>


        </div>

        

        
          
  <div class="page-comments">
    <div id="disqus_thread"></div>
    <script>
      var disqus_config = function () {
        this.page.url = 'https://www.ivansiu.com/2014/05/15/a-comparison-of-least-square-l2-regularization-and-l1-regularization';
        this.page.identifier = 'https://www.ivansiu.com/2014/05/15/a-comparison-of-least-square-l2-regularization-and-l1-regularization';
      };

      (function() {
        var d = document, s = d.createElement('script');
        s.src = 'https://iveney.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  </div>


        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/2014/05/02/debugging-applescript-print-to-a-file">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> Debugging AppleScript: print to a file

      </span>
    </a>
  

  
    <a class="page-next" href="/2014/05/19/sparse-signal-reconstruction-via-l1-minimization">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        Sparse Signal Reconstruction via L1-minimization
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>


    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: {
    extensions: ["mediawiki-texvc.js"],
    Macros: {
      complex: ['\\mathbb{C}'],
      norm: ['\\left\\lVert#1\\right\\rVert', 1],
      given: ['\\mathbin{\\vert}'],
      data: ['\\mathcal{D}'],
      argmax: ['\\mathop{\\arg\\,\\max}\\limits'],
    }
  }
  });
</script>
<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://about.me/iveney"><i class="fas fa-external-link-square-alt fa-2x" title="About"></i></a><a class="social-icon" href="https://twitter.com/ivanzxiao"><i class="fab fa-twitter-square fa-2x" title="Twitter"></i></a><a class="social-icon" href="https://github.com/iveney"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="https://www.linkedin.com/in/zgxiao/"><i class="fab fa-linkedin fa-2x" title="LinkedIn"></i></a><a class="social-icon" href="/atom.xml"><i class="fas fa-rss-square fa-2x" title="Feed"></i></a></div><div class="copyright">
    
      <p>&copy; 2021 Ivan Xiao. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


<!-- MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>

</html>
