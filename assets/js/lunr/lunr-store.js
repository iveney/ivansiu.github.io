var store = [{
        "title": "Notes on SSH port forwarding",
        "excerpt":"SSH Local Port Forwarding   Use ssh -L to bind a local port to a remote port   Dynamic Port Forwarding  Use ssh -D to dynamically bind a local port for forwarding. A SOCKS5 will be created. Example:   ssh -D 1080 user@remote-server.com   This binds to remote-server.com and uses local port 1080 for forwarding.   Optionally use -C for data compression.   The above command will log into the remote-server.com, use -f to put ssh to background, and use -N to not to execute a command. i.e.,   ssh -f -N -D 1080 user@remote-server.com     Usage Example   If the tool/app supports socks natively, use localhost:1080 in its setting. Example (curl)   curl --socks5 localhost http://icanhazip.com   If set up correctly, the IP returned will be the machine that is forwarding (remote-server.com)   If not, use some socksify tool, for example, use dsocks in OSX:   dsocks.sh ssh another-server.com   You can see where you logon using who | grep &lt;user-name&gt;.   A Real World Scenario   Scenario: I want to connect to VNC server at peabody::5903, however, peabody only accepts connection from CSL LAN, while my Macbook is in IllinoisNet (wireless).   Solution: use my office computer ‘orange’ as a proxy. Use local port forwarding:   ssh -L 5903:peabody:5903 orange   This means, forward localhost:5903 via orange to peabody:5903 So if I connect to the vnvserver at:   localhost :1 # port is 5901   I am forwarded to   peabody :3 # port is 5903   More On Port Forwarding   Port Forwarding  ","categories": ["network security"],
        "tags": ["socks proxy","port forwarding","networking"],
        "url": "https://www.ivanxiao.com/2013/05/09/notes-on-ssh-port-forwarding",
        "teaser": "https://www.ivanxiao.com/images/post_network.jpg"
      },{
        "title": "Roadtrip in America 2013",
        "excerpt":"In this year, I did several crazy road trips, each trip has about 3k miles. Here’re them.   Winter Trip  The first trip is a trip with my parents, where they flied to Chicago and I drove them to UIUC for a visit. We then flied to LA and rented a car to start our road trip. The whole trip was mainly in California, though we drove a bit east and visited Las Vegas and Grand canyon.        Route 66  I drove from Champaign all the way to Mountain View, with most of the part of Route 66.      I-80  After my internship in California, I drove back from Mountain View to Champaign.      ","categories": ["travel"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2013/08/28/roadtrips-in-us-2013",
        "teaser": "https://www.ivanxiao.com/images/post_hit_the_road.jpg"
      },{
        "title": "Installing a new car stereo",
        "excerpt":"I did not realize until my car’s radio’s broken that, with less than a hundred bucks, you can buy a new Head Unit with nearly all functionality. Some note-worthy features include:      iPod/iPhone support. Control the iphone directly from the panel.   App support. This includes I-heart-radio, pandora, etc.   Bluetooth Audio: stream music directly from a paired device. Even supports Pandora!   Upgraded FM/AM support. Displays text from radio stations.      Since I never used CD, I actually decided to buy a Digital Media Receiver: JVC KD-X250BT. Since every car has a different wiring harness, we need to buy a compatible wiring harness. My car has a Double DIN design but the DMR is a single DIN one, so I actually need to buy a installation kit. The wiring setup is fun:                                                ","categories": ["automotives"],
        "tags": ["car","DIY"],
        "url": "https://www.ivanxiao.com/2013/08/30/installing-a-new-car-stereo",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "Network Flow: Ford-Fulkerson Method",
        "excerpt":"Basic method (framework) is Ford-Fulkerson. It’s called a “method” because it’s general, there can be different implementation that yield different complexity. It looks like this:   Let F be an empty flow While there is augmenting path P from s to t     Augment F with P End while   The problem is how we find the augmenting path efficiently. A naive implementation is to use DFS to randomly pick a $s-t$ path and augment it. However, it has two problems:        The algorithm may not terminate, this happens when the capacity is irrational number (we can always scale rational to integer). When the capacity is irrational, the flow may fluctuate and never converge.   Even when the capacity is integer, the algorithm may be too slow. The runtime depends on the size of flow \\(\\vert F\\vert\\), which means if \\(\\vert F\\vert\\) is large, it takes a long time to stop. To see this, note that DFS takes \\(\\vert E\\vert\\) time, in the worst case, \\(F\\) may grow by 1 at each iteration (since capacity is integer). Thus, the complexity is \\(O(\\vert E\\vert F\\vert )\\).   To handle, this, Edmonds-Karp algorithm simply replaces the DFS as BFS, which finds a shortest each time (use unit length on edges in the residual graph). The major points are:      The length shortest path is guaranteed to monotonically increase at each iteration. This can be proven by contradiction.   The number of iterations are \\(O(\\vert V\\vert E\\vert )\\).            Each augmenting path (shortest path) can saturate one edge, and this edge will disappear from the residual graph. We call this edge ‘critical’.       An edge \\((u, v)\\) becomes critical to the time when it next becomes critical, the distance to \\(u\\) from the source increases by at least 2.       Thus, the total number of critical edges during execution is \\(O(\\vert V\\vert E\\vert )\\).       Each augmenting path contains at least one critical edge, thus \\(O(\\vert V\\vert E\\vert )\\).           The running time is thus \\(O(\\vert V\\vert E\\vert ^2)\\) since each iteration we run a BFS which costs \\(O(\\vert E\\vert )\\). This can be improved to \\(O(\\vert V\\vert ^2\\vert E\\vert )\\) with more efficient data structure (Dinic).  ","categories": ["algorithm"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2013/10/03/network-flow-ford-fulkerson-method",
        "teaser": "https://www.ivanxiao.com/images/post_circuit_board.jpg"
      },{
        "title": "Network Flow: Push-relabel algorithm",
        "excerpt":"According to CLRS: Many of the asymptotically fastest maximum-flow algorithms are push-relabel algorithms, and the fastest actual implementations of maximum-flow algorithms are based on the push-relabel method. Push-relabel methods also efficiently solve other flow problems, such as the minimum-cost flow problem.   This algorithm has a very different flavor. The overall idea is to generate a ‘preflow’ that may not satisfy the flow properties, and keep ‘pushing’) and ‘elevating’ (relabelling) the vertices until we cannot do that. We then remove the ‘excess’ from the preflow and obtain a valid flow, which is also a max flow. In particular, the in-flow may be larger than out-flow at a vertex. The amount of overflow is called ‘excess’.     The intuition behind is this: think of the vertices as platforms that have different height, where initially \\(S\\) has \\(\\vert V\\vert\\) height and all other vertices have 0 heights. The flow can only be pushed from higher vertices to lower vertices. Whenever we do not have any flow to push, we find some vertex  that has unsaturated out edge to its neighbor vertices to ‘relabel’, i.e., elevating its height such that we can continue to ‘push’. Thus, there are two operations ‘push’ and ‘relabel’ (and thus the name):   push: sending excess from u to v relabel: increase the height of u to min({v: neighbor of u})+1   The algorithm is as follows:   Initialize s.h = |V|, u.h = 0 for u != s. // u.h is the height of vertex u For all (s, u), push c(s, u). // saturate all outgoing edges from s. While there is vertex that can be pushed or relabel   do push or relabel End while return F.   The naive implementation has runtime \\(O(\\vert V\\vert ^2 \\vert E\\vert )\\), and can be improved to \\(O(\\vert V\\vert ^3)\\).  ","categories": ["algorithm"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2013/10/07/network-flow-push-relabel-algorithm",
        "teaser": "https://www.ivanxiao.com/images/post_circuit_board.jpg"
      },{
        "title": "Graph Matching: Augmenting Path",
        "excerpt":"Introduction   Let \\(G=(V, E)\\) be an undirected graph. Matching in \\(G\\) is a subset of edges \\(M \\subseteq E\\) such that at most one edge is incident to each vertex in \\(V\\).   A vertex is matched is it is incident to some edge in \\(M\\), otherwise it is called free or exposed.   Augmenting Paths      Alternating path: a path in which the edges belong alternatively to the matching and not to the matching   Augmenting path: an alternating path that starts from and ends on exposed vertices   Clearly, an augmenting path can be ‘flipped’ to increase the matching size by one, i.e., just make free vertex matched and make matched vertex free in this path.   Berge’s Theorem:  The matching M is maximum iff there is no augmenting path w.r.t. \\(M\\). I personally think the proof is easy but quite powerful.   Thus, we can immediately use the theorem to design an algorithm: find augmenting path iteratively until no more can be found. The problem is how we can find the augmenting path.     Algorithm for bipartite graph   FIND-AUGMENTING-PATH(\\(G=(V_1 \\cup V_2, E), M\\))      \\(V'_1\\) = a set of free vertices in \\(V_1\\)   \\(V'_2\\) = a set of free vertices in \\(V_2\\)   Construct the directed graph \\(G_M = (V_1 \\cup V_2, E_M)\\)            \\(E_M\\) is a set of directed edges such that it includes all arcs from \\(V_1\\) to \\(V_2\\), and all matching arcs from \\(V_2\\) to \\(V_1\\)       i.e., \\(E_M = \\\\{(v_1, v_2) : v_1, v_2 \\in E \\setminus M, v_1 \\in V_1, v_2 \\in V_2\\\\} \\cup \\\\{(v_2, v_1) : v_1, v_2 \\in M, v_1 \\in V_1, v_2 \\in V_2\\\\}\\)           Find a simple path \\(p\\) from \\(V'\\_1\\) to \\(V'\\_2\\) in \\(G_M\\)   Note that the above graph \\(G\\_M\\) is similar to the residual network in network flow. Apparently, \\(p\\) starts from a free vertex in \\(V'\\_1\\) and ends at another free vertex in \\(V'\\_2\\), thus it is an augmenting path.   Complexity   The maximum size of matching is upper bounded by \\(\\vert V\\vert /2\\), and each step, the matching size is incremented by one. Thus, the number of augmenting path found will be at most \\(O(\\vert V\\vert )\\). At each step, it takes \\(O(\\vert E\\vert )\\) to find a path. Thus the overall running time is \\(O(\\vert V\\vert E\\vert )\\).  ","categories": ["algorithm"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2013/10/07/graph-matching-augmenting-path",
        "teaser": "https://www.ivanxiao.com/images/post_circuit_board.jpg"
      },{
        "title": "Graph Matching: Hopcroft-Karp Algorithm",
        "excerpt":"Overview   Hopcroft-Karp algorithm also utilizes the augmenting path. The difference between the simple augmenting path algorithm is, instead of searching augmenting path one by one, it looks for many paths in the same time. The paths found at each iteration are in fact vertex disjoint path. By doing so, the number of iterations can be cut down, since there cannot be too many disjoing paths.   The major observation is that the length of augementing path grows at each step. Thus, if we keep finding a set of vertex disjoint augmenting paths, the algorithm is guaranteed to stop eventually and faster than finding the path one by one.     Finding Maximal Set of Vertex Disjoint Paths   Let \\(G=(U \\cup V, E)\\) be the bipartite graph and \\(G_M\\) be the directed graph w.r.t to matching \\(M\\). The layered graph is contructed out of \\(G_M\\), where the distance at a vertex \\(v\\) is defined as the length of shortest path from some vertices in \\(U\\).   This can done by simply running a modified BFS on \\(G_M\\): we start by enqueing all the free vertices $U’\\(in\\)U$ and label them as 0 (distance), propagate and label until one or more free vertices in \\(V\\) are reached. Let the label (distance) be \\(k\\), and denote this graph as \\(L\\).   We can then run a modified DFS on \\(L\\), starting from any of the vertices in $U’$. Whenever we reach a free vertices in \\(V\\), we delete the path \\(p\\) from \\(L\\), and repeat. It can be proven that all the paths found are shortest vertex disjoint paths of length \\(k\\). This set of paths are the maximal set we are looking for.   Algorithm Outline      MAXIMAL-SET-OF-PATHS(\\(G=(U \\cup V, E), M\\))            Let \\(P \\gets \\varnothing\\)       \\(L \\gets\\) Run modified BFS on \\(G_M\\)       \\(U' \\gets\\) free vertices in \\(U\\)       for \\(u \\in U'\\)                    \\(p \\gets\\) PARTIAL-DFS(\\(G, v, T\\))           if \\(p \\neq \\varnothing\\)                            Let \\(P \\gets P \\cup p\\)               Remove \\(p\\) from \\(L\\)                                               return \\(P\\)                HOPCROFT-KARP(\\(G\\))            Let \\(M \\gets \\varnothing\\)       repeat                    \\(P \\gets\\) MAXIMAL-SET-OF-PATHS(\\(G, M\\))           if \\(P \\neq \\varnothing\\)                            Let \\(M \\gets M \\oplus P\\)                                               until \\(P = \\varnothing\\)       return \\(M\\)           Analysis  It can be shown that the loop will be executed at most \\(O(\\sqrt{|V|})\\) times, and running the DFS and BFS requires \\(O(|E|)\\) time. Thus the overall run time is \\(O(\\sqrt{|V|} |E|)\\).   Reference     Wikipedia article   Lecture note from Sapienza University of Rome  ","categories": ["algorithm"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2013/10/08/graph-matching-hopcroft-karp-algorithm",
        "teaser": "https://www.ivanxiao.com/images/post_circuit_board.jpg"
      },{
        "title": "Updating Octopress Theme",
        "excerpt":"The themes are stored in .themes/&lt;name&gt;, we can use the rake command to update either the source or style, as documented in the office website. However, the syntax should be more clear, i.e.,   rake update_source['theme name']  Without the brackets, it defaults to .themes/classic.   Caveats:      Do NOT use rake install to install the theme again, since it will overwrite the whole thing, including the files in source/_include/custom directory.   When working on the blog, anything outside of source/_include/custom should be considered the theme. Thus, do not make changes directly in source. Make changes to theme and update the theme to reflect the changes to the blog.  ","categories": ["octopress"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2013/10/17/updating-octopress-theme",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "Understanding Page Components of Octopress: Using Foxslide as An Example",
        "excerpt":"  ","categories": ["octopress"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2013/10/17/understanding-page-components-of-octopress-using-foxslide-as-an-example",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "Saving $80 by Fixing MagSafe by Yourself",
        "excerpt":"After four years of use, the wire on my MagSafe finally fray out. After searching a bit, I found that a new one costs $80!! My old memories came back: almost all accessories by Apple are rip-offs. See how low the ratings are:      I know that this can be easily fixed if you know a little soldering and have courage: the adapter is working perfectly fine, it is just the wires. I decided to DIY. There are several pretty good articles on how to open the MagSafe and how to solder it.     The following are my workflows. The first step is to open it. You can see there are two wires, in fact in my case only the black wire is broken. I don’t even need to solder.                                    Okay, go ahead and twist them. Use insulating tapes to wrap them tight.      Finally, reassemble it and use more tapes, since the outside layer is cut out. More importantly, strong taping help to protect another fray.      It looks a bit odd though, since the Apple product is supposed to be WHITE. Here’s how I fix it. I don’t have good tools available, so I just use whatever I have. I recalled that I have some Google stickers, and I just cut the white pieces out.      After more taping, it looks like better now. Maybe I should consider wrapping better things outside later.      Hooray! Now it is working again! In fact, I am amazed that this piece of gadget has been working for four years, daily. It is still working just fine :)   There are other common issues for this model, and they can be easily fixed. See my following posts:      Replace faulty SATA cable   Fix unrecognized RAM   If you have similar experience, do not hesitate to let me know. If you find my instruction helpful, leave a comment and share it!   ","categories": ["DIY"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2013/11/08/saving-80-dollars-by-fixing-magsafe-by-yourself",
        "teaser": "https://www.ivanxiao.com/images/post_diy.jpg"
      },{
        "title": "LaTeX Tips and Tricks Collection",
        "excerpt":"This post summarizes some common useful tips in LaTeX editing.     Figures      Horizontal alignment of subfigures   subfloat in subfig   % Style: Figure 1(a) \\usepackage[subrefformat=parens,labelformat=parens]{subfig}  \\subfloat[]{ \\includegraphics[height=\\textheight]{example} \\label{fig:example} }  \\subref*{fig:example}  Bibliography      How do I run bibtex after using the -output-directory flag with pdflatex, when files are included from subdirectories?   biblatex simple usage   % unsrt, abbrev, last option means use unsrt in \\cite[xxx,yyy,ccc] \\usepackage[sorting=none,firstinits=true,sortcites=true]{biblatex}  % add this in preamble \\bibliography{dsa-verification}  % at the end \\printbibliography     Item spacing   Graphics     Graphics path   \\graphicspath{{figs/}}     Use other file extensions:   \\DeclareGraphicsExtensions{.pdf, .jpg, .png}  Table and Tabular     threeparttable: Use footnote in tabular   % inside tabular: \\tnote{$\\dagger$}  % after tabular \\begin{tablenotes}[para] % do not break line between items \\item [$\\dagger$] A foot note. % this reference to the item \\end{tablenotes}  Theorem, Definition     New definition/theorem   \\newtheorem{definition}{Definition}     Useful packages:            ntheorem       amsthm           Fonts   Packages     csvsimple: can automatically read csv and create tabular.   algpseudocode: the most updated algorithm / code environment, need to sit inside with algorithm package.   biblatex: provides better control for Bibliography.   multirow: provides \\multirow and \\multicolumn.   url: provides \\url.   xspace: prevent a command eating space.   flushend: provides \\flushend, balance two columns.   Controlling space     reduce space between float and text   \\setlength{\\textfloatsep}{10pt plus 1.0pt minus 2.0pt} \\setlength{\\floatsep}{10pt plus 1.0pt minus 2.0pt} \\setlength{\\intextsep}{10pt plus 1.0pt minus 2.0pt}     reduce bib item separation in biblatex   \\setlength\\bibitemsep{0.5\\itemsep}     geometry: control the margins at top, bottom, left and right        enumitem: control the spacing related to enumerate and itemize.       LaTeX Tips n Tricks for Conference Papers   Latex: Squeezing the Vertical White Space   Squeezing Space in LaTeX   Commands and tools:     latexmk: automatically compile multiple times to resolve reference.   biber: replacement for bibtex for biblatex   texdoc: use texdoc &lt;package&gt; to search for a manual.   IEEE     proof: use \\IEEEproof   multiple equations: use \\IEEEeqnarray or array.   Uncategorized     Where do I place my own .sty files, to make them available to all my .tex files?   Creating a central bibliography  ","categories": ["software"],
        "tags": ["LaTeX"],
        "url": "https://www.ivanxiao.com/2013/12/03/latex-tips-and-tricks-collection",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "Fix (ugly) Safari 7.0 not using local pac file",
        "excerpt":"I found that Safari in Mavericks is not using local proxy.pac at all. Turns out because of sandboxing, it will not allow reading file from local. A traditional solution is to turn on Web Sharing, and thus use HTTP to read the pac file such as http://localhost/proxy.pac.   However, this cannot be done that simple, since Apple removed Web Sharing from normal version of Mavericks. To turn on the web service (Apache), do this:   sudo apachectl start  Also, place the pac file under /Library/WebServer/Documents, which is the default Document Root of Apache.   Tom Fischer proposed another way to get around, however I don’t think it a good idea to mess around the system files.  ","categories": ["network security"],
        "tags": ["osx","networking"],
        "url": "https://www.ivanxiao.com/2013/12/05/fix-ugly-safari-7-dot-0-not-using-local-pac-file",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "Batch remove photos in iPhone, batch convert videos and upload back",
        "excerpt":"One of the problems that drives iOS user crazy is the user-unfriendly manipulation/syncing of photos in iOS. There are just no easy way to remove photos in batch (though you can delete the entire camera roll in the settings).     There are several ways to manipulate it, but really not enough:      use iPhoto/Aperture to import and delete. However, the photos must be imported first, I cannot just delete it. Ever since there is iCloud, importing becomes not that important.   use iTunes to Sync. Same as above, ever since iCloud, no reason to sync again.   the only way to use built-in tools to manipulate the camera roll is to use Image Capture utility. That way, one can select and delete photos. However, it still does not have custom filtering of photos and does not support albums.   The workflow I adopted is a two-step approach: mark those I want to keep in the iPhone, delete all stuff using the 3rd method, then upload back.   If you have some videos that occupy lots of space, you can convert them to lower quality. To do this, HandBrake is the excellent tool, which is open-sourced. However, it does not support batch change. Luckily, HandBrakeBatch by OSOMAC can achieve this.   To upload things back, SimpleTransfer can help, especially for videos, which cannot be put back into camera roll easily (I have no idea why apple restricts this). The lite version is free, but you can only upload one item at a time, and there are also some restrictions.  ","categories": ["software"],
        "tags": ["workflow","iphone"],
        "url": "https://www.ivanxiao.com/2014/01/30/batch-remove-photos-in-iphone",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "Source-highlight for matlab/octave",
        "excerpt":"   I recently worked with Matlab a lot. When in console, sometimes I want to use less to quickly examine the file content, and I have already set it up such that it uses source-highlight to output colorful escape sequence to the console. However, source-highlight does not come with a syntax support for Matlab by default. Luckily, this post and this (in Chinese) provides a solution.     First of all, install source-highlight using homebrew.   brew install source-highlight  Note that  source-highlight depends on boost, and as of the date of this post, brew provides a precompiled library (bottle) for boost. However, the python support is compiled against the system python, so if you installed a custom one (say installed via homebrew) and use it by default, brew will compile the boost from source instead, which takes an extremely long time.  To prevent this, we need to unlink it first, and link it back afterwards. That is,   brew unlink python brew link python    # after installation  Go to the following folder and create two files,   cd `brew --prefix source-highlight`/share/source-highlight  keyword blue; string #a020f0; comment darkgreen;  keyword = \"break|case|catch|classdef|continue|else|elseif|end|for|function|global|if|otherwise|parfor|persistent|return|spmd|switch|try|while\"  keyword = '^\\s*(break|case|catch|classdef|continue|else|elseif|end|for|function|global|if|otherwise|parfor|persistent|return|spmd|switch|try|while)(?=\\s)'  include \"function.lang\"  state command start '^\\s*[a-zA-Z][\\w]*(?=\\s)(?!\\s+\\.?[*/+-:]\\s+\\w)(?!\\s+=)' begin         comment start \"%\"         string delim '[[:blank:]]' '[^%;]*' end  comment delim \"%{\" \"%}\" multiline nested comment start \"%\"  string delim '(?&lt;![\\w.])\\'' \"'\" escape \"''\"  Finally, edit lang.map to create a mapping for matlab file.   echo 'm = matlab.lang' &gt;&gt; lang.map  ","categories": ["software"],
        "tags": ["tips","matlab","homebrew","source highlight"],
        "url": "https://www.ivanxiao.com/2014/05/01/source-highlight-for-matlab-slash-octave",
        "teaser": "https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/2014-05-01-source-highlight-for-matlab-slash-octave.png"
      },{
        "title": "Send ctrl-a in tmux after rebinding to it",
        "excerpt":"   While tmux provides much better functionality than screen, most of us that work with tmux have been using screen for a long time, and it is more comfortable for us to use ctrl-a than the default ctrl-b, which is finger-strechy. Thus the first thing I will do after installing tmux is to rebind the prefix to ctrl-a. That gives us the most handy way of swapping last two windows by typing ctrl-a ctrl-a.     However, this comes for a price. That is, in a shell environment that is integrated with readline, ctrl-a is used to jump the begining of line. Now that it is mapped as prefix, we can no longer do that.   Surprisingly, the solution is pretty simple. Just use ctrl-a a to send the prefix itself, and use ctrl-a ctrl-a to go to the last window. Specifically, add these two lines in ~/.tmux.conf:   #  ~/.tmux.conf bind a send-prefix bind-key C-a last-window  via.  ","categories": ["software"],
        "tags": ["tips","tmux"],
        "url": "https://www.ivanxiao.com/2014/05/01/send-ctrl-a-in-tmux-after-rebinding-to-it",
        "teaser": "https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/2014-05-01-send-ctrl-a-in-tmux-after-rebinding-to-it.png"
      },{
        "title": "OS X: get full path of file using realpath",
        "excerpt":" ➜  /Applications  realpath iBooks.app /Applications/iBooks.app ➜  /Applications  realpath is a command line utility that is included in most UNIX distributions but not Mac OS X. Thanks to Stuart Campbell, a minimal implementation is provided here, and my fork.     If you use homebrew, you can tap my repo, and install it using homebrew.   brew tap iveney/mocha brew install realpath  Viola! Now you can get the full path of file in console. A nice thing I often use is to chain it with pbcopy to copy the full path to the OS X clipboard.  ","categories": ["software"],
        "tags": ["osx","tips"],
        "url": "https://www.ivanxiao.com/2014/05/01/os-x-get-full-path-of-file-using-realpath",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "OS X: reveal file in console",
        "excerpt":"   One of the neat things you can do in OS X is to reveal a file in Finder.app from some other applications. Turns out lots of the time, we also want to do that in the terminal. The following script helps you with that:     #!/usr/bin/osascript on run args \tset fullpath to do shell script \"realpath \" &amp; (quoted form of first item of args) \tset theFile to POSIX file fullpath \ttell application \"Finder\" \t\treveal theFile \t\tactivate \tend tell end  Note:      You should put this into a directory in your search path. I recommend in ~/bin. Also remember to chmod +x.   The above requires another script realpath, which returns the fullpath of a file. You can find it here.  ","categories": ["software"],
        "tags": ["osx","tips"],
        "url": "https://www.ivanxiao.com/2014/05/02/os-x-reveal-file-in-console",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "Debugging AppleScript: print to a file",
        "excerpt":"Debugging AppleScript is easy when you work with the script editor, simply use log to print out anything in the console. However, after you compiled it to an app, this cannot work anymore.   I find there are several ways to do it in this thread. The two approaches that work best for me are:        Use logger to log to the syslog. E.g.,   do shell script \"logger -t 'AS DEBUG' \" &amp; myObj  However, I don’t know why sometimes this is not logged. So I will use the following:      Echo to file   do shell script \"echo \" &amp; quoted form of (myObj as string) &amp; ¬     \" &gt;&gt; ~/Desktop/as_debug.txt\"  ","categories": ["software"],
        "tags": ["AppleScript"],
        "url": "https://www.ivanxiao.com/2014/05/02/debugging-applescript-print-to-a-file",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "UFLDL: Sparse Coding",
        "excerpt":"Sparse Coding  Sparse Coding 可以看做一种 unsupervised learning 的方法，作用是从数据里学习一组 over-complete 的 basis，使得输入数据 x 可以表示为这些 basis 的线性组合。 根据 UFLDL 的说法，这些 basis 可以反映、捕捉输入数据内在的一些规则与结构：      The advantage of having an over-complete basis is that our basis vectors are better able to capture structures and patterns inherent in the input data.         Over-complete 的作用相当于是增强冗余。假设我们使用 autoencoder 学习到了图像的 2 个 edge feature（想象成是横与竖），那么对于对角线这样的 edge，对应的 coeffifient 需要对这两个 feature 进行插值，一旦有 noise，就很容易造成 coefficient 的变化。 想对地，如果我们学到了，比如 64 个 edge，那么每个 edge 可以看做是互相的插值， 而对应的 coefficient 也比较 smooth：表示任意角度的 edge 我们都可以只需要给临近的几个 edge features 非零系数即可。根据 wiki 的说法，这与人类视网膜的结构类似：      The human primary visual cortex is estimated to be overcomplete by a factor of 500, so that, for example, a 14 x 14 patch of input (a 196-dimensional space) is coded by roughly 100,000 neurons.    Sparsity and Under-determined Linear system   因为跟 “sparse” 有关，我首先从线性代数的角度阐述一下。   假设我们有数据 \\(x \\in R^n\\), 那么每个维度 就是一个 basis。如果我们旋转这组数据，就相当于得到了另外一组 basis。一般来说， 如果数据维度是 n，那么我们就需要 n 个这样的 basis 来表示数据。如果我们把这些 basis 看做一个矩阵 \\(A \\in R^{n \\times n}\\)，其中每个 column 是一个 basis，\\(s \\in R^n\\) 是数据在这个 basis 下的 coefficient，那么 \\(x = As\\)，就是相当于把数据映射到这个空间后的值。 显然，当 \\(A\\) 是方阵并且这些 basis 是 linear indeependent时，给定 x 后解 s 是唯一的。   但是当 A 的行数小于列数时，即 \\(m &lt; n\\)，这时的 linear system 是 under-constrained, 那么会有无穷多个解。如下面的 matlab code 所示，我们用 \\ (最小二乘法) 以及 pinv (伪逆矩阵) 求解得到的 s 是不同的。     但是如果我们加入了一些 constraint，情况就会有所不同。具体来说，我们希望 coefficient 里面的 non-zero elements 尽可能地说，也就是所谓的『sparsity』 constraint。   值得注意的是， PCA 也是找 basis 的一种方法， 但是 PCA 只能找到 \\(k \\le n\\) 的 basis， 所以无法解决我们的问题（我们需要 \\(k &gt; n\\)）。 对于 Sparse Coding Exercise 里用到的 20000 个 8x8 的 image patch，用 PCA 找到的 basis 看起来是这样的：      Sparse Coding: Objective Function   我们再考虑这个问题：假设 \\(x\\) 是输入数据，我们希望寻找到 k 个 basis \\(\\phi_i\\) 使得 \\(x\\) 能被表示为他们的线性组合，其中 \\(a_i\\) 为对应的 coefficient，i.e.,   \\[x = \\sum_{i=1}^k a_i \\phi_i\\]  使用经典的 least square method，对于 m 个数据 \\(D = \\{x^{(1)}, \\ldots, x^{(m)}\\}\\)， 我们可以写下如下的 Objective function 进行优化：   \\[\\min_{a_i^{(j)}, \\phi_i} \\sum_{j=1}^m \\left\\lVert x^{(j)} - \\sum_{i=1}^k a_i^{(j)} \\phi_i \\right\\rVert^2\\]  注意到三点：      不同于经典的 linear system, 这个 Objective function 我们有两样东西要优化： 系数 \\(a_i^{(j)}\\) 与 basis \\(\\phi_i\\) (稍后解释[如何处理][Sparse Coding: Optimization])。   这个 Objective function 的解可以有无穷多个（因为是 under-determined）。   对于一组解，我们可以 scale up \\(a_i^{(j)}\\) 同时 scale down \\(\\phi_i\\) 从而得到新的解， 但事实上这是冗余的。   如上面所讨论的，我们希望系数 \\(a_i^{(j)}\\) 是 sparse 的，也就是说，对于每个数据 \\(x^{(j)}\\), 我们希望尽可能多的 \\(a_i^{(j)}\\) 为 0。如果用 \\(S(a)\\) 来表示一个 penalize sparsity 的 function，当 \\(\\vert a\\vert  &gt; 0\\) 时，\\(S(a)\\) 返回一个很大的 cost。理论上，我们可以使用所谓的 \\(L_0\\) norm 作为这个 cost function，也就是当 \\(a=0\\) 时 \\(S(a) = 0\\), 否则 \\(S(a) = 1\\). 可是这个 function 既不是 smooth 也不不可微。所以我们一般使用 \\(L_1\\) penalty \\(S(a) = \\vert a\\vert _1\\) 或者 log penalty \\(S(a) = \\log (1+a^2_i)\\). 为了解决第三个问题，我们可以限制 \\(\\phi_i\\) 的 scale。把所有以上考虑进去，我们可以得到新的 Objective function:   \\[\\begin{split} \\min_{a_i^{(j)}, \\phi_i} &amp; \\sum_{j=1}^m \\left\\lVert x^{(j)} - \\sum_{i=1}^k a_i^{(j)} \\phi_i \\right\\rVert^2 + \\lambda \\sum_{i=1}^k S(a_i^{(j)}) \\\\\\\\ s.t. &amp; \\left\\lVert \\phi_i \\right\\rVert^2 \\le C, \\forall i=1, \\ldots, k \\end{split}\\]  其中第一项可以看为 reconstruction cost, 第二项看为 sparsity penalty, \\(\\lambda\\) 是 scaling factor，\\(C\\) 是一个常数。   Sparse Coding: Autoencoder Interpretation   以上的优化问题，其实可以看作一个 sparse autoencoder 的变种：input layer 对应于 sparse feature s, hidden layer 则是已知数据 x，他们之间的 connection 是 \\(W^{(1)} = A\\)，A 的每个 column 对应于一个 basis。我们希望学习到 A 与 s， 使得 x 能被 \\(As\\) 重构，因此 \\(\\vert As - x\\vert _2^2\\) 可以看做 reconstruction cost。 从线性代数的角度来说，A 可以看做一个 linear transform，s 是数据在 feature space 的 representation，而 x 是原始数据的 representation。As 的效果是把数据从 feature space 映射到 data space。 同时，我们希望 feature 是 sparse 的，因此我们有 sparsity penalty term \\(\\lambda \\vert s\\vert _1\\)。这个 Objective 写为：   \\[\\begin{split} \\min_{A, s} &amp; \\left\\lVert As - x \\right\\rVert^2_2 + \\lambda \\left\\lVert s \\right\\rVert_1 \\\\\\\\ s.t. &amp; \\ A^T_j A_j \\le 1 \\ \\forall j \\end{split}\\]  其中 \\(A^T_j A_j \\le 1\\) 是为了限制 basis 的 norm 大小而设置的，如上面所讨论。 注意到加了这个 constraint 后上面是一个 constrained programming problem， 要使用类似 gradient descent 的算法解决此类 问题，我们可以把 constraint 加到 Objective 里，成为一个所谓的 『weight decay』 term。因此我们定义 Objective function \\(J(A,s)\\) 为：   \\[J(A, s) = \\left\\lVert As - x \\right\\rVert^2_2 + \\lambda \\left\\lVert s \\right\\rVert_1 + \\gamma \\left\\lVert A \\right\\rVert^2_2\\]  注意到几点：      \\(\\min J(A, s)\\) 此时是一个 unconstrained programming，并且主要有三个 term 组成， 分别是 reconstruction cost, sparsity penalty, weight decay term。   \\(L_1\\) norm 在 0 点时无法求导。一般的方法是用 approximation 的方法来 “smooth out”。 具体地， 我们使用 \\(\\sqrt{s^2 + \\varepsilon}\\) 来代替 \\(\\vert s\\vert _1\\)。其中 \\(\\varepsilon\\) 是一个 smoothing parameter, 当 \\(\\varepsilon\\) 比 \\(s\\) 大时， \\(x+\\varepsilon\\) 会被 \\(\\varepsilon\\) 所 dominate, 所以整个 sqrt term 会约等于常数 \\(\\sqrt{\\varepsilon}\\)。 换句话来说，当 \\(s\\) 足够小（符合 sparsity constraint），这个 approximation 的产生的最小 penalty 为 \\(\\varepsilon\\)；否则， 这个 approximation 相当于 penalize non-sparsity。 因此 \\(J(A, s)\\) 可以改写为：   \\[J(A, s) = \\left\\lVert As - x \\right\\rVert^2_2 + \\lambda \\sqrt{s^2 +\\varepsilon} + \\gamma \\left\\lVert A \\right\\rVert^2_2\\]  其中 \\(\\sqrt{s^2+\\varepsilon}\\) 是 \\(\\sum_k \\sqrt{s_k^2+\\varepsilon}\\) 的简写。   值得一提的是，从推导的过程中，我们可以看到，从最简单的 reconstruction cost 开始， 我们逐步地加入更多的 cost/penalty terms 来处理 constraints，包括 sparsity 以及 weight decay （regularization）。这其实在 convex optimization 里也是很常用的技巧： primal-dual。 对于原始的 constrained problem（primal），我们会定义一个 dual problem， 并且把 constraints 加到原来的 Objective function 里使得问题变成 unconstrained。这个 technique 叫做 lagrangian relaxation；加上去的这些 terms 对应的 coefficients 称为 lagrangian multipliers。Dual problem 里，原来要优化的变量我们看做常量， 而 lagrangian multipliers 是要优化的变量。 可以证明这个 dual problem 是 convex 的。在我们这个问题里， 我们并没有直接优化 dual problem，相反地，我们把 lagrangian multipliers 定义为常量， 可以理解为一种 heuristic，因为这些 multipliers 也是可调的参数。   此外，对于不可导的函数， 采用 approximation 的方法把他们替代为可导的函数也是很常用的技巧。 另外一种常见的 approximation 是把 \\(max(x)\\) 替代为 log-sum-exp (x)，因为 exp 会把 max(x) 放大，然后加起来取 log 相当于这个 max(x) 会 dominate 整个 function。   Sparse Coding: Topographic Features   个人觉得 UFLDL 上关于这一块讲得不是特别清楚，所以这里会把它尽量解释清楚。 并且加入了大量的例子以及 visualization。 根据上面所述，我们会学习到 basis A 以及 feature s。但是根据观察，人类大脑皮层检测 edge 方向的神经元往往是有序的。具体来说，相邻的神经元检查的 edge 朝向接近， 比如检查 40, 45, 50 度的神经元相邻的。所以，我们也希望学习到的 feature 是 topographically ordered。直观来说，这样的效果是，如果某个 feature i 的值非零， 那么相临的 i-1, i+1 等，也应该是相似的非零值。比如说 s 大概长成 [0.0 0.0 0.12 0.34 0.67 0.95 0.73 0.45 0.32 0.11 0.0 0.0] 这样子。   我们原来的 sparsity penalty 只要求 feature 有 sparse 特性。 对于每个 feature \\(s_i\\)，我们只有一个对应的 penalty term \\(\\sqrt{s_i^2 + \\varepsilon}\\)。 反映到 2D 图像上， 如果我们希望 impose topographic ordering，我们可以 “group” 相邻的 features，使得某个 feature activate 时，周围区域的 features 也该 similarly activate。所以如果把这个 feature vector plot 出来，看起来某个地方是有一个明显的块状。这意味着，比如 40 - 50 度的 neurons 会被 activate。如面两图所示，左右分别是 Topographic / Non-topographic Features 与 Basis。可以看到，topographic features 的图像里有很多明显的小方块，而 nontopographic 看起来只是一堆无规律的 noise。对于 Basis images 则更明显了， non-topographic basis 看起来正是使用 sparse autoencoder 学习到的 edge features。 而 topographic basis 则是很漂亮地排了序。                                                            好了，有了 intuition，我们具体怎么做呢？假设我们把 feature vector 重新排成一个 square matrix，我们希望把 3x3 regions group 起来，那么我们只需要在 sparse penalty 里加入类似这样的项：   \\[\\sqrt{\\sum_{i=1}^3 \\sum_{j=1}^3 s_{i,j}^2 + \\varepsilon}\\]  其中 \\(s_{i,j}\\) 代表第 \\(i\\) 行 \\(j\\) 列的 feature。上面表示的是把 (1,1) - (3,3) 这个区域的 features group 起来。对于所有的 3x3 regions，我们都该加入类似的项， 可以想象成对这个 feature matrix 用一个 sliding window 生成所有项。   这里我们可能会有疑问：我们是对 feature 进行 grouping，为什么出来的 basis 就被『排序』了呢？直观来说，如果我们把每个 basis 看做一个 neuron，并且把它们排列成一个 matrix， 我们希望相邻的 neurons 能检测『接近』的 edges，所以每个小区域里的 neuron 对应的 edge filter 总是相似的（如上面左下图所示）。所以如果 features 的 value 总是成块出现的，对应的 neurons 也该被 “block activated”。具体到矩阵运算上面，参考下面的 1D 例子：      A    * s = x             B    * t = x 1 0 0 0   1   1          1 0 0 0   1   1 0 1 0 0   1   1          0 0 1 0   0   1 0 0 1 0   0   0          0 0 0 1   1   0 0 0 0 1   0   0          0 1 0 0   0   0   其中 A 与 B 是 Basis matrix，每个 column 对应与一个 basis。注意它们是代表的 是完全相同的 basis space，只是 column 排列顺序不同。 并且，A*s 与 B*t 都生成相同的 x， 但 s 里的两个 non-zero 是相邻的，而 t 则非相邻。   至此，我们已经理解了如何进行 topographic sorting，因此，我们可以重写 Objective function 为：   \\[J(A, s) = \\left\\lVert As - x \\right\\rVert^2_2 + \\lambda \\sum_{\\text{all groups g}} \\sqrt{\\left( \\sum_{\\forall s \\in g} s^2 \\right) + \\varepsilon} + \\gamma \\left\\lVert A \\right\\rVert^2_2\\]  我们可以只用一个 “grouping matrix” V 来方便程序实现，其中 V 的每行代表一个 group； \\(V_{r,c}=1\\) 代表 group \\(r\\) 包含 feature \\(c\\)。换句话说， V 是一个『选择』矩阵。 举例来说，假设我们把一个 feature vector \\(s\\in R^9\\) 看成一个 3x3 matrix， 把 matrix 里的每个 2x2 region看做一个 group，那么就会有四个 group：1245, 2356, 4578, 5689. 下面的代码说明了如何使用 V 来实现 grouping （请把 [1 … 9] 看做 \\(s_1^2, \\ldots, s_9^2\\)）：    1 2 3 4 5 6 7 8 9          V           s^2  1 1 0 1 1 0 0 0 0   [1 2 3 4 5 6 7 8 9]^T   =&gt;   1 + 2 + 4 + 5  0 1 1 0 1 1 0 0 0                                2 + 3 + 5 + 6  0 0 0 1 1 0 1 1 0                                4 + 5 + 7 + 8  0 0 0 0 1 1 0 1 1                                5 + 6 + 8 + 9   因此，我们把  Objective function 改写为：   \\[J(A, s) = \\left\\lVert As - x \\right\\rVert^2_2 + \\lambda \\sum \\sqrt{V s^2 + \\varepsilon} + \\gamma \\left\\lVert A \\right\\rVert^2_2\\]  其中 \\(\\sum \\sqrt{V s^2 + \\varepsilon} = \\sum_r \\sum_c D_{r, c}\\), \\(D = \\sqrt{V s^2 + \\varepsilon}\\)   注意我们还应该实现 “wrapping around”，细节请参考 UFLDL。   Sparse Coding: Vectorization and Summary   以上的推导都是只假设了一个 sample x 以及一个 feature vector s。高效的实现时需要完成 vectorization。 如果我们用 \\(S\\) 以及 \\(X\\) 代表 feature 以及 data matrix， \\(S^{(i)}\\), \\(X^{(i)}\\) 对应于第 i 个 column，那么最终的 Objective 为：   \\[J(A, S) = \\frac{1}{m}\\sum_{i=1}^m \\left\\lVert AS^{(i)} - X^{(i)} \\right\\rVert^2_2 + \\lambda \\sum \\sqrt{V S^2 + \\varepsilon} + \\gamma \\left\\lVert A \\right\\rVert^2_2\\]  注意这里的 \\(S^2\\) 是指 \\(S \\circ S\\)1。   在 matlab 里，我们可以 vectorize 上面一些步骤。比如 reconstruction cost 可以写成 sum(sum((A*S - X).^2)) / m。 其中里层的 sum 相当于算出每个 sample 的 reconstruction cost，即 \\(\\left\\lVert As^{(i)} - x^{(i)} \\right\\rVert^2_2\\)， 而外层的 sum 则是把所有的 cost 加起来。具体的 code 请参考我的 solution.   Sparse Coding: Gradients   上面提到，我们有两个变量需要优化：A, s。注意到，\\(J(A, s)\\) 里的 sparsity 以及 weight decay term 都是 convex 的。问题出在 reconstruction cost 上：它不是 convex 的。 根据 UFLDL ，以及 A. Ng nips06 的 paper [1]，观察到 fix A (或 s)， 则对于 s （或 A），\\(\\left\\lVert As - x \\right\\rVert^2_2\\) 都是 convex 的。 因此，我们可以迭代解决对于 A 与 s 的优化问题。直观来说，这有点像 [EM] 算法： 每次固定一个变量时，我们都能得到另外一个变量的新的、更小的 upperbound。如此迭代下去， 每个变量都能保证至少不比上一次迭代要大，直到收敛。   对于 \\(\\min_A J(A ; s)\\) （注意符号，读作 find A that minimizes J(A, s) given s）， 我们可以 drop 掉 sparse penalty （因为与 A 无关），\\(J(A; S)\\) 只是一个关于 A 的 quadratic optimization，所以我们可以很容易得到一个 closed-form formula，只需要 使关于 A 的 gradient 为零，也就是：   \\[\\nabla_A J = 0 \\Leftrightarrow 2 (As - x) s^T + 2 \\gamma A = 0 \\Leftrightarrow A = xs^T (s s^T + \\gamma I)^{-1}\\]  其中关于 \\(\\left\\lVert As - x \\right\\rVert^2_2\\) 的 gradient 计算请参考 The matrix cookbook 公式 (83)。注意上式针对的是 s, x  为 vector 的情况。我们现在来考虑 \\(J(A; S)\\)。注意到我们可以用 Frobenius norm 的形式来表示 \\(J(A, S)\\):   \\[J(A, S) = \\frac{1}{m} \\left\\lVert AS - X \\right\\rVert^2_F + \\lambda \\sum \\sqrt{V S^2 + \\varepsilon} + \\gamma \\left\\lVert A \\right\\rVert^2_2\\]  其中 \\(\\left\\lVert A \\right\\rVert^2_F = \\sqrt{\\sum_i \\sum_j \\left \\vert  a_{ij}\\right \\vert ^2}\\)，所以我们有：   \\[\\nabla_A J = 0 \\Leftrightarrow 2 (AS - X) S^T / m + 2 \\gamma A = 0 \\Leftrightarrow A = XS^T (S S^T + \\gamma m I)^{-1}\\]  可以看到，与 vector 形式是很相似的。   对于给定 A 求 S 的情况，很遗憾我们无法找到一个 closed-form formula，因此我们尝试使用 gradient descent。首先我们要找 S 的 gradient，难点在 sparsity penalty term \\(\\lambda \\sum \\sqrt{V S^2 + \\varepsilon}\\) 里，注意这里有点滥用数学符号： \\(\\sqrt{\\cdot}\\) 与 \\((\\cdot)^2\\) 在这里都是指 element-wise 的运算。 这里似乎使用最原始的方法展开整个表达式来推导 gradient 比较直观，或者把式子当做 复合函数然后使用 chain rule 来进行求导， 我暂时没有想到很简洁的矩阵表达方法。这里给出最后的 gradient 为：   \\[\\nabla_S J = V^T (V S^2 + \\varepsilon)^{(-1/2)} \\circ S\\]  Sparse Coding: Optimization   如上面所提到的，因为整个问题是 non-convex 的，我们必须迭代求解 \\(A\\) 与 \\(S\\) 进行优化。 然而正如 EM 等算法，我们必须先解决先有鸡还是先有蛋的问题：假设我们先优化 \\(A\\)，则必须 先有一个 \\(S\\) 的初始值。根据 UFLDL， 随机产生的 \\(S\\) 很可能使得优化收敛速度过慢。 一个好的初始化 \\(S\\) 的方法如下:       \\[S \\gets A^T X\\]      对于每个 \\(s = S^{(i)}\\)，使用对应的 basis \\(A^{(i)}\\) 进行 normalize。也就是：   \\[s \\leftarrow \\frac{s}{\\norm{A^{(i)}}}\\]  此外，一个实际的考虑是如果我们每轮迭代都使用所有的数据进行优化，也就是所谓的 batch 方法，那么优化时间会过长。想对地，如果我们每次使用一个随机的子集，比如 2000 out of 10000， 来进行优化，往往优化速度可以被大大加快。   Experimental Results and Observations   以下是我在完成 Sparse Coding Exercise 时的一些观察与体会：      大量参考了这里。 证实了里面提到的两点：            normalize 会产生问题。表现为每个 Iteration 的 fObj 震荡很厉害（e.g., 12.34, 623.12, 10.12, 702.12, …）。       在优化算法层面上 lbfgs 似乎的确不如 cg 得到的结果好。根据观测， lbfgs 收敛快并且能跳出 local minimum，最后得到的结果看起来不如 cg 好，但是跟初始解不一样。    \t而 cg 更稳定，做成动画 (youtube, tudou) 后，能看出基本上 feature 的 topographic 跟初始解差不多，但是慢慢地会 learn 到一些更清晰的 edges。           UFLDL Sparse Coding Exercise 里的 \\(s s^T\\) 应该是 \\(s^2\\) 或者  \\(diag(s s^T)\\)，因为 s 是 column vector，而我们要生成 \\(s \\circ s\\) (element-wise square, dot product)。   关于使用 backpropagation 算法来推导 gradient，UFLDL 上的推导似乎不是很严谨。在 Example 2 里推导 A 时，A 是作为第一层的 weight 存在，即 \\(W^{(1)}\\)。 如果严格按照 BP 算法，那么对于 \\(A=\\nabla W^{(1)}\\) 应该只需要计算到第二层的误差 \\(\\delta\\)，使得 \\(\\nabla W^{(1)} = \\delta^{(2)} (a^{(1)})^T\\)。而教程里是直接算到了第一层的误差 \\(\\delta^{(1)}\\). 而在 Example 3 里推导 s 时，s 却是作为输入存在的（第一层的 neurons），同样也需要算到第一层的误差，同时，经典 BP 里的 gradient 算的总是针对 connections 上的 weight，而这里 s 是 input layer。 一个 unified 的办法也许是加多一个第 0 层，输入是 \\(I\\) (Identify function)，并且把 \\(W^{(0)}\\) 设置为需要计算 gradient 的矩阵（也就是说， Example 2 里是 A, Example 3 里是 s），连接到第 1 层上。那么这时算我们要算的 gradient 就统一为 \\(\\nabla W^{(0)}\\) 了。   References   [1]H. Lee, A. Battle, R. Raina, and A. Y. Ng, “Efficient sparse coding algorithms,” in Advances in neural information processing systems, 2007, pp. 801–808.                 Element-wise multiplication. See more details here. &#8617;           ","categories": ["machine learning"],
        "tags": ["deep learning","computer vision","Chinese"],
        "url": "https://www.ivanxiao.com/2014/05/09/ufldl-sparse-coding",
        "teaser": "https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/2014-05-09-ufldl-sparse-coding/sparse_coding.png"
      },{
        "title": "A Comparison of Least Square, L2-regularization and L1-regularization",
        "excerpt":"   Problem Setting   Ordinary Least Square (OLS), L2-regularization and L1-regularization are all techniques of finding solutions in a linear system. However, they serve for different purposes. Recently, L1-regularization gains much attention due to its ability in finding sparse solutions. This post demonstrates this by comparing OLS, L2 and L1 regularization.     Consider the following linear system:   \\[Ax = y\\]  where \\(A \\in \\reals^{m \\times n}\\), \\(m\\) is the number of rows (observations) and \\(n\\) is the number of columns (variable dimension), \\(x\\) is the variable coefficients and \\(y\\) is the response. There are three cases to consider:      \\(m=n\\). This is the common-seen case. If \\(A\\) is not degenerate, the solution is unique.   \\(m&gt;n\\). This is called over-determined linear system. There is usually no solutions, but an approximation can be easily found by minimizing the residue cost \\(\\norm{Ax-y}^2_2\\) using least square methods, and it has a nice closed-form solution \\(x_{ls}=(A^T A)^{-1} A^T y\\). In L2-regularization, we add a penalize term to minimize the 2-norm of the coefficients. Thus, the objective becomes: \\(\\min_x \\norm{Ax-y}^2_2 + \\alpha \\norm{x}_2\\) where \\(\\alpha\\) is a weight to decide the importance of the regularization.   \\(m&lt;n\\). This is called under-determined linear system. There is usually no solution or infinite solutions. This is where it get interesting: when we have some prior knowledge in the solution structure, such as sparsity, we can have a ‘metric’ to find a better solution among a whole bunch. The objective is thus: \\(\\min_x \\norm{Ax-y}^2_2 + \\alpha \\norm{x}_1\\) The optimization technique for the above problem is called lasso, and there is an advanced version called elastic net, which combines the L2 and L1 regularization together, hoping to get the advantages of both: L1 regularization finds sparse solution but introduces a large Mean Square Error (MSE) error, while L2 is better at minimizing MSE.   An Example   In the following, we show their performances by solving a simple case.   % regression_ex.m % Compare Ordinary Least square (no regularization), L2-reguarlized (Ridge), % L1-regualarized (Lasso) regression in finding the sparse coefficient % in a underdetermined linear system  rng(0);  % for reproducibility m = 50;  % num samples n = 200; % num variables, note that n &gt; m  A = rand(m, n); x = zeros(n, 1); nz = 10; % 10 non-zeros variables (sparse) nz_idx = randperm(n); x(nz_idx(1:nz)) = 3 * rand(nz, 1); y = A*x; y = y + 0.05 * rand(m, 1); % add some noise  % plot original x subplot(2, 2, 1); bar(x), axis tight; title('Original coefficients');  % OLS x_ols = A \\ y; subplot(2, 2, 2); bar(x_ols), axis tight; title('Ordinary Least Square'); y_ols = A * x_ols;  % L2 (Ridge) x_l2 = ridge(y, A, 1e-5, 0);  % last parameter = 00 to generate intercept term b_l2 = x_l2(1); x_l2 = x_l2(2:end); subplot(2, 2, 3); bar(x_l2), axis tight; title('L2 Regularization'); y_l2 = A * x_l2 + b_l2;  % L1 (Lasso) [x_l1, fitinfo] = lasso(A, y, 'Lambda', 0.1); b_l1 = fitinfo.Intercept(1); y_l1 = A * x_l1 + b_l1; subplot(2, 2, 4); bar(x_l1), axis tight; title('L1 Regularization');  % L1 (Elastic Net) [x_en, fitinfo_en] = lasso(A, y, 'Lambda', 0.1, 'Alpha', 0.7); b_en = fitinfo_en.Intercept(1); y_en = A * x_en + b_en;  MSE_y = [mse(y_ols-y), mse(y_l2-y), mse(y_l1-y), mse(y_en-y)]; disp('Mean square error: ') fprintf('%g    ', MSE_y); fprintf('\\n\\n');  % Plot the recovered coefficients figure, hold on plot(x_l1, 'b'); plot(x_en, 'r'); plot(x, 'g--'); legend('Lasso Coef', 'Elastic Net coef', 'Original Coef');  Output:   Mean square error: 1.81793e-29    7.93494e-15    0.0975002    0.0641214  The above code snippets generates an under-determined matrix \\(A\\), and a sparse coefficients which has 200 variables but only 10 of them are non-zeros. Noises are added to the responses. We then run the proposed three methods to try to recover the coefficients. It then generates two plots:      The first plot is as shown in the top. As we can see, OLS does a very bad job, though the MSE is minimized to zero. L2-regularization do find some of the sparks, but there are also lots of non-zeros introduced. Finally, L1-regularization finds most of the non-zeros correctly and resembles the original coefficients most.   The second plot shows how similar the recovered coefficients by lasso and elastic nets resemble the original coefficients. As we can see, both of them can recover most parts, while elastic nets contain some small ‘noise’. However, elastic net yields a slightly better MSE than lasso.      Probing Further   Scikit has some excellent examples on regualarization (1, 2). Quora has an excellent discussion on L2 vs L1 regualarization. I found the top three answers very useful in understanding deeper, especially from the Bayesian regularization paradigm perspective by thinking the regularization as MAP (Maximum A Posteriori) that adds a Laplacian (L1) or Gaussian (L2) prior to the original objective.   ","categories": ["machine learning"],
        "tags": ["least square","regularization"],
        "url": "https://www.ivanxiao.com/2014/05/15/a-comparison-of-least-square-l2-regularization-and-l1-regularization",
        "teaser": "https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/2014-05-14-a-comparison-of-least-square-l2-regularization-and-l1-regularization/coef.png"
      },{
        "title": "Sparse Signal Reconstruction via L1-minimization",
        "excerpt":"   This is a follow-up of the previous post on applications of L1 minimization.     As we know, any signal can be decomposed into a linear combination of basis, and the most famous one is Fourier Transform. For simplicity, let’s assume that we have a signal that is a superposition of some sinusoids. For example, the following:   .5*sin(3*x).*cos(.1*x)+sin(1.3*x).*sin(x)-.7*sin(.5*x).*cos(2.3*x).*cos(x);  With discrete consine transform (DCT), we can easily find the coefficients of corresponding sinusoid components. The above example’s coefficients (in frequency domain) and signal in time domain are shown in the post figure.   Now, let’s assume we do not know the signal and want to reconstruct it by sampling. Theorectically, the number of samples required is at least two times the signal frequency, according to the famous Nyquist–Shannon sampling theorem.   However, this assume zero-knowledge about the signal. If we know some structure of the signal, e.g., the DCT coefficients are sparse in our case, we can further reduce the number of samples required. 1   The following code snippet demonstrates how this works. We generate the original signal in time domain and then perform a DCT to obtain the coefficients.   % sparse signal recovery using L1  rng(0); N = 256; R = 3; C = 2;  % some superposition of sinoisoids, feel free to change and experiment f = @(x) .5*sin(3*x).*cos(.1*x)+sin(1.3*x).*sin(x)-.7*sin(.5*x).*cos(2.3*x).*cos(x); x = linspace(-10*pi, 10*pi, N); y = f(x);  subplot(R,C,1); coef = dct(y)'; stem(coef); xlim([0 N]); title('Original signal in frequency domain');  subplot(R,C,2); plot(x,y); xlim([min(x) max(x)]); title('Original signal in time domain');  Let’s assume that we have a device that can sample from the frequency domain. To do this, we create a random measurement matrix to obtain the samples. We use 80 samples here. Note that we normalize the measurement matrix to have orthonormal basis, i.e., the norm of each row is 1, and the dot product of different row is 0.   % measurement matrix K=80; A=randn(K, N); A=orth(A')';  % observations b=A*coef;  We first try a least-square approach, which boils down to inverse the matrix and obtain \\(\\hat{x}=A^{-1} b\\). Note that as A is not square, we are using its pseudo-inverse here. Furthermore, as A is othornormal, its transpose is the same as pseudo-inverse.   % min-energy observations c0 = A'*b; % A' = pinv(A) here since A is a full-rank orthonormal matrix subplot(R,C,3); stem(c0); xlim([0 N]); title('Minimum energy recovery - coef');  subplot(R,C,4); y0 = idct(c0, N); plot(1:N, y0,'r', 1:N, y, 'b'); xlim([0 N]); title('Minimum energy recovery - signal'); legend('Recovered', 'Original');  As we can see, there are lots of non-zeros in the coefficients, and the recovered signal is very different from the original signal.   Finally, we use L1-minimization for reconstruction. I used lasso to perform a L1-regualarized minimization. Another package that performs various L1-minimization is l1-magic.   % L1-minimization [c1, fitinfo] = lasso(A, b, 'Lambda', 0.01); % If use L1-magic % addpath /path/to/l1magic/Optimization % [c1] = l1eq_pd(c0, A, [], b, 1e-4); subplot(R,C,5); stem(c1); xlim([0 N]); title('L1 recovery - coef');  subplot(R,C,6); y1 = idct(c1, N); plot(1:N, y1, 'r', 1:N, y, 'b'); xlim([0 N]); title('L1 recovery - signal'); legend('Recovered', 'Original');  The above shows that L1-minimization successfully recovered the original signal. A complete code snippet can be found here.                  In order to recover f perfectly, we need at least \\(B \\log (N)\\) samples (source). &#8617;           ","categories": ["machine learning"],
        "tags": ["signal processing","algorithm"],
        "url": "https://www.ivanxiao.com/2014/05/19/sparse-signal-reconstruction-via-l1-minimization",
        "teaser": "https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/sparse_signal_reconstruction.png"
      },{
        "title": "Sparse Image Reconstruction via L1-minimization",
        "excerpt":"                                              Original       Minimum Energy Reconstruction       Sparse Reconstruction           Introduction   This is a follow up of the L1-minimization series. The previous two posts are:      A Comparison of Least Square, L2-regularization and L1-regularization   Sparse Signal Reconstruction via L1-minimization    We have explored using L1-minimization technique to recover a sparse signal. The example shows a 1D example. This post demonsrates  on a 2D example, where the image is viewed as a signal. This makes sense as we can perform 2D Fourier Transform in the image, where the basis are a combination of horizontal and vertical waves. For a complete introduction to FFT on images, refer to this tutorial. Notice that similar to 1D signal, we do not measure the image directly in time domain, but we do it in the frequency domain. Concretely, say \\(x\\) is the 2D image collapsed to 1D, and \\(A \\in \\reals^{k\\times n}\\) is the measurement matrix, \\(b\\) is the observation, we then have \\(Ax=b\\). Usually we will require \\(k = n\\) to obtain an exact solution for \\(x\\) given \\(A\\) and \\(b\\). Now, if we use FFT and obtain the frequency coefficients as \\(\\hat{x}\\), we can also perform similar measurements \\(\\hat{A} \\hat{x} = \\hat{b}\\), and the requirement \\(k = n\\) is the same. In other words, the required samples (the information) is the same. By using the inverse fourier transform, we can convert \\(\\hat{x}\\) back to \\(x\\). The only difference is that the measurement \\(\\hat{A}\\) is taken in frequency (Fourier) domain. As we can see later, we can utilize sparse information to reduce \\(k\\).   Image Gradients and Total Variation   We first introduct the concept of image gradients. For any 2D real image I, if we think about each row as a signal, we can then view the ‘difference’ between adjacent pixels as (horizontal) gradient Gx(I), this makes sense since a sharpe change denotes an edge. Similary, we can define the vertical gradient Gy(I) for columns. Thus, we have   \\[Gx(I) = \\begin{cases} I_{i+1, j} - I_{ij} &amp; i &lt; n \\\\\\\\ 0 &amp; i = n \\end{cases} \\qquad Gy(I) = \\begin{cases} I_{i, j+1} - I_{ij} &amp; j &lt; n \\\\\\\\ 0 &amp; j = n \\end{cases}\\]  where the image size is \\(n\\times n\\).   Collectively, the image gradient G(I) is defined as the magnitude (2-norm) of both components:   \\[G(I)_{ij} = \\sqrt{(Gx(I)_{ij})^2 + (Gy(I)_{ij})^2}\\]  The following shows Gx, Gy and G of the phantom image:                                                 Gx(I)       Gy(I)       G(I)           The total variation TV(I) of an image is just the sum of this discrete gradient at every point.   \\[TV(I)= \\norm{G(I)}_1 = \\sum_{i,j} G(I)_{ij}\\]  We notice that \\(TV(I)\\) is just the L1-norm of \\(G(I)\\), which leads us to the following: if we have an image that is sparse in its image gradients, we can exploit that and use our L1-minimization trick.   Sparse Gradient Image Reconstruction   The ratio of non-zero elements in Gx, Gy and G of the phantom image is 0.0711, 0.0634 and 0.0769, respectively. These ratios are really small - and we consider the gradient as sparse.   Let \\(F: \\reals^{n\\times n} \\to \\complex^{n\\times n}\\) be the FFT operator, and \\(F I\\) be the Fourier transform taken on image I. Define a set \\(\\Omega\\) as the \\(k\\) two-dimensional frequencies chosen according to some sampling pattern from the \\(n \\times n\\). We further define \\(F_\\Omega I: \\reals^{n \\times n} \\to \\complex^k\\) as the \\(k\\) observation taken from the fourier transform of image I. We can then solve the following optimization problem to recover \\(I\\):   \\[\\min_I \\norm{F_\\Omega I - b}^2_2\\]  where \\(F_\\Omega\\) can be view as the measurement matrix, \\(b\\) is the observation, and we want to find \\(I\\) such that the reconstruction cost (energy) is minimized.   However, the above does not quite work. As we can see in the following images, the L2-minimization does a poor job, either for a random measurement or a radial measurement [@candes2006robust] in Fourier domain.                                                 Random measurement       L2-minimization       L1-minimization                                                         Radial measurement       L2-minimization       L1-minimization           To utilize the sparse information, we add a L1-regularization term to the above objective function, which yields the following:   \\[(TV_1) \\quad \\min_I \\norm{F_\\Omega I - b}^2_2 + \\lambda TV(I)\\]  Without surprise, optimizing the above gives us a perfect reconstruction of the original image. It is shown that if there exists a piecewise constant I with sufficiently few edges (i.e., \\(G(I)_{ij}\\) is nonzero for only a small number of indices i, j), then \\((TV_1)\\) will recover I exactly.   A heavily commented code example is available in my github repository. Leave a comment if you have any question.   Probing Further   Now, take a look at another example cameraman, which has the following gradients (intensity rescaled using matlab’s imagesc.                                          Cameraman       Gradient           The following shows the reconstructions (left two are using random measurements, right two are using radial measurements).                                                        Rand (L2)       Rand (L1)       Radial (L2)       Radial (L1)           As we can see, the results are not as good. In fact, the non-zero ratio of its gradient is 0.9928, which is not sparse at all. However, if we plot the histogram of gradients, we will find that most of the gradient magnitudes are small:      In particular, most of them are smaller than 200, which means the number of ‘changes’ that are larger than 200 is small. In fact, the ratio of gradient &gt; 200 is only 0.0964! Thus, there are two possible ways to discard these information and get a ‘compressed’ image that is sparse in gradients:      Use mean-shift algorithm to segment the regions such that they have the same color intensities. K-means or quantization should achieve a similar result, though might not as good as mean-shift.   Use image filtering to smooth the image, which can effectively average colors and discard high frequency information.   I’ll leave these conjectures for furture implementation. For those intereted, please try them yourself and let me know your results. If you have any thoughts, do not hesitate to leave a comment.   References   For interested readers, the following references will be helpful. [1], [2], [3], [4]   [1]E. Candes and J. Romberg, “l1-magic: Recovery of sparse signals via convex programming,” vol. 4, 2005. [2]J. S. Hesthaven, K. Chowdhary, E. Walsh, and others, “Sparse Gradient Image Reconstruction from Incomplete Fourier Measurements and Prior Edge Information,” IEEE TRANSACTIONS ON IMAGE PROCESSING, 2012. [3]J. K. Pant, W.-S. Lu, and A. Antoniou, “A new algorithm for compressive sensing based on total-variation norm,” in Circuits and Systems (ISCAS), 2013 IEEE International Symposium on, 2013, pp. 1352–1355. [4]E. J. Candès, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information,” Information Theory, IEEE Transactions on, vol. 52, no. 2, pp. 489–509, 2006.  ","categories": ["machine learning"],
        "tags": ["computer vision","signal processing"],
        "url": "https://www.ivanxiao.com/2014/05/19/sparse-image-reconstruction-via-l1-minimization",
        "teaser": "https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/cameraman.png"
      },{
        "title": "My Octopress Blogging Flow",
        "excerpt":" After blogging with Octopress for a while, I have already gained some insights on it, and my publishing flow has been smoother. I think it is right time to share my flow as a reference.     The Flow   The following sections outlines the flow. The last section contains assorted tips and tricks. For the basic configuration of Octopress, please refer to the official website. I also recommend installing Alfred.app.   Creating a post: alfred workflow (blog publish)   Artem Yakimenko created an awesome alfred workflow for publishing and generating octopress websites. Use blog publish [title] to create a new post:      It then opens the post in your specified text editor with template.   Editing: Sublime Text.app + Marked.app + Marked App Menu (ST Plugin)   I use Sublime Text (ST) as my text editor, because it provides VIM keybinding and there is a huge repository of plugins. While editing, I use [Marked] to instantly render the markdown file and view the result. In fact, the title image shows my editing and previewing in action.   To make the process sweeter, there is a ST plugin called Marked App Menu that allows you to open the current file in Marked. Just search in ST Package Control to install it.   Previewing: pow + rake watch + pow alfred workflow   To preview the generate website, simply install pow and execute rake watch under octopress directory to monitor the change. Octopress official website provides some explanation. After installation, you can view your website locally at http://octopress.dev.   You should also install the pow alfred workflow, which can help you open pow website in a breeze.      Math Rendering: MathJax   Since I am an EECS student, I write a lot of Optimization, Machine Learning and Computer Vision stuff, which relies heavily on mathematics. Thus, writing math formulas is a must for me. MathJax is a javascript library for rendering math by writing LaTeX math. To do this, one needs to configure the site and link to the library. Put these two lines into &lt;octopress&gt;/source/_includes/custom/head.html:   &lt;script type=\"text/javascript\" \tsrc=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"&gt; &lt;/script&gt;  &lt;script type=\"text/javascript\" src=\"/javascripts/MathJaxLocal.js\"&gt;&lt;/script&gt;  The first script block loads MathJax, and the second loads a custom configuration in source/javascripts/MathJaxLocal.js. It is a good place to write your own macro there. For instance:   MathJax.Hub.Config({ \tTeX: { \t\tequationNumbers: { autoNumber: \"AMS\" }, \t\tTagSide: \"left\", \t\tMacros: { \t\t\treals: ['\\\\mathbb{R}'], \t\t\tcomplex: ['\\\\mathbb{C}'], \t\t\tnorm: ['\\\\left\\\\lVert#1\\\\right\\\\rVert', 1], \t\t\tgiven: ['\\\\:#1\\\\vert\\\\:', 1], \t\t\tdata: ['\\\\mathcal{D}'], \t\t} \t} });  MathJax.Ajax.loadComplete(\"/javascripts/MathJaxLocal.js\");  Now you can write math!   \\[e^{i \\pi} + 1 = 0\\]  There are a couple of good articles for reference:      Blogging With Math: Octopress, MathJax, and Pandoc   Writing Math Equations on Octopress   LaTeX Math in Octopress   Image hosting: Dropbox public folder   I host my images in Dropbox Public folder, since you can simply copy the public link and paste it to the post source, for example:      Image editing: Skitch + OmniGraffle   The previous image is done by a timed capture from skitch. For advanced vector graphics, I use OmniGraffle.   Markdown converter: pandoc   Pandoc is a swiss-army knife like tool that convert documents in multiple formats to several dozens of output formats. I mainly use it as the markdown converter for Octopress. A plugin can help you with that.   After installation, I update the markdown section of _config.yml with the following:   markdown: pandoc pandoc:   format: html5   extensions:     - smart     - mathjax     - bibliography: source/blog.bib     - csl: _style/ieee.csl  which tells Octopress to use pandoc, and pass the option smart, mathjax and use the style file ieee.csl to format the biliography blog.bib. For example, refer to [@xiao2013optimally] generates refer to [@xiao2013optimally] (scroll down to see the [References] section).   Pandoc’s Markdown Reference: Dash.app and Dash alfred workflow   Dash.app is an API Documentation Browser and Code Snippet Manager. It provides an convinient alfred workflow that searches the documents:      opens      Tips and tricks   Continuously growing…      There are lots of undocumented/less documented things in Octopress, which can help you write blog posts kinda ‘programmatically’. For example, https://www.ivanxiao.com returns the url of site, which is  https://www.ivanxiao.com in my case. In fact, anything in _config.yml is a variable under site.   Conclusion   In conclusion, Octopress is a revolutionary blogging framework. It provides a robust static site building framework (jekyll, bootstrap, scss, etc.) and allows complete control over the source, which is perfect for  users that have basic coding and source control skills. In fact, it gives me a similar feeling of getting touch with a Mac. That is, compared to Windows, which is too close and does not provide built-in programming-friendly environment (Console, UNIX stuff, etc.), and compared to Linux, which is very open but too many variations and too many customizations needed, it combines their advantages by presenting a user- friendly interface and provides all sorts of underlying UNIX tools. I am very satisfied about this and my intention to write posts have  revived. However, some sort of basic configuration is still needed. In particular, I would say Mathjax rendering and better image support definitely need  to be integrated in the next release.   What’s your thought? Do you have any neat tricks publishing with Octopress? Please leave your comments.   References   ","categories": ["blogging"],
        "tags": ["octopress","jekyll"],
        "url": "https://www.ivanxiao.com/2014/05/23/my-octopress-blogging-flow",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "Replacing SATA cable in Mid 2009 MBP",
        "excerpt":"          Replacing MBP cable        Recently, my mid 2009 MBP (Model A1278) fails to recognize the hard drive. My first bet was  another disk failure on me, but it was not the case. I took down the hard drive and put it to a mobile hard drive case and it can be read smoothly.     It turns out that it is due to the SATA cable fault, which is a notorious problem for mid 2009 MBP model. See the threads and discussions here, here, here and here.   Luckily, the solution is simple, just go ahead and purchase a replacement cable and replace it. iFixit has a very detailed illustrated document on the procedures. However, Amazon has a cheaper option, and it works fine for me. I didn’t test the infra-red though, since I never and not plan to use it.   There are other common issues for this model, and they can be easily fixed. See my following posts:      Fix frayed Magsafe cable   Fix unrecognized RAM   If you have similar experience, do not hesitate to let me know. If you find my instruction helpful, leave a comment and share it!   ","categories": ["DIY"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2014/05/28/replacing-sata-cable-in-mid-2009-mbp",
        "teaser": "https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/fix-mbp/sata-cable.jpg"
      },{
        "title": "Fix Mid 2009 MBP RAM not recognized issue",
        "excerpt":"          Possible cause of unrecoganized RAM        A few days ago, I encountered an issue which seems to be common among mid-2009 MBPs: one of the RAM (in slot 1) is not recognized anymore. Or, sometimes it is recognized, but after sleep and wake up, the computer freezes and impossible to recover but force power off.     It turns out this is a common issue in this model. See the discussion in this thread and this thread. In the following, I am going to present my temporary fix for this problem. For those of you that still want to stick to the old MBP, the fix shall last for a while. But I do recomend you backup all the files and prepare to migrate some day soon.   As I suggested in my reply, this may due to a RAM slot degradation. My guess is, the RAM slot cannot align the RAM to a correct contact positions any more. Precisely, see the post image above. Notice that the two clips on the left and right are used to hold the rams in a horizontal position, otherwise they will bend upwards. I took a close look at those clips and found that the plastic wore out and cannot hold them as original. I don’t have great ways to fix them, so I just cropped some papers and insert them between the RAMs and the back edges on the body, hoping they can help tucking the RAMs.   As I am fixing it, I accidentally broke the left holder. So I have to customized a plastic holder and stuck it with the logic board to hold the inner ram (slot 1).      To verify my theory, I then taped a padding at the corresponding RAM position in the back cover:      And they look like the following before I close:      Note that you have to screw it real tight to create the pressure such that the RAM is aligned. That said, there will be a ‘bump’ at that position, and will easily cause scratch. So I use an apple sticker to cover my ass:      This method worked for me, well, at 99% of the time. Sometimes after sleep, the MBP still won’t wake up. I notice that this usually due to running  the MBP for a long time, and it’s hot inside. Nevertheless, this is the best solution I can come up with by now. If you have any other cheap solution that does not require replacing the logic board, please let me know in the comments.   Finally, Zach Clawson created a dedicated page for this issue, which lists lots of reference and provides explanation to it. Make sure you check it out if you have encountered similar issue.     There are other common issues for this model, and they can be easily fixed. See my following posts:      Fix frayed Magsafe cable   Replace faulty SATA cable   If you have similar experience, do not hesitate to let me know. If you find my instruction helpful, leave a comment and share it!  ","categories": ["DIY"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2014/05/29/fix-mid-2009-mbp-ram-not-recognized-issue",
        "teaser": "https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/fix-mbp/mbp09-ram-theory.png"
      },{
        "title": "Notes on Maximum Likelihood, Maximum A Posteriori and Naive Bayes",
        "excerpt":"Let \\(\\data\\) be a set of data generated from some distribution parameterized by \\(\\theta\\). We want to estimate the unknown parameter \\(\\theta\\). What we can do?     Essentially, we want to find a most likely value of \\(\\theta\\) given \\(\\data\\), that is \\(\\arg \\max P(\\theta | \\data)\\). According to Bayes Rule, we have   \\[P(\\theta \\given \\data) = \\frac{P(\\data \\given \\theta)P(\\theta)}{P(\\data)}\\]  and the terms have the following meanings:      \\(P(\\theta \\given \\data)\\): Posterior   \\(P(\\data \\given \\theta)\\): Likelihood   \\(P(\\theta)\\): Prior   \\(P(\\data)\\): Evidence   Maximum Likelihood Estimation (MLE)   An easy way out is to use the MLE method. We want to find a \\(\\theta\\) the best explains the data. That is, we maximize \\(P(\\data \\given \\theta)\\). Denote such a value as \\(\\hat{\\theta}_{ML}\\). We have   \\[\\hat{\\theta}_{ML} = \\argmax_\\theta P(\\data \\given \\theta) = \\argmax_\\theta P(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\given \\theta )\\]  Note that the above \\(P\\) is a joint distribution over the data. We usually assume the observations are independent. Thus, we have   \\[P(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\given \\theta ) = \\prod_{i=1}^{N} P(\\mathbf{x}_i \\given \\theta )\\]  We usually use logarithm to simplify the computation, as logarithm is monotonically increasing. Thus, we write:   \\[\\mathcal{L}(\\data \\given \\theta) = \\sum_{i=1}^N \\log P(\\mathbf{x}_i \\given \\theta )\\]  Finally, we seek for the ML solution:   \\[\\hat{\\theta}_{ML} = \\argmax_\\theta \\mathcal{L}(\\data \\given \\theta)\\]  If we know the distribution \\(P\\), we can usually solve the above by setting derivative of \\(\\theta\\) to 0 and solve for \\(\\theta\\), that is,   \\[\\frac{\\partial L}{\\partial \\theta} = 0\\]  Maximum A Posteriori (MAP)   In MAP, we maximize \\(P(\\theta \\given \\data)\\) directly. Denote the MAP hypothesis as \\(\\hat{\\theta}_{MAP}\\), we have:   \\[\\begin{array}{rl} \\hat{\\theta}_{MAP} = &amp; \\argmax_\\theta P(\\theta \\given \\data) \\\\\\\\  = &amp; \\argmax_\\theta \\frac{P(\\data \\given \\theta)P(\\theta)}{P(\\data)} \\\\\\\\  = &amp; \\argmax_\\theta P(\\data \\given \\theta)P(\\theta) \\end{array}\\]  Note that the last step is due to the evidence (data) \\(\\data\\) is constant, and thus can be omitted in \\(\\argmax\\).   At this step, we notice that the only difference between \\(\\hat{\\theta}_{ML}\\) and \\(\\hat{\\theta}_{MAP}\\) is the prior term \\(P(\\theta)\\). Another way to interpret is that we consider \\(MAP\\) is more general than \\(MLE\\), as if we assume all the possible \\(\\theta\\) are equally probable a priori, e.g., they have the same prior probability, or uniform prior, we can effectively remove \\(P(\\theta)\\) from the MAP formula, and it looks like exactly the same as MLE.   Finally, if the independent observation holds, again we can use logarithm and expand \\(\\hat{\\theta}_{MAP}\\) as:   \\[\\begin{array}{rl} \\hat{\\theta}_{MAP} = &amp; \\argmax_\\theta L(\\data \\given \\theta) \\\\\\\\  = &amp; \\argmax_\\theta \\sum_{i=1}^{N} \\log P(\\mathbf{x}_i \\given \\theta ) + \\log P(\\theta) \\end{array}\\]  The extra prior term has the effect that we are essentially ‘pulling’ the \\(\\theta\\) distribution towards prior value. This makes sense as we are putting our domain knowledge as prior and intuitively the estimation is biased towards the prior value.   Naive Bayes Classifier   Assume that we are given a set of data \\(\\data\\), where each example \\(\\mathbf{x_j}=(a_1, a_2, \\ldots, a_n)\\),  which can be viewed as conjunctions of attributes values. \\(v_j \\in V\\) is the corresponding class value. Using MAP, we can classify an example \\(\\mathbf{x}\\) as:   \\[v_{MAP}=\\argmax_{v_j\\in V} P(v_j \\given a_1, \\ldots, a_n)\\]  The problem is that it is hard to find a joint distribution for  $P(\\mathbf{x} \\given \\theta)$. If we use the data to estimate the distribution, we typically don’t have enough data for each attribute. In other words, the  data we have is very sparse compared to the whole distribution space.   Naive bayes makes the assumption that each attribute is conditionally independent given the target class \\(v_j\\), that is,   \\[P(a_1, \\ldots, a_n \\given v_j) = \\prod_{i=1}^n P(a_i \\given v_j)\\]  which can be easily estimated from the data. Thus, we have the following naive bayes classifier:   \\[v_{NB} = \\argmax_{v_j \\in V} P(v_j) \\prod_{i=1}^n P(a_i \\given v_j)\\]  Note that the learning of naive bayes simply involves in estimating \\(P(a_i \\given v_j)\\) and \\(P(v_j)\\) based on the frequencies in the training data.   Normally the conditional independence assumption does not hold, but naive bayes performs well even if so. More importantly, when conditional independence is satisfied, Naive Bayes corresponds to MAP classification.   Conclusion   MLE, MAP and Naive Bayes are all connected. While MLE and MAP are parameter estimation methods that returns a single value of the paramter being estimated, NB is a classifier that predicts the probability of the class that an example belongs to. We also have the following insightes:      Given the data, MLE considers the paramter to be a constant and estimates a value that provide maximum support for the data.   MLE does not allow us to ‘inject’ our beliefs about the likely values for the parameter (prior) in the estimation process.   MAP allows the fact that the paramter can take values from a prior (non-uniform) distribution that express our prior beliefs regarding the paramters.   MAP returns paramter value where the probability is highest given data.   Again, both MLE and MAP returns a single and specific value for the paramter. By contrast, bayesian estimation computes the full posterior distribution \\(P(\\theta \\given \\data)\\).   Thoughts   After reading this article, I have the following interpretation:      The Maximum Likelihood approach can be roughly regarded as traditional “frequentist” thinking.   The MAP approach is a direct applicatin of Bayes Theorem. Thus, it can be regarded as a “bayesian” way of thinking.   ","categories": ["machine learning"],
        "tags": ["probability","maximum likelihood","MAP","naive bayes","math"],
        "url": "https://www.ivanxiao.com/2014/09/24/notes-on-maximum-likelihood",
        "teaser": "https://www.ivanxiao.com/images/post_math.jpg"
      },{
        "title": "精英主义与投票考试",
        "excerpt":"题记  有感于最近香港要求普选的民运，心中大概有点想法。     考虑到自己对政治的认识与时间的限制，为了防止与 别人无谓的争辩， 不想把这些想法发到社交网络上，毕竟，这只是个人的想法，或者更严格来说， 是一种理念。因为很多主义与思想，都没有绝对的对错，我们只能做到根据自己的人生观与思想来 做出取舍。而偏偏这个世界上又很多人喜欢跟别人 argue，一定要说服别人为止——我根本没有这个时间 与精力去辩论。另外，这也同时印证了我为什么是 pro-精英主义的，我深知自己对这些事物的认识 不够，所以自觉需要更严谨的学习才有资本与别人争论。我相信多年以后，自己的看法一定又会 有所改变。彼时回首看这些文章，一定会觉得自己幼稚。不过无论怎么说，还是记录下自己的想法。   精英主义与民粹主义   我对这件事情的看法很复杂——我对于民运人士的诉求部分支持又带有保留。争取投票权与心中的理想 当然是一种很好的东西，这点我是支持的。但是普选是否就意味着民主？更大的两个问题是，           普选就能保证选出来的人代表民意吗？ 这就相当于是多数人的暴政。对于这点，已经有很多论述。 这不是我的主要主题，在此不述。            如何保证选民投票的质量？ 很显然很多人限于知识水平，也许只能看到对自己最直接的利弊影响， 而并无法理解哪位候选人的纲领政策对社会，对国家，对世界的更深层次影响，反过来说，这些影响 也许反而会给自己带来并不那么直接的有害效益。 同时，没有自己独立思考过的选民，往往对候选人的印象仅仅来源于道听途说以及政治宣传，甚至 金钱操作，因此这些投票是受到影响的。而『知道自己在做什么』的人实在太少了，这一点在我 求学生涯中越来越明显越来越深刻。我见过太多例子，并不是表面直觉看起来那么简单，经过 深层分析抽丝剥茧后，才发觉真相往往是非常 counter-intuitive 的。对于知识水平完全不在 一个层面的人的 bold move，很多时候只能说 『too simple, sometimes naive』。       所以，这些选票，我认为必须是有权重的。   第二点体现了我 pro-精英主义的最重要原因。想对地，支持普选的更偏向于民粹主义。 两者都有他们的 pros and cons。这两篇文章介绍得很详细：      民粹主义和民粹政治是什么？   精英主义的最大问题在哪里？   投票考试   就投票权这一点来说，这两种主义就相当于两个极端：一个是认为选票控制在精英手里，一个是认为 人人有选票。我希望选票最终是要受到限制的，但是这个限制在公平与效益最大化的前提下做到最小。   一个可能的解决方案，就是进行 voter’s test。就好像高考一样，诚然每个人都应该享有教育权， 但是在现实资源的限制下，我们必须用考试这种相对公平的方法来保证我们的资源要用在对的人 身上，对于参考者来说，也算是进行了投入，付出了机会成本，并且能认识到这个权利是要争取回来的。 同时，这个考试，也保证了投票者具有基本的知识修养。另外打一个比方，就是人人都可以享有路权（投票权）， 但是要真正在马路上开车行驶（行使投票权），就必须对他人负责，通过考试来证明自己有资格使用这条路 并且不会影响他人的生命安全（投票时做出理性选择）。这里有篇文章与我观点类似。   这不是一个新的 idea，事实上，美国以前为了限制黑人投票，的确也实行过类似的考试。但那并不是说 这个制度就行不通，只是当时它被 abuse 了。这个考试必须是公平的，比如我们不设任何年龄与学历限制。这篇文章介绍了一些背景，里面提到了一些支持 voter’s test 的人。当然，我再次认识到这只是自己的主观认识，而且没有经过大量的背景阅读，所以很可能 这个制度会有更深层的问题。日后我会继续就这个 topic 展开讨论。  ","categories": ["politics"],
        "tags": ["meritocracy","elitism","populism"],
        "url": "https://www.ivanxiao.com/2014/10/02/jing-ying-zhu-yi-yu-tou-piao-kao-shi",
        "teaser": "https://www.ivanxiao.com/images/post_ballot.jpg"
      },{
        "title": "Watch Review: Rodina R005GB",
        "excerpt":" So NOMOS has this awesome looking Bauhaus style watch: Tangente. Apparently everyone wants one, but not everyone wants to break their bank ($2330 USD).   Luckily, we have an alternative here: Rodina series watches (or with Date).     The Review   Some people think it a copycat watch, while some others think it a homage. The truth is, with price only $139.99, it is  a really affordable price. During Valentine’s, it is on sale with $99.99 shipped. The website frequently has sales. Without further delay, I bought it when it’s one sale. It is shipped from Tianjin, China via EMS, and USPS after it arrives in US. After 10 days of waiting, I finally received it!      It’s funny that as others commented, the soft bag is supposed to be used to hold the watch, but after opening the case, the watch is not wrapped around it.      However, the watch itself is fine and intact. It is wrapped in plastic strips under good protection.      These are all the stuff in the box. I can understand that, as I don’t think Rodina is not any famous big brand. Don’t expect full service inside the box.   This is really how it should come:      It’s transparent back, which is a perfect show-off for automatic watches. As you can see, it’s 5ATM, which is slightly better than 3ATM that a lot of cheap watches have. It is said to have a Seagull ST1713 movement (with date function).      The crown is really a bummer. If you go to look for its pictures online, the ‘R’ will have a blue coating. However, the blue coating on mine is totally off. I wiped them clean instead. You can see there is still blue pieces in the cleaning cloth. Nevertheless, I like the letter ‘R’ in the crown.      The watch dial is unbeatable. Case is made with sapphire crystal, which is astonishing for this price. The hands are in blue dark blue, which looks really cool. The only problem as someone else pointed out is that the fonts does not match: the numbers are sans-serif, while the Rodina letters are serif fonts. Furthermore, ‘CHINA MADE’ sounds weird, although I can understand they may feel proud to say this.      The strap. Leather is of cheap quality as expected. Feels like plastic. However, the making is not bad. The stitching, the holes, are definitely satisfactory. I picked the brown one as I think black is too dressy for everyday wear. I have a skinny wrist, so I almost used the last hole.      This is how it looks like on my wrist. 39mm is a bit small for me. However, the winding piece is not stable enough. It creates noise and you can feel it moving when you move your arm and thus triggering the winding action. The ticking sound is pretty small though, compared to the notorious Timex Weekender.         Conclusion  With lots of imperfection, this watch is still worth buying. I really like it’s style. I wish I get one with roman literals and dates (the one with Roman literal does not have dates). I wish the leather is of better quality. But with this price point, one cannot expect too much, and the quality of the watch itself is really good. Here’s the pros and cons:   Pros:      Affordable price   Good style, similar to the famous NOMOS Bauhaus style   Seagull ST1713 automatic movement   5ATM (instead of 3ATM)   Stainless steel case   Sapphire crystal!   Cons:      Noisy winding action   Low quality leather (notice that the strap is good, only the leather is bad quality)   Fonts in the dial do not match   Blue coating in the crown not printed at the perfect location   Whether it is a copycat or homage is debatable.   In general, my first impression for the watch is good. Might need some more time wearing it to see if , what do you think?   [Update] After several days of wear, I have to say that I really like the style. I also observed that the watch accuracy is excellent: only 2~3 seconds slow.  ","categories": ["fashion"],
        "tags": ["watches","review"],
        "url": "https://www.ivanxiao.com/2015/03/27/watch-review-rodina-r005gb",
        "teaser": "https://www.ivanxiao.com/images/post_watches.jpg"
      },{
        "title": "Popular watches under $150",
        "excerpt":"                  I’ve been looking around entry-level watches for some time. Recently, I answered a question in Zhihu (Chinese Quora), and I feel like to transcribe this collection in my blog.     Dress Watch             Rodina R005, $119 via Amazon        This watch is discussed in my previous blog post. It is a bit debatable as some people think it completely ripoff the NOMOS Tangente. However, considering there so many Rolex Submariner homages, this should also considered a homage. It uses a Seag  ull ST1701 movement, sapphire crystal, and display case back. At this price point, it is really a good deal. Get in on Amazon.             Orient Bambino, $130 via Amazon        I’ve always like the ORIENT logo, much more than SEIKO’s. This watch features Roman literals, blue hands and white classic dial. Amazon             Daniel Wellington, $120 Men        This has been quite popular in China, due to their successful marketing campaign. It is a quartz (Japanese) watch with multiple variation, and looks really good. Get the best looking ones here and here.   Diver’s Watch   Now moves to my favorite category. Rolex designed the true classic look of today’s diver’s watch. However, the retail price for a submariner is probably too high for lots of people. Fortunately, Japanese brands SEIKO and ORIENT provide some affordable options here.             SEIKO skx007        A very popular one. Considered as a poor man’s Rolex. You can get it around $180 at Amazon.             Orient Ray        Also a very popular one. As I said, I really like Orient’s LOGO. I find it far more interesting to see the lions and shields rather than just the word “SEIKO”. $130 at Amazon.             Orient Mako        Ray’s brother, Mako. The watch face is slightly different, thinner hands, arabic numbers at 6, 9 and 12 o’clock. Also around $130 at Amazon.   Field/Flight/Pilot Watch             Seiko SNZG13        One of my favorite. It’s lume is fantastic and got countless compliments. Only $100 at Amazon.             Orient FER2A001B0 Pilot Watch        This is my favorite style. Look at the beautifully designed digits! It’s lume is also awesome: lumes at all numbers! When it’s on sale you can get it at $125.             SEIKO SNK 809        Bestseller in Amazon. Come with various colors (black, green, blue, beige). It’s so cheap ($50) that people are getting all colors and rotate everyday. Also, since it’s so affordable, it will become your workhorse. You won’t treat it like million dollar watch that only put in your locker.   Skeleton Watch             Seagull M182SK        Quite popular in watchuseek forum. Very nice seagull movement with a complete see-through design. That said, it might be too busy to see the hands. $135 at Amazon.             Fossil Grant        This skeleton is cleaner than the Seagull’s. Nevertheless, Seagull is more reputable than Fossil in watch making. Amazon.   Conclusion   So there you have it. These watches are very affordable and have the great quality that earn them reputation. You can see them in nearly every watch forums. If you just started watch collecting, consider getting one of them. Let me know if you have any other suggestions.  ","categories": ["fashion"],
        "tags": ["watches"],
        "url": "https://www.ivanxiao.com/2015/05/04/popular-watches-under-150",
        "teaser": "https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/watches/top10.jpg"
      },{
        "title": "美国留学：学生保险拔智齿",
        "excerpt":" 在快奔三的年纪，终于长出了最后一颗智齿。     本来想着说这辈子不用拔智齿的——因为我25岁前就已经长完了三颗智齿，并且都 align 得非常好，并没有阻生。之前在中国时的医生也告诉我，没有必要拔掉，因为他们出来得迟，因此比起其他牙齿也会掉得迟。所以以后牙齿掉完后，他们还会在那里，可以做到固定镶牙的用途。当然我在美国没有听到有医生这样提起的，而且美国的习惯是 4 颗一起全部拔掉，一了百了！听起来有点 scary，只是他们的考虑也不是没有道理：智齿一般会长歪，即使没有长歪，也可能引发各种疾病，比如由于长得比较靠里，比较难清洁到，因此很容易蛀牙。更甚的是，蛀牙后，会引起正常的牙齿也蛀牙！这个情况就发生了在我身上：我的下面两颗支持都有蛀牙，并且引起了旁边的蛀牙！为了这个问题，我已经补过两次牙齿。这成为了我这次拔掉他们的最大原因。   我的最后一颗智齿是左上那颗，长歪了，好在不是顶着别的牙齿，而是横向的生长，朝着舌头方向长出来了。所以舌头蹭到很不舒服。大概5个多月前刚长出来时去检查过，还拍了 panarama 的 x-ray （学校的 dental delta 保险三年只 cover 一次，这次只能自费，花了大概 70 刀）。但由于只是刚长成，并不清楚趋势，因此我也带着侥幸的心理没有理他（考虑到别的三颗都乖乖的直直地长了出来！）。最近终于把找工、TA、论文的事情忙得差不多，感觉不拔不行了，赶紧联系医生。可惜之前去的 dental clinic 不能做 oral surgery，于是医生 refer 我去当地最大的 local hospital：carle。总结起来，大概 timeline 是这样的：   12月份看的牙医，拍 X-ray，医生诊断需要拔智齿，开 referal paper 让我去 carl 做『oral surgery』。我偷懒没有马上去做。   5月初打电话给 carle schedule 了一个 appointment。护士告诉我让 clinic 发 x-ray 给他们。于是我打电话让 clinic 发，他们说会在我 appointment 前发过去（结果后来没发！）   过了一个星期去做了 examination，竟然是免费的。护士帮我一检查，发觉 x-ray 竟然没发过来。于是我马上电话过去让他们发。护士和医生很好人，说如果他们不发，给我免费做一个。等了好久，终于给我发了过来。医生也是顺理成章地问了下我情况，问我要不要做 sedation，全身麻醉 (anesthesia，Dental IV)，还是做 local 的局部麻醉。作为一个小白，当然是咨询医生意见。医生说大家都做 IV，睡一觉醒来就拔好了。于是我就说好我也要（只是没想到后来麻醉竟然要 375）。由于我以为我保险马上要过期，于是眼泪汪汪地让他给我这周马上做手术。他们非常 accomodating，愣是给我 schedule 到了第二天。拔智齿在美国算是小手术了，因此要签一张 paper，类似于 disclosure。走之前，我拿着手术清单去 patient account 查价 （check against insurance），但是前台阿姨说不能马上知道结果，只能给我第二天打电话。回到家后，我对着 bill 大概估算了一下，一颗牙 285 x 4 = 1140 + 麻醉 375 = 1515，而保险的 upper limit 是 1000，即使 100% cover 也要自付 515。感觉还是太贵，于是打电话问他们能不能取消掉。得到的答复是，我必须跟医生商量。但是医生一直 busy，一直没有给我答复。直到他们下班前，护士才电话我说，第二天跟医生讲就好。整个流程持续了大概 2 个多小时，花了 3 刀 parking。   第二天早上收到医院电话，得到具体 insurance 的信息以及bill。我掐指一算：一颗牙 285 x 4 = 1140 + 麻醉 375 = 1515，但是保险 upper limit 是 1000，而且不！包！麻！醉！这样 quote 下来我的价格是 (285x4 - 45) * 30% + 375 + 45 = 748.5，其中 45 是保险的 deductible。并且由于 carle 不属于 delta dental 的 network，保险只 cover 70%，which means I pay 30%! 太贵了。于是我果断选择了只做局部麻醉，因为是免费的。这样下来就是我的 out of pocket 是 373.5, which is not too bad。果然穷人多受罪…… 不过换个角度想，全身麻醉怎么说还是比较伤身的，没用也好。不过为了预防万一，我还是做好了全身麻醉的准备。根据给我的资料，全身麻醉需要在术前8小时不能进食，任何水、candy、gum都不能吃！（据说是为了防止你大小便失禁？）由于我手术是下午1:45开始，那么算起来我最迟也要在凌晨 5 点时吃东西……要那么早爬起来准备+吃，还是算了吧。我昨天晚上是8点吃的晚饭，这样算起来我有将近 18 小时滴！水！未！进！T_T 并且由于全麻带来的作用，术后人会不清醒，还必须有人带你去医院负责把你运回家，非常严肃，当然也非常麻烦。   下午开车到了位于 champaign 的医院（carle 在 urbana 和 champaign 各有一间，医生轮岗）。check-in ，乖乖地贡献 373.5 后等了半小时进手术室。总的来说护士们都很 caring，会一直跟我聊天，也很详细地给我解释术后的一些注意事项。不久后，医生助手进来给我打麻药。老实说，整个过程中最痛的其实是这个环节，因为他们会给智齿附近的 gum 打针，扎进去恐怖+疼的感觉，一半一半吧。整个人也会不自觉地绷紧了，双手牢牢地按住自己大腿。不过其实局部麻醉对我来说也是常事了，之前补牙时也做过，所以多少有点心理准备。不久后，整个嘴麻了，连吞口水也感觉有点困难。   医生进来后手术正式开始。新出来的那颗（左上），我估计 5 秒就拔出来了，一点感觉都没有。医生会 count：this is one。我当时心里一阵窃喜，原来这么水啊！可惜如意算盘打错了，拔下面那颗时，我感觉整颗死死地嵌在牙腔里，怎么也拔不出来，牙根处有点隐隐作疼，但更疼的，是我的下巴。因为作用力太大，我感觉整个下巴都要被扯起来了…… 也许是因为太牢固，也许是因为补过牙，医生决定把它钻开再拔。于是就听到了钻头在口腔里巨大的声音。钻好后，医生估计是一片一片地拔出来的，其实我感觉不到具体手法，只知道他们一直在里面捣鼓。期间医生也会预警：你接下来会听到一些碎裂的声音…… 右上的智齿虽然也是 fully erupted，但是比起下面的好拔多了，虽然不如左上的容易，但是不需要钻就拔了出来。右下的是最 hard core 的，不仅要钻，而且钻完了也花了好大功夫才解决。估计下面的开口比较严重，所以医生都给我缝合了。没感觉到他给我穿针，但是能感觉到他拉线和打结时把牙肉缝起来。其实真正拔牙的整个流程估计不超过10分钟？老实说也没有网上写的那么恐怖，可能是我身体素质比较好吧 :)   完了后做起来歇息了一下便走了（说到这，我都佩服我自己了，没有过敏，没有烟酒，没有遗传病，没有重大病史，各种指标正常，拔牙不叫不闹，估计医生也喜欢我这种病人吧…… 感谢上天感谢爹娘给我的强壮体魄~）。回家后一直要咬着纸巾止血。大概3-4个小时候麻药效果过去了。根据护士说法，我可以服用一些 ibuprofen （美国常见止疼药）来止疼。把纸巾取出来后的大概半个小时，真的很疼。小时候牙疼的那种感觉又回来了。不过好在半小时过后就没怎么疼了，也许是止疼药功效上来了，也可能伤口愈合得好。总的来说，还是没什么大问题。   接下来要做的，就是看它恢复得如何了。刚拔完自然是不敢吃比较硬的食物的。乖乖地给自己熬了粥喝。希望身体赶紧回复过来，一周后还要去 DC 和 NY 旅行呢~   后记  过了两周，总算没那么疼了。牙线也开始脱落，能正常吃东西了。我觉得我恢复得还是不错的，没有需要用上医生开的处方药，只是用 OTC 的 ibuprofen 即可止疼。只是每天早上起来时会比较疼，因为平躺着会让它充血。   PS. 如果有在 UIUC 的朋友也想去拔智齿，可以先去 Creative Smiles Dental （在 marketplace 那边）检查。没有租生的智齿它们是可以直接拔的。提我的名字（说我 refer 你）可以每人各拿 $50 的优惠 :)      ","categories": ["health"],
        "tags": ["wisdom teeth","dental","insurance"],
        "url": "https://www.ivanxiao.com/2015/05/15/study-abroad-wisdom-teeth-removal-with-student-insurance",
        "teaser": "https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/2015-05-14-study-abroad-wisdom-teeth-removal-with-student-insurance.jpg"
      },{
        "title": "在美國換輪胎的經歷",
        "excerpt":"最近給車換了輪胎，發覺學問非常大，決定把經歷記錄下來。     背景  我開的車是 MB-C300。買完後才發現，自己的輪胎是所謂的 “runflat”。這東西的好處是，即使是爆胎了，它的 sidewall1 足夠堅硬，所以可以支持已經失去氣壓的輪胎在 50mph 的速度以內再走 50 miles。而壞處就是…… 幾乎沒法修，一旦損毀，基本上只能馬上趕去修車店，而且由於算是比較新的技術，不一定馬上有貨；而且造價一般也比較貴。由於 sidewall 硬，一般也會使得駕駛非常 bumpy （這點我深有同感，車子開起來非常顛簸，感覺一直在抖，但是好處是感覺非常敏感抓地，飆起來時很有 sporty 的感覺）。最後，一般帶了 runflat 的車子，都不會再配給備胎，所以如果爆胎了，那麼只能乖乖去修車店了，當然，好處就是少帶了一個備胎，trunk 可以省出很多空間。除了以上這些，根據我對我自己輪胎的觀察，還發現一到冬天，胎壓降得特別快，基本上2個月就要去打一次氣。   上面說的那些，在我開了五年的舊車上從來沒有體驗過，因爲一般我都定期半年保養，而胎壓什麼的都應該在保養時搞定了。這也間接導致了我對輪胎非常小白，因爲舊車的輪胎似乎非常耐擦，我只是在買回來後換過一次，接下來五年關於輪胎的問題真的一次沒遇過。   Runflats  說起 runflat，可真是多牢騷。由於我的車買的是運動版，配了 19’ 的 wheel 以及 Pirelli P-Zero Summer runflats。開車至今一年半，我的左前胎就壞過兩次[^flat tire]。好在都在保修期內，所以基本上只花了幾十塊免費換新胎2。但是這一來一去也是非常花時間的，還不能開車上班。   最近，我發現我的右前胎的外側基本上已經磨平了。剛發現那會非常震驚，因爲我的車才開了 15k miles 不到，竟然就快掛了。後來經過種種網上的研究，這款胎的確壽命就是很短！因爲 Pirelli 是意大利專給跑車做 performance tires 的…… 所以比較 prestigious，而這一款又是特別的薄，所以網上很多人都說甚至不到 10k 就要換了。   選擇新輪胎  網上做了大量功課，發現原配 runflat 的車裝 non-runflat 也是可以的。想起之前的種種麻煩，斷然決定還是入手 non-runflat。在 tireack 上做了功課，基本上瞭解了有幾個考慮：      Summer vs. All season: summer tires。 適合在溫度不低於結冰的地區使用，在乾溼的路上，防滑與操縱都很好。其實，基本上就是加州這種地方。但是，在雪地上會非常危險。All season 顧名思義，犧牲了在普通路面行走的性能，使得可以在雪地上行走。   壽命。根據 UTQG3 的定義，我原車胎的 treadwear 才 220! 而大衆喜聞樂見那些輪胎，則一般都能上到 500，也怪不得我以前那部車開了那麼就一點問題也沒有。   價格。這個其實問題不大，因爲我原裝的輪胎一個要上 $400 … 基本上任何輪胎都會比它便宜。   tirerack 的界面做得很好。在它的幫助下，我基本上鎖定了以下這幾款：      MICHELIN PILOT SUPER SPORT (PSS)。這是大家有口皆碑的 performance summer tires。基本上對車有追求的都會推薦這款。缺點是可能太 sporty，並且 treadwear 比較低。   MICHELIN PILOT SPORT A/S 3+。根據網上評論，跟 PSS 其實差別不大，會稍微舒適一點。而且很 tricky 的一點是，雖然評級是 all season，但是性能跟 summer tire 很接近。最大的優點是，treadwear 非常好。   Continental EXTREMECONTACT DWS 06。也是評價很不錯的，但是比起上面那兩款來說則是比較中規中矩沒有太多兩點。   優劣很明顯了，我很快就決定了入手 PS3+。這裏要再表揚一下 tirerack，線下功能也做得非常好。選好了 tire 後，還能寄到指定的 tire shop 進行安裝，並且價格也是有保證的。根據它的推薦，我很快發現了 Nielsen Automotive。這家店在 yelp 是100+的五星級review。而且後來也證實，他們的服務非常靠譜！（見下節）   網上購買輪胎與比價  tirerack 給定的價格還是非常好的。我一套輪胎下來，才不到 800 刀。相對來說，costco 的價格更低，但是仔細研究後發覺，他們的 service 口碑非常差。這也是我最終決定選擇 tirerack 的原因。   一切準備就緒後，我就在網上下了 order，讓他們寄到 Nielsen Automotive。這裏有兩個小插曲：      我的車原配輪胎是 front 225 rear 255 的 (tire width)，而這款輪胎的推薦配置是 225 + 245，或 235 + 255。tirerack 的 salesagent 電話專門跟我確認後，決定還是要 225 + 255 的   電話裏，sales rep 還跟我提到如果我 hold order 兩天，那麼將會 qualify 原廠的一個 rebate，價值 $70 的 prepaid card。tirerack 又再次立功了，這點我非常感激。   安裝輪胎  輪胎是從 NV 寄過來的，週三寄出週四就到了。週四下午我接到 Nielsen Automotive 的電話後，約好週五下午去修。他們的報價是 $100，比 tirerack 上推薦的還要便宜 $20!   週五去到店裏，發覺是一個外表非常老舊的店。修車的也都是老頭子。但是！老頭子們都非常非常 nice &amp; gentle，讓我感覺非常好，沒有那種普通修車店那種笑里藏刀的感覺。我一開頭還不相信，知道最後面修好車發現，他們是真真正正的 nice，一點也沒坑我。說好了一個小時最後其實花了兩個小時，修車技師 Chris 主動給我道歉並解釋說這個系列的車比較難，有一個地方他總也搞不定，所以花費了更多的時間。老先生很誠懇，我當然也不好意思多說什麼。開出去走了一圈，發現輪胎氣壓不是非常對稱，讓 Chris 再幫忙 balance 一下，也是二話不說就幫我弄好了。當我準備給他一點小費時，他還非常禮貌地拒絕了。不收小費，這我在美國還真是第一次見到。   吐血推薦這間 tire shop。地址是 888 El Camino Real, San Carlos, CA 94070。   總結  經歷過這次讓我對輪胎的維護與保養有了比較深的認識。tirerack 價錢合理並且服務也好，更重要的是上面有大量的科普文章，非常有營養。                  指的就是輪胎的側面，起的支撐作用。 &#8617;                  一般輪胎都自帶 warranty，比如一年內，以及車輪磨損在接受範圍以內，遇到任何 road hazard 以及質量原因，廠家包換。 &#8617;                  Uniform Tire Quality Grade &#8617;           ","categories": ["automotives"],
        "tags": ["tires","maintenence","cars"],
        "url": "https://www.ivanxiao.com/2017/04/02/zai-mei-guo-huan-lun-tai-de-jing-li",
        "teaser": "https://www.ivanxiao.com/images/post_ferrari.jpg"
      },{
        "title": "Migrating from dropbox public folder",
        "excerpt":"Dropbox recently removed its public folder feature, which was used to host my blog’s images. I have to figure out a solution to host and fix the paths to these images…     Dropbox announced that As of March 15, 2017 the Public folder in your Dropbox account has been converted into a standard folder. Sad, all my previous links are hosted on Dropbox, and they all become invalid URLs.   I figured that I have to use another image hosting service. After some searching online, Cloundinary seems to be a good option. One design of my hosted images are that they are organized in sub-folders under a ‘blog’ folder. This means that if I want to seamlessly convert the links, I need to preserve the folder structure too.   Cloudinary seems to suggest they support auto-creating folders. Unfortunately that does not quite work for me.      In fact, I wrote a small script to do this.   # Upload a folder to cloudinary import os import cloudinary import cloudinary.uploader import cloudinary.api  # Upload the folder in rootdir to cloudinary, preserving the file structure  cloudinary.config(   cloud_name = \"&lt;name&gt;\",   api_key = \"&lt;your key&gt;\",   api_secret = \"&lt;your secret&gt;\" )  rootdir = 'blog' for root, subs, files in os.walk(rootdir):     for file in files:         cloudinary.uploader.upload(             os.path.join(root, file),             folder=root, use_filename=True,             unique_filename=False, resource_type='auto')  With that, I did a simple sed run on all my post sources:   sed -i '' 's/&lt;dropbox base url&gt;/&lt;cloundinary base url&gt;/' *.markdown  then rake preview. Boom! All the images are shown again!   ","categories": ["octopress"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2017/04/03/migrating-from-dropbox-public-folder",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "Making an auto insurance claim after got hit by road debris (flying object)",
        "excerpt":"Just a normal day of going to work. Bang! I got hit by a flying object … :(     How it happened  This video best explains it:               Getting a quote  While I am pretty sure that this should be a comprehensive claim, I wanted to avoid it if this can be repaired under a few hundred dollars. The common saying is that comprehensive claim will not increase your premimum, but there is also saying in the web that said if you claim three times in a three year period, it might. I went ahead to get quote from two shops, and turned out that it is way more than I expected: they range from $1800 to $2200!.   First, let’s take a look at my damanges:                                                                        The scratches are all over, and the most severe one, as shown in the first image, is that my base layer seems to be damaged.      Depending on different level of damage on the layers, including base, prime, base (paint) and clear coat layer, this is the hardest one so it actually takes quite a lot money to fix. For a simple clear coat damage, it is easy enough to DIY, and I’ve found some awesome tutorials online, including this and this. There are also a bunch of videos online.   Unfortunately my problem is way over that, and I guess I do have to make a claim. I do need to pay $500 deductible, so the insurance will actually pay $2k-ish minus deductible. The good thing is that while my car is being fixed, I still got a ride from Hertz, paid by my insurance company.   Making a claim  Making a claim is easy. Just called the number and make the case. A case number will be created and an insurance adjuster will be assigned. After discussing with the adjuster, I can take the car to the body shop and an appraiser will come and do an inspection.   Getting the paint repaired  Well, just drive the car to a body shop, share your insurance info and let them get approval from the insurance company and bill to them. As a unspoken rule, they typically jack up the price if you have insurance. You can pay less if you decided to pay cash yourself. Unfortunately, even though going through the insurance route does not affect your out of pocket (you just pay the deductible), it will certainly increase your insurance rate more over the next couple years when the payout is higher. Essentially you are paying the increased amount via your premium in installments.   This is really unethical practice across the insurance industry in the US IMO. The worst place happens in medical bills. You literally see thousands of dollars billed to the insurance, while you can typicall negotiate a lower price if you don’t have insurance.   Conclusion  My overall experience with comprehensive insurance has been very good. I have claimed it a few times to repair not-at-fault damages, and in fact it already paid off my insurance premimums paid these years… However, I do have to pay deductible per-case basis, which is non-trivial also (except glass claim). Getting a zero-deductible claim will increase the premium a lot, so it seems $500 deductible is the norm.  ","categories": ["automotives"],
        "tags": ["auto insurance","claim","comprehensive insurance"],
        "url": "https://www.ivanxiao.com/2017/04/30/making-an-comprehensive-insurance-claim-after-got-hit-by-road-debris-flying-object",
        "teaser": "https://www.ivanxiao.com/images/post_bills.jpg"
      },{
        "title": "美國租車陷阱：Toll Charges",
        "excerpt":"今天收到了一張大概兩個月前的 bill，說我欠款 $30 美金……   （如果你看到這篇文章，並且知道最新進展，請務必留言讓我知道。）        兩個月前，我因爲車子被 flying debris 刮傷 hood，而拿去了修車店修理。同時保險公司也 cover 我租車的費用。於是，在 Hertz 租了一輛小車。   由於我上班下班都會經過 Dumbarton bridge，所以單程會被收取 $5 費用。由於我自己有 fastrak，所以都是過橋時拿出來使用。但是有一次過橋忘記了。當時想着，也許跟之前一樣，會寄賬單到家，所以也沒多想。直到今天看到賬單，才大吃一驚：你收取我原價 $5 的過路費，我認了。但是由於我只是一次沒有交，而收取我整個 rental period 的 service charge，每天 $4.95，封頂 $24.75。這多收的服務費，都已經是我的過路費的5倍了。這未免太坑了點？？   仔細想想，這家名爲 PlatePass 的公司，實在是太陰險毒辣了。賬單在我結束租車後近兩個月後才寄出，此時早已經過了保險 claim 完成的日子。這間狠毒的公司，說白了就是躺着就把錢給賺了。   仔細上網搜搜，我不是第一個人。網上早已經有N多的抱怨。不少的文章也披露了這些公司無賴的行徑。可惜的是，我並沒有看到操作性太強的 argue 方法。只能吃一塹，長一智，以後一定要廣而告之，揭發這些無恥商家的醜惡行徑。文章裏面給出的建議是：      遇到 toll bridge/road，不要走快速通道，走現金通道。   自帶 toll transponder   設 gps 時，avoid toll roads。   希望下次大家不要再犯這樣的錯誤了。    更新：看到有文章介紹說，三藩市已經有律師在告類似公司的 shady practice 了。希望能告贏。其實以前就已經有一個 class action suit ，不過已經是好幾年前的事了。非常震驚他們竟然還在繼續下去。  ","categories": ["automotives"],
        "tags": ["travel","tips","rental cars"],
        "url": "https://www.ivanxiao.com/2017/08/02/zu-che-xian-jing-toll-charges",
        "teaser": "https://www.ivanxiao.com/images/post_ferrari.jpg"
      },{
        "title": "夏威夷遊記（一） 瓦胡島",
        "excerpt":"借着 CVPR 2017 的機會，終於去了一趟嚮往已久的夏威夷。主要是參觀了瓦胡（oahu）島以及茂宜（maui）島。非常喜歡，嚴格來說是我第一次去“度假”。美好的自然風光非常愜意。趁在在飛機上有時間，可以寫一些流水帳。             Oahu 一景          夏威夷有好幾個島，其中一般遊客會去的島有 Oahu, Hawaii (大島，big island)，Maui，Kauai。一般會有兩種玩法：Island Hopping，或者集中參觀1-2個島。由於這次的假期包括開會在內大概只有7天左右，所以我安排了2天 Oahu 以及5天 Maui。這次行程的規劃（以及施行）還算不錯，算是比較深度地玩了兩個島。路線大概是：   Oahu  主要是參觀了幾個主要景點以及環島自駕。住在 Honululu。到 Waikiki 的 Avis （Sheraton 裏面）租了敞篷跑車。   第0天  SFO - HNL，入住在 Waikiki Marina Resort。這是一個 condo 型的旅店，與隔壁似乎比較高級的 Ilikai 酒店 share 同一個 building，但是因爲是 condo 型的其實我們覺得住得也算很舒服，因爲沒有人管你（包括 house keeping），樓下就有各種超市，下樓就是海灘，每層都有洗衣機和乾衣機自給自足。當然最主要還是有廚房，我們可以偶爾自己做飯，一來健康，二來省錢 :)   第1天  環島自駕。主要是從 Waikiki Beach 開始，逆時針地繞着島轉了一圈。沒有去西邊，而是從北邊直接開回來的。一天如果早點出發是完全足夠的，只是我們比較 laid back 所以到後來有點倉促，從 north shore 開始開回來時已經天黑了。值得一提的是環島遊租敞篷真的很爽！雖然我們租的時候可能算是旺季，價格特別貴大概80一天，但我們還是不後悔的，因爲實在是太過癮了：夏威夷的氣候非常適合敞篷，溫度不冷不熱，風吹過來是一種很涼爽的感覺，而不像北加那樣是冰涼冰涼的，或者像別的內陸城市那樣風也是熱的（其實 Maui 也是潮溼悶熱）。另外一定要做好防曬。租車是在 Waikiki Avis，本來預訂了一部基本的 Mustang，Agent 問我要不要加一點錢 Upgrade 成 GT。當時還是蠻興奮的，結果他說弄錯了，GT 以及租出去了… 但還是 Upgrade 成了一部配置好一點的 Mustang。   環島遊很棒，我們從 Waikiki 開始，經過了 Diamond Head State Beach (不是 park，這個是在南邊的沙灘)，恐龍灣，然後一直繞着海岸線直到 north shore。路上風光非常漂亮，以至於我們停在了太多地方拍照而後面太趕。其中恐龍灣我們並沒有去浮潛，而是進去就出來了，因爲要收門票 :/ 我們打算第二天才去，所以直接 skip 了。             環島路線，主要參考了這篇                  租的 Mustang                  Diamond Head State Beach                  Makupuu Lighthouse                  在飛機上拍的 Waikiki Beach                  Lāʻie Point State Wayside                    第2天  參觀幾個主要的景點，包括珍珠港，鑽石頭山 Hiking，以及恐龍灣浮潛。   珍珠港想必不用我多提。最出名的景點是 USS Arizona 號，是日軍偷襲珍珠港當天直接炸沉的一艘戰艦，船上近千人直接喪生！這艘船的殘骸一直留在珍珠港，部分還露出水面。政府在上面建了一個類似紀念堂的小建築，需要從珍珠港坐船前往。值得一提的是這個船是可以提前預訂的。門票免費，只是收一點手續費。但是由於太 popular，一般早早就預訂完了。不過，提早 24 小時還會有一批票放出，我們就是提早一天搶到的票。當然也有一定的 walk in 名額，但可能會很浪費時間。   在上船前，我們會在一個劇院觀看大概10分鐘的歷史回顧。個人認爲，這段視頻拍得非常好，引用了大量的照片與視頻還原了原來的歷史事件，並且沒有加入太多愛國主義或者主觀感情，而是以客觀的態度回顧歷史，讓人感慨戰爭的殘酷。根據太太說，現場有不少男士都流下了眼淚——也包括我。現在每每想起來，也總是覺得眼眶溼潤。真正參觀紀念館時，倒是沒有那麼觸動。只是想到陣亡將士的屍骨至今扔留在深海，心裏不免會沉重。   珍珠港比較出名的還有密蘇里號。當時日本投降時，天皇就是在這裏寫下的降書。別的還有一些博物館以及潛艇，但我們沒有具體參觀。             USS Arizona                  模型，可以看到這個紀念館是建在殘骸上的                  部分殘骸露出了水面                  陣亡將士紀念館里的石碑，他們的名字被銘記在這裏……                  USS Bowfin        下午我們爬了鑽石頭山。這裏能俯瞰整個 Oahu 島，特別是 Waikiki。這其實是比較累的一段 Hike，因爲幾乎都是 unpaved road，elevation 也不低，並且中間還有幾段沒有燈的 tunnel。山上風光異常漂亮，就是那種你會在視頻，文章裏面會看到的那種熱帶風光。而這個鑽石頭山，本身的 highlight 是作爲一個火山爆發形成的 Crater，倒是比較 unimpressive，可能是因爲這個高度並不是特別能看出它的地貌。不過，後來我們在 HNL - OGG 的飛機上，有幸錄到了一段視頻而得以一窺它的全貌，這時大自然的鬼斧神工，老天爺的饋贈，才得以呈現。               最後的行程是恐龍灣浮潛。說來慚愧，這是我的第一次浮潛，所以並沒有任何經驗。但是根據接下來幾天的浮潛經歷來看，我認識到恐龍灣的確是一個非常漂亮的浮潛聖地。因爲它的礁石就在岸邊，連新手如我都能一窺究竟。比起 Maui 基本上中午開始就狂風大浪，恐龍灣的浪是只要會游泳的人就能接受的。可惜的是，我們還是太拖沓，到了恐龍灣大概已經是5點，而真正到沙灘前又先要排隊觀看一個介紹視頻，並且還要步行一段距離，浪費了至少30-45分鐘。這時太陽能照耀到的海面，基本上已經不多了，能看到的景色自然遜色了點。加上我第一次浮潛，基本是在學習而沒怎麼觀察景色。             Hanauma bay        第3天  這天我們乘坐了 Island Air 的小飛機前往 Maui。有趣的是，這班飛機在 HNL 里是被認爲『commuter』，是在 commuter terminal 里登機的，對於通勤的島民來說簡直是打飛的上班。行程非常短，只飛了大概30分鐘就到達茂宜。來之前並不清楚茂宜這個島，來了後，才發覺自己太喜歡它了！   茂宜行程，寫在了下一篇。   夏威夷遊記系列      夏威夷遊記（一） 瓦胡島   夏威夷遊記（二） 茂宜島 Road to Hana, Ahihi-Kinau, Lahaina   夏威夷遊記（三） 茂宜島 Molokini Snorkelling   夏威夷遊記（四） 吃吃喝喝  ","categories": ["travel"],
        "tags": ["oahu","maui","hawaii"],
        "url": "https://www.ivanxiao.com/2017/08/06/xia-wei-yi-you-ji-yi-wa-hu-dao",
        "teaser": "http://res.cloudinary.com/maomao/image/upload/v1501979601/blog/hawaii/oahu%20roundtrip.jpg"
      },{
        "title": "夏威夷遊記（二） 茂宜島 Road to Hana, Ahihi-Kinau, Lahaina",
        "excerpt":"茂宜島：陽光沙灘與海，雲海聖山與樹，城市人羣與浮潛，茂宜島 has it all! 瓦胡島的行程參見上篇。             OGG 機場          Maui  茂宜島的經歷讓人難忘。沒有來之前，總覺得 5 天是不是太長了。而最終證明，如果你不是特別 hardcore，其實只是剛好而已！ 我們大概的行程安排是：      Road to Hana   早上去 Ahihi-Kinau 浮潛，晚上去 Lahaina 商業街就餐與參觀。   月亮灣浮潛，下午休息（因爲被曬傷了 &gt;.&lt;）   早上去浮潛看海龜，中午去Haleakala National Park。沒有看日出是因爲……見下文   第0天  嚴格來說我們只呆了 4.5 天，因爲第一天是 HNL - OGG。我們到達後，機場租車，然後開去 Walmart 買了一些生活用品以及浮潛用具。我們住在 Kihei，是一個比起西邊少商業化很多的地方，並且跟在瓦胡一樣，也是 condo。但是事實證明，這個地方非常適合我們：恰到好處的商店與人流，門口就是海灘，並且離好幾個浮潛的地方都很近，非常方便。   我們當天下午悠哉悠哉地歇到傍晚才出門與了附近的一個沙灘，結果被告知，在 maui 的最佳浮潛時間，是早上 7am-11am；過了這個時間，狂風大作，海浪太大，一來危險並且海浪把沙都捲起來了，能見度很差，二來魚兒是早上活躍，這時候很難看到什麼好景觀了。於是只好作罷 :(   月亮灣浮潛是當地最火最簡單的一個浮潛項目，於是我們定下了 Redline rafting 這個公司，每人大概 $140 這樣子。比起其他的公司貴了不少，但是好處是，我們乘坐的是小船，不超過 20 人，有比較 personalized service，並且我們早上是第一班出發的，能趕在別人前面把景點都玩了，並且我們這個 trip 會去好幾個地方，包括月亮灣與海龜鎮等。當然，這個主要是他們的說法，我們也沒有跟別的比較 :)   第1天  Road to Hana 是大家必玩的一個行程。這條路大家一般是從 Paia 出發，順時針玩的。其中以 Haleakala NP 爲分界線，或者玩一半，或者玩整圈。很多網上攻略都說後半段路很難開，租車公司協議規定不能開 unpaved road 之類的。但其實這都是一些 rumor。詳細的解釋並且後半段的攻略看這裏。對於前半段，我們主要參考的是這篇。最後證明，只要早上按時間出發並且不要在景點停留太久，一天轉一圈是完全沒問題的。             Road to Hana full loop (map)        老實說，這段路非常 unimpressed，比起我們開過的幾段 scenic road，比如 CA-1, Keywest, Columbia Gorge， 差遠了。前面那段的熱帶雨林氣候，非常泥濘溼熱，讓人很不舒服。路上的一些瀑布，比起別的地方的瀑布遜色不少；如果你不想着下去游泳的話，則更沒意思了……反倒是後面那段山路，有海岸，有峽谷，有落日，我們還比較喜歡（網上也確實不少人喜歡）。             彩虹樹                  黑沙灘                  某瀑布                  椰子雪糕店，還蠻好吃的                  後半段的一景，視野開闊                  這裏竟然有個中山公園！                  日落        第2天   這裏以及這裏推薦了不少浮潛的好地方。因爲 Ahihi-Kinau 離我們很近，並且不少攻略都推薦，於是我們選擇了這裏。這次是第一次正式地浮潛，我們連 fin 都準備好了。綜合我們後面幾次的浮潛，這裏的確是一個好地方，無論是魚類的數目以及多樣性都非常好（只是沒經驗沒看到海龜），鄭重推薦。   由於當時沒有考慮到想要拍照，所以並沒有準備好。回來查了一下，發覺要在水裏面拍照，一般有幾個方法：      傳統的一次性水下相機，比如。缺點是像素低，評價也普遍不好。不推薦。   專用的水下攝像機，或者 gopro 之類的，比如：http://amzn.to/2v8AC1Q。這個應該是質量最好的，但是偏貴，使用頻率不高（除非是 go pro）。   手機防水套，比如 http://amzn.to/2hyXyml。這個比較實用，雖然後來試用發現水下還是光線不夠，手機容易排虛。但是如果只是想拍一下人，還是能拍出來的。而且非常便宜。比如這款在 amazon 只需要5刀。如果是臨時去島上的 ABC store 買，那款當時花了我 20 刀！             剛下水就能看到非常多這種銀色的魚。來源                  這種小黃魚也很多，很有熱帶氣息。來源        早上浮潛是自己認真的第一次（恐龍灣那次算是試水），所以比較累。下午休息後想去西邊，主要是 Lahaina 和 Banyan tree。但是發覺下午交通非常差，原來只需要 30 分鐘的路程，要開 1 個小時！根據後來的觀察，似乎這是常態，因爲到了那邊後，只有 1-lane，只要有一部車開得慢，後面就全堵上了。   Lahaina 跟我們設想的差不多，非常商業化，跟 Waikiki 差不多。對於更希望接觸自然，有不一樣風情的我們來說，並沒有太吸引。而且本身出門就遲，我們到達後，剛好趕上日落，我們便隨便找了家餐廳吃飯。雖然去得晚，並且很少停車位，幸運的是在那邊的正規餐廳似乎都能爲顧客在旁邊的一個 outlet 提供 3 個小時的 free parking。             Lahaina 日落        晚飯後我們散步到了 Lahaina Banyan Court。這裏有一棵美國最大的 Banyan tree，非常讓人震驚。這棵樹的樹枝垂下鬚根接觸到地面後，又會長出粗壯的樹幹支撐樹枝本身，並且汲取水分。只用普通相機根本無法表現出現場那種獨木成林的神祕氣息……             網上找的一張圖片，注意這裏面所有的“樹”都是同一棵。你們感受一下……          茂宜島接下來的兩天的遊記(浮潛 + Haleakala)見此篇。   夏威夷遊記系列      夏威夷遊記（一） 瓦胡島   夏威夷遊記（二） 茂宜島 Road to Hana, Ahihi-Kinau, Lahaina   夏威夷遊記（三） 茂宜島 Molokini Snorkelling   夏威夷遊記（四） 吃吃喝喝   ","categories": ["travel"],
        "tags": ["maui","road to hana","hawaii"],
        "url": "https://www.ivanxiao.com/2017/08/06/xia-wei-yi-you-ji-(er-)-mao-yi-dao",
        "teaser": "http://res.cloudinary.com/maomao/image/upload/v1501985551/blog/hawaii/Ahihi-Kinau.jpg"
      },{
        "title": "夏威夷遊記（三） 茂宜島 Molokini Snorkelling",
        "excerpt":"此文爲夏威夷茂宜島遊記的下部分。             Haleakala 上的雲海          第3天  今天終於到了去大名鼎鼎的月亮灣 (molokini) 浮潛的日子。月亮灣是 maui 西邊的一個小島，因爲長得像一輪彎月而得名。             月亮灣        我們早上6:30am集合，7點就出發。上文提到，這間公司雖然貴點，但是人少出發早；並且提供了一頓早餐和午飯。它們的潛水服和浮潛用具也是包括在內的。               我們一共浮潛了以下幾個地方：      月亮灣正面。這是大家都會來的一個地方。水非常清澈，能看到海面幾十米以下的海底。   月亮灣背面。這裏的海浪沿着月亮灣直入海底的懸崖直起直落，彷彿在坐升降機一般。   La Perouse Bay 某處的珊瑚礁。這裏是觀看熔岩入海點的。   看海龜。             路線                  與魚共舞                  海龜與魚                    總的來說，這個 trip 是第一次來茂宜的人必來的。比較意外的是，這幾個地方其實魚類的多樣性以及密度其實都不如我們昨天去的 Ahihi Kinau，但是深海浮潛的好處是能看到許多在岸邊看不到的景色，以及非常清晰的能見度。這一次的經歷讓我喜歡上了浮潛，下次如果有機會來的話，會想去這裏提到的 Coral Garden 以及 Black Rock。   第4天   我們把最後一天的行程定在了哈雷阿卡那山。這裏有一個小插曲。我們本來是打算按照網上的介紹，凌晨3點出發趕去看日出的。但是臨行前晚上，卻不幸地發現從2017年2月起，必須通過 nps 預約才能進入公園。每天只有大概 150 張票。這麼熱門的景點，當然早早就訂完了，包括旅行社。我們只能臨時決定改去看日落。這麼算起來，中午出發就足夠了，所以我們早上先去了浮潛。經過了昨天的經歷，我們今天浮潛有了更多的經驗。並且昨天我們已經記下了海龜所在的地方，發覺就在我們家不遠處，於是決定再去觀看一回。             大概在這兩點就能看到海龜，離岸邊有點距離，不推薦給水性不好或者第一次浮潛的新手        中午回來休息後，開始向 Haleakala 出發。這裏是當地人心中的聖山，除了被開發爲旅遊區外，還一直充當着夏威夷本地人進行宗教活動的場所。一路順利，我們很快便到達。進入公園的是一條盤山公路。隨着海拔越來越高，我們看到的景色越來越壯闊。到達山頂時，我們見到了非常漂亮的雲海。但是由於一下提升了3000多米的海拔，我們都感到了一點高原反應…… 公園的牌子也提示說，走路行動要悠着點。             我們主要參觀的景點                  瞭望遠方                  我把這張圖片叫做冰與火之歌，因爲站在山脈上，一邊是潮溼而雲霧瀰漫的熱帶雨林；一遍是乾燥而清澈可見的廣袤大地。        等我們把前面幾個景點參觀得差不多後，我們來到了山頂。雖然我們已經提早了45分鐘，但山頂的停車場早已水泄不通（怪不得早上需要預訂，否則必須是一團亂）。等我們停下車後，已經是時間觀賞日落了。可以說這次日落是我在美國看過最好的一次，的確與衆不同。由於這裏特殊的地勢與氣候，氣象幾乎是瞬息萬變的。在太陽的光輝下，浮動的雲不停地變換與閃耀着，甚是讓人心馳神往。             日暮下的天文臺                    日落完後，大家開始陸續離開。按照網上推薦，我們決定繼續留待一會，因爲據說這裏的星空也是絕美的。可惜今晚月亮不是新月，非常光亮，天氣狀況也稍微有點多雲。我們沒有看到特別多的星星或者銀河。   第5天  至此，我們的夏威夷行程告一段落。這裏距離三藩要飛近5個小時，但是已經基本算是距離美國大陸最近的地方。也難怪不少人都稱這裏是加州人民的後花園。夏威夷主要的島嶼還有 Kauai 以及 Hawaii (big island)，留待下一次再相遇。   夏威夷遊記系列      夏威夷遊記（一） 瓦胡島   夏威夷遊記（二） 茂宜島 Road to Hana, Ahihi-Kinau, Lahaina   夏威夷遊記（三） 茂宜島 Molokini Snorkelling   夏威夷遊記（四） 吃吃喝喝  ","categories": ["travel"],
        "tags": ["snorkel","maui"],
        "url": "https://www.ivanxiao.com/2017/08/06/xia-wei-yi-you-ji-(san-)-mao-yi-dao-molokini-snorkelling",
        "teaser": "http://res.cloudinary.com/maomao/image/upload/v1502046918/blog/hawaii/Molokini.jpg"
      },{
        "title": "夏威夷遊記（四） 吃吃喝喝",
        "excerpt":"網上很多（華人的）遊記都提到一個問題，就是夏威夷的餐廳比較少。但根據我們的10天經歷，卻並不這麼覺得。        夏威夷有很多日本人，所以非常適合我們——無論是壽司、刺身、定食、Poke，我們都吃了個爽。             Omusubi Gaba 傳說中的 Gaba 飯糰。這間小店是日本人開的，yelp 評價非常好。                  Omusubi Gaba 傳說中的 Gaba 飯糰。這間小店是日本人開的，yelp 評價非常好。                  最近我們非常喜歡吃的一種東西——Poke。圖爲 South Maui Fish Company 供應的 Poke                  Ramen Nakamura，Waikiki 一間非常火的拉麪店，很正宗。                  Omakase，亦即『廚師發辦』。攝於位於茂宜基黑的 Koiso Sushi Bar。店面很小，我們去了之後才發覺一般需要預訂。但我們運氣很好剛好有一桌人沒來。來這裏就餐的似乎主要都是日本人，店主和師傅跟他們都是日本交流的。Omakase 大約要 $50-$60 一個人。                  Lahaina Fish Co.，食物其實一般。要推薦的是這裏的地理位置，如果能坐到比較靠近窗邊的位置能看着日落享用晚餐。                  在酒店做的牛扒。來之前害怕如網上所說害怕飲食成問題，所以專門定的 studio 式旅店，偶爾自己做一下還是健康又省錢的~        至此，夏威夷遊記系列暫告一段落。下一次估計就是去大島觀看火山國家公園了。   夏威夷遊記系列      夏威夷遊記（一） 瓦胡島   夏威夷遊記（二） 茂宜島 Road to Hana, Ahihi-Kinau, Lahaina   夏威夷遊記（三） 茂宜島 Molokini Snorkelling   夏威夷遊記（四） 吃吃喝喝  ","categories": ["travel"],
        "tags": ["poke","food","hawaii"],
        "url": "https://www.ivanxiao.com/2017/08/07/xia-wei-yi-you-ji-(si-)-chi-chi-he-he",
        "teaser": "http://res.cloudinary.com/maomao/image/upload/v1502065880/blog/hawaii/poke.jpg"
      },{
        "title": "Rime 增強包",
        "excerpt":"鼠鬚管是我在 Mac 上使用了很久的中文輸入法，非常好用。     使用了大概一年後，主要的 complain 是字庫稍微小了點，上網搜了一下，發覺已經有人準備好詞庫了，值得推薦。  ","categories": ["software"],
        "tags": ["efficiency tips"],
        "url": "https://www.ivanxiao.com/2017/08/29/rime-zeng-qiang-bao",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "爲 blog 增加 linklog 功能",
        "excerpt":"最近更新了一下鼠鬚管的配置，本來是希望寫一篇 linklog 作爲推薦的。但是發現竟然不支持！！！而官方文檔明明有提到這個功能的……     簡單來說，octopress 的作者自暴自棄，停止了它的開發。所謂穩定版其實還停留在很早之前的版本，而之後的 2.x, 3.x 版本，也停滯了，包括 linklog 這個功能。   所以，只能暫時手動把 link 提供在此了：http://tieba.baidu.com/p/4125987751/。這個鏈接提供了網友自製的鼠鬚管詞庫，用上後準確率提高了很多。   增加 linklog support  花了一晚時間參考了一些文章，大致是把這個功能給加到 theme 里了。大致步驟：      安裝 linkblog   基於 octopress 沒有 merge 進 master 的 diff， 增加 support   Daring Fireball 作爲基準，理論上可以達到它的效果。   有意思的是，這個功能大概體現爲：      在 index 頁面，title 渲染爲 &lt;title&gt; + &lt;marker&gt;，其中 title 指向引用的文章，而 marker 指向本地文章的 permanent link。            這個 marker 的設計很有意思，下一步爲加上 hover text，提示指向的地方。           在 page 頁面， title 渲染爲 &lt;title&gt;，title 指向引用的文章   增加了 linklog 頁面，只顯示類型爲 linklog 的文章。  ","categories": ["octopress"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2017/09/03/wei-blog-zeng-jia-linklog-gong-neng",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "Recommendation System Review",
        "excerpt":"A pretty good review for recsys. Goes over content-based and collaborative filtering (item-based and model-based) approach.  ","categories": ["machine learning"],
        "tags": ["recommendation system"],
        "url": "https://www.ivanxiao.com/2017/09/04/recommendation-system-review",
        "teaser": "https://www.ivanxiao.com/images/post_blogging.jpg"
      },{
        "title": "States and Countries I've Visited",
        "excerpt":"Keep track of the places I’ve been too :)     States I’ve visited                  Countries I’ve Visited      ","categories": ["travel"],
        "tags": [],
        "url": "https://www.ivanxiao.com/2017/11/26/states-and-countries-ive-visited",
        "teaser": "https://www.ivanxiao.com/images/post_hit_the_road.jpg"
      },{
        "title": "Backdoor Roth IRA and Mega Backdoor Roth IRA",
        "excerpt":"A while ago I came across “Mega Backdoor Roth IRA”, which is a good way to place fund with tax-efficiency. I read a few documents and found that even though there are lots of good materials on it in English, not quite so in Chinese. So I want to write down my notes and share the knowledge here.        背景知识：美国税务与退休金制度      美国的个人所得税包括联邦税，州税，社会保险，联邦医疗保险（Medicare） 以及 联邦医疗补助（Medicaid）。而最重要的 federal tax 是累进的。   美国常见的退休金账户分为 401(k) 与 IRA，其中 401(k) 是公司提供的账户，IRA 是个人申请的账户。   401(k) 又分为 Traditional 401(k) 与 Roth 401(k)：            Traditional 401(k) 是所谓的推迟收税账户 (tax-deferred account)，即存入这个账户的钱当时不需要交税，而退休取出来的时候需要收税，但是你可以控制每个月提取的金额。可想而知，这里有两个因素决定你究竟延迟交税是省了还是亏了：1是你每个月取多少，2是未来的税阶会变成怎样。理论上来说，如果你控制每个月提取额度并且未来税是逐渐降低的，那么你则是省了税。另外，放在账户里的钱可以用来投资，但投资的收入一样需要交税。       Roth 401(k) 存入的时候需要交税，也就是说存的是税后的钱。因为交过税了，所以取出来时不需要再交。主要好处是这个账户的投资收入也是免税的！因此如果投资合理，盈利是很可观的。           类似地，IRA 与分为 Traditional IRA 以及 Roth IRA。   每年个人存进 401(k)的资金有上限，2018年是 $18500。一般公司会匹配一定的程度，但个人与匹配加起来最高不超过一定限额，2018是 $55000。   类似地， IRA的上限是 $5500。但是，当你的收入超过一定程度，则会受到一定限制：            Traditional IRA: 当收入超过限额，则免税额递减至0。但是你仍然可以存税后的钱到这个账户——这是后文要提到的 backdoor IRA 的基础。       Roth IRA: 当收入超过限额，则可存资金递减至0。           Backdoor Roth IRA   对于一般雇员来说，一个好的策略是至少存一定量的 401(k)，使得能拿到公司匹配的最大限额（相当于免费的钱）。另外，很多人其实是会存至上限 (max out) ，因为 traditional 和 roth 的优惠都好。可是对于收入很高的人来说，他们没办法免税地存钱到 IRA，也不能存到 Roth IRA，有什么办法可以做到呢？那就是所谓的 backdoor IRA。   虽然我们没法免税存钱到 IRA，但是我们仍然可以存税后的钱到 IRA。资金到位后，我们可以把这笔钱从 traditional IRA 转到 Roth IRA 账户。这样的好处是显而易见的：本来这些税后的钱在 IRA 账户投资收入是未来要交税的，转为 Roth 后，不用交税了！由于 $5500 的上限，用这种方法最多可以 backdoor $5500 到 Roth IRA。虽然这个方法被称为 “backdoor”，但它完全是合法的。   更细节的操作方法，请参考这篇文章。   Mega Backdoor Roth IRA   那么我们有没办法超越 $5500 的上限？答案是肯定的！使用类似 backdoor IRA 的方法，我们可以存高至 $35000 的钱到 Roth IRA。因为这个钱几乎是 backdoor 的六倍，因此大家也叫它为 Mega backdoor Roth IRA。当然，这个方法需要有一定前提条件：公司的 401(k) 必须支持这个操作。   具体方法跟 backdoor 很类似，我们先 max out limit，假设是 2018 的 $18500，并且公司给 match 了 $6000，那么你还能再存 $55000-$18500-$6000=$30500 的税后资金到 401(k)。这笔钱有两种转换方法：      In plan conversion，直接转换为 Roth 401(k)   Roll over to Roth IRA，如果 broker 支持的话，可以让它们把这笔 Roth 401(k) 转存到 Roth IRA。   具体的限制也请参考文章。   Tax-efficient fund placement   使用以上两种办法，可以有如下的退休金储蓄计划：      Max out 401(k) （税前 $18500），或者 Max out Roth 401(k) （税后 $18500）   Max out backdoor Roth IRA - $5500   Max out Mega backdoor Roth IRA   其中第一步究竟是哪种要见仁见智。但是一般的说法是，如果处于事业上升期，那么应该优先 max out Roth，因为现阶段的税阶并不是职业生涯里最高的，并且早期的投资可以有更高的复利，而这些复利最终又是免税的。   ","categories": ["personal finance"],
        "tags": ["notes","tax"],
        "url": "https://www.ivanxiao.com/2018/01/19/backdoor-roth-ira-and-mega-backdoor-roth-ira",
        "teaser": "https://www.ivanxiao.com/images/post_coins.jpg"
      },{
        "title": "Write Blog Post with Dropbox Paper",
        "excerpt":"My new workflow is using Dropbox Paper so that I can easily write in WYSIWYG style (rich text), and easily export as Markdown. Very convenient.     My previous blogpost was written using this method. Steps as below.   1. Write your post     2. Export as markdown        3. Copy paste the downloaded markdown to your blog system   After clicking ‘Download’, a md file will be downloaded. In my case, I use Octopress. You can easily fix up the metadata and preview. What is super-nice is that the image is automatically uploaded to cloud and you can use it as is, unless you want to upload it to your own host. Of course, this is also a disadvantage, you will need to manually upload those photos. I haven’t figured out a way to automate this yet.   ","categories": ["blogging"],
        "tags": ["tips","dropbox paper"],
        "url": "https://www.ivanxiao.com/2018/01/21/write-blog-post-with-dropbox-paper",
        "teaser": "https://www.ivanxiao.com/images/post_blog_with_dropbox_paper.jpg"
      },{
        "title": "2018年在Fremont买房是否时机合适之我见",
        "excerpt":"本文首发于知乎，作为《2018年在Fremont买房是否时机合适，与在湾区桥对面买房有何优劣？》的回答。手动搬运回博客。仅代表本人拙见，仅供参考。     刚好最近在看，过来分享一下心得。   先说第一个问题的个人观点：合适，全湾区哪里买房都合适。看看 2017 年的美国科技股，涨得不要不要的。2018 年我还会继续看涨。不介意学区的话，Newark 倒是一个不错的地方，靠近 880，有大华和 Newpark mall，生活还是很便利的。房子相对也很便宜，120w 预算基本上能住得像个皇帝~   再说第二个问题，Fremont 买房的主要优点是房价便宜，相对安全，有 Bart，以及对于在 fb 上班的人来说交通算是便利。但是，相应地也要考虑几大问题：自然灾害，学区，交通，工作机会，生活环境，保值与升值。   自然灾害   先说自然灾害，之前一场4.3级地震把大家都吓到了。Fremont 自古以来都是大家心中认为的“穷乡僻壤”，“自然灾害区“（不要说我 Fremont 黑，我也住在这，毕竟是穷人呐），不是没有来由的，因为的确这边蛮多问题的。现在先看好做好心理准备了，买房时看到 Natural Hazard Disclosure不会被吓尿。。这些资讯网上都能看到，比如 usgs 网站预测，未来几十年，湾区发生 6.7 级以上地震的概率是 72%，而东湾首当其冲，Hayward Fault 和 Calaveras Fault 直接穿过。。。      source: https://pubs.usgs.gov/ 再来看看别的，Home Page - Cal MyHazards，有包括地震，火灾，洪水，海啸等的区域预测 。基本上大半个湾区都处于地震液化带：      Fremont 看起来比较安全的是 880 往北，94536 那一块 (brookvale)，以及 central Fremont。但是，那里又直接被地震带穿过了…… 有一个说法是，湾区的房子基本上抗震的级别都很高，所以其实怕的不是地震，而是怕土壤液化。随便放狗搜一下有大量相关讨论，比如这个。Flood risk 也是稍微要注意的，幸好看起来只有 Ardenwood, Newark 那里稍微有点。   从自然灾害层面来说，南湾，特别是 82 / El camino Real 往南的一块，的确是有优势的。基本上避开了大部分自然灾害（当然也不能太南了，有个 San Andreas Fault）。传统好区，Palo Alto, Los Altos, MTV 94040, Sunnyvale 94087, Cupertino 等，都在这一块。      学区   再说说学区，Fremont 的 MSJ 可是很厉害的，在某些评分里可能比 Gunn High, Palo Alto High 什么的还要高。但是那边离 F, G, A, N, L，都很远。Ardenwood 也是大家趋之若鹜的地方，但是据说小学 overflow 得厉害，基本上住在那也要靠运气才能上到；而且大家都抱怨被印度人支配了 （所谓的小孟买..）。退而求其次的，可以考虑 Brookvale, Irvington 那一块。有 American High / Irvington High 做支撑。   为什么要考虑学区？即使没有小孩，这也是很重要的一点，意义在于学区基本上是跟社区安全，增值潜力挂钩的。有学区的地方不怕将来卖不出好价钱。华人向来注重教育，所以湾区几大名校总是挤满了老中，房价也是节节升高。   交通   交通的话东湾有 pros and cons。先说好处：Fremont 的位置很中庸，比较少互联网公司在这边，离哪里都不近，换句话说，离哪里都不远。因为有 Bart，所以对于有在城里上班的双职工其实位置很不错。84 / Dumbarton bridge 是个麻烦。不堵车其实很快，15-20分钟内就能到 fb 了。但是由于桥以及 willow，84交界是个瓶颈。平时一不小心就堵到收费站了。平时早上8-10am上班，基本要花个 30-40min。比如随手查的明天 8:30am 出发看起来是这样子。到 Google 就更远了，要 50-60min。      别小看这10分钟的差别，每天来回多个 20分钟，一年 52 周每周 5 个工作日，就 5200 分钟接近90个小时了。个人感觉是上班心情是随通勤时间指数下降的……   最坑爹是这桥上只有三个 lane，没有路肩，经常有车在上面抛锚，直接就使得桥上少了一个 lane，晚上 7pm, 8pm 也照赌不误，很是闹心。随着越来越多人才涌入湾区，买不起南湾的人会被 price out 到东湾，交通问题恐怕只会更赌。   工作机会   东湾对于科技人才来说，呃，几乎是没有什么就业机会吧，在 Paseo Padre 上面的科技园有些创业公司。Redfin 发布了一个 opporturnity score，定义是在步行30分钟内能到达的工作数量与种类的丰富程度，湾区看起来是这样的：      其中，刚过桥那一块，基本上就是住宅区了，opportunity score 巨低。我们可以看到，在 Palo Alto，Mountain View，一直延伸到 San Jose 那一块，101 与 280之间，基本上都是 80-90 的分数，这就是传统意义上的硅谷所在地了。从这方面来看，的确东湾的升值潜力会比南湾低一点。   生活环境   前面说的几点 Fremont 几乎没占到优势，但是生活环境倒算是有其独到之处。虽然不如 PA, MTV downtown 的 up and coming，但这边有很多很不错的中餐馆，也有不少的华人超市，包括大华。 Newark 的 plaza，Fremont 的 automall parkway，都是华人周末聚集之地。不过可惜，除了吃的之外，别的可能就差点。这边感觉主要年龄层次是 family，所以没有太多年轻人可以玩的地方。所以不少年轻人都喜欢在 MTV, PA 租房住，吃喝玩乐都比较丰富。   社区安全，低犯罪率也是大家津津乐道的一点。Trulia 上关于可以看到 Fremont crime rate 比较低：      CrimeReports: 南湾人民生活在水深火热之中。。。      虽然数据看起来是这样子，但不知道为什么老是有听说 Fremont 被抢劫什么的，比如：      保值与升值   关于这点，大家谈论得比较多的应该就是看几次经济危机后，不同地区房屋降价以及恢复的水平吧。这个数据从 redfin，zillow 和 trulia 都能找到。redfin 的数据是用 tableu 做的，但是数据只有几年，不是很能看出长期的trend。zillow 的数据也有同样问题。我这里用 trulia 来讨论，因为它的数据似乎最齐，虽然没办法把几个city一起看。先贴几个 median sales price 的数据：                     我们可以看到，Mountain View，Palo Alto, Cupertino 这几个 city 基本上在08经济危机也是没怎么跌的。Sunnyvale, Santa Clara, Fremont 基本上跌到了 02，03年的水平。但是 Sunnyvale, Santa Clara 这几年猛地上涨，在经济危机前大家差不多都在 700k 左右的 median，Sunnyvale 在两年前就突破了 1m 的坎，Santa Clara 也在去年就过了。而 Fremont 还差一点，应该今年会达到。   我们再来玩玩 redfin 的数据吧。在过去五年，基本上湾区的房子都在稳健地增长。相对来说，Fremont应该是涨幅比较小的。当然，用城市来看只是很笼统的看法，具体应该看到每个 neighborhood，比如 Ardenwood 和 Brookvale 就卖得挺好的。好区都涨得比较厉害。跟开篇说的结论呼应一下，只要没有黑天鹅事件，湾区房市算是一个不错的投资。      总的来说，还是印证了一个观点：买力所能及的房子。洋洋洒洒说了一大堆，希望能帮助到楼主。也希望无论如何买到哪里，正如大家所说的，回头看都是 deal :)   1/25 更新： 补一个有意思的数据：https://demographics.virginia.edu/DotMap/index.html 这个图显示了人种分布，不过亚裔没有“细分”，所以看不出是老中还是老印。从图可以看出，Fremont, Milpitas, Sunnyvale, Cupertino 都是亚洲人聚居地（其中 Cupertino 据说是台湾人的天下）。      ","categories": ["personal finance"],
        "tags": ["house buying","investing","mortgage"],
        "url": "https://www.ivanxiao.com/2018/05/02/2018nian-zai-fremontmai-fang-shi-fou-shi-ji-he-gua-zhi-wo-jian",
        "teaser": "https://www.ivanxiao.com/images/post_fremont_house_buying.jpg"
      },{
        "title": "自建代理访问中国音乐服务",
        "excerpt":"由于版权原因，在外国访问中国音乐服务即使付费会员也可能没法听到音乐。同时，中国音乐服务版权现状复杂，使得音乐版权分布在不同的服务之间，使得用户没办法在一个平台上听到所有喜欢的歌曲，体验很差。本文介绍一个项目的配置来解除一些限制。     工作原理     使用 UnblockNeteaseMusic 建立代理，可以运行在本地或者云端（文本选择 host 在阿里云）            使用云端的好处是对于 iOS 可以设置自动代理，不需要自己设置一台本地机器作为服务器           使用 proxy.pac, proxifier 设置 iOS 以及 Mac 的代理   申请云服务器 VPS   使用腾讯云以及阿里云等都可以。似乎阿里云稍微便宜一点，因为我配置的 VPS 完全就是为了这个而服务，所以我申请的是最低级的一档，并且按照网络流量付费带宽（非固定带宽）。那么一个月大概是 24.1 元 + 流量：      欢迎使用我的购买链接进行注册。也欢迎留言让我知道哪里有更便宜的虚拟主机。   登录 VPS 以及配置 UnblockNeteaseMusic   如何配置 VPS 以及登录这里就略过不谈了，应该在云服务器处都有教程。一般来说登录后要先安装一些必要的软件，比如 git, nodejs 等。   准备就绪后，安装 UnblockNeteaseMusic。   git clone https://github.com/nondanee/UnblockNeteaseMusic.git   安装好后，可以按照项目说明运行服务器。我使用的是旧方法：   node app.js -p 8000:8043   其中 : 里面，port1 是提供给客户端的端口，port2 用于内部转发，应该随便一个都可以。   注意默认状态下阿里云的 8000 端口是关闭的，需要在阿里云的安全组里自行设置。这是一个例子：      Bonus：我一般在后台使用 mosh + tmux 对远程机器进行操控，免去每次登录。有兴趣的可以搜索如何配置。经观察，这似乎并不占用带宽并引起流量消费。   配置客户端   iOS   iOS 的客户端类似 shadowrocket 等要钱，最简单的就是设置 proxy.pac 并且把相关服务 IP 重定向到服务器。下面是一个例子：   function FindProxyForURL(t) {   return shExpMatch(t, \"http://*.music.163.com/*\")   || shExpMatch(url, \"http://*.126.net/*\")   || shExpMatch(url, \"http://ipservice.163.com/*\")       ? \"PROXY &lt;VPS IP&gt;:8000\"       : \"DIRECT\" }   需要把  设置成你的服务器的公网地址。把这段代码存成 proxy.pac。   你可以把这个文件放在 VPS 上，我选择了把它放在了 dropbox。所以我在 iOS 上填的是类似以下的地址：   https://www.dropbox.com/s/abcdefg/proxy.pac?dl=0   记得要把 dl=0 改成 dl=1，否则会定向到 dropbox 文件的下载页面，而不是文件本身。   Mac Mac 的设置参考了这篇文章。这里使用了 proxifier 这个软件。设置很简单，只需要设置对应的服务器，以及规则即可。         测试   使用网易云音乐，查看一些平时看不到的灰色的歌曲，如果代理设置成功，可以看到服务器的输出，类似如下：      参考文献   [1] https://www.sheyilin.com/2019/06/unblockneteasemusic/   ","categories": ["network security"],
        "tags": ["netease music"],
        "url": "https://www.ivanxiao.com/2019/08/10/zi-jian-dai-li-fang-wen-zhong-guo-yin-le-fu-wu",
        "teaser": "https://www.ivanxiao.com/images/post_china_proxy.jpg"
      },{
        "title": "Http 与 Https 代理区别 / Proxifier 与 Charles 使用心得小结",
        "excerpt":"HTTP Proxy 与 HTTPS Proxy 的区别很容易混淆。本文通过使用 Proxifier 与 Charles 实战作为一个笔记备忘。     HTTP Proxy 与 HTTPS Proxy 的区别   如何使用 proxifier 处理以 https 开头的 url，也就是 SSL 连接？这种连接一般是访问 443 端口，而普通 http 连接使用的是 80 端口。摘自 Proxifier 官网:      It is a common misconception to confuse HTTP proxy and HTTPS proxy. HTTP proxy servers can process HTTP connections (port 80). They can also support HTTPS connections (SSL) but usually such connections are only allowed on port 443 (the standard port for HTTPS). For example this is the default configuration for Squid and Microsoft ISA proxy servers. If an HTTP proxy allows HTTPS connections on arbitrary ports, it can be called HTTPS proxy server (also called CONNECT or SSL proxy). In this case it can be used for generic TCP connections like SOCKS v4/5 proxy. Proxifier can work with HTTP proxy servers that do not support HTTPS on arbitrary ports. Due to the technical limitation of this protocol it is only possible to process HTTP connections with such proxy servers. This means that you must configure the Proxification Rules accordingly.    基本上的意思就是呢，广义的 HTTP server 是可以处理 HTTPS 连接的。但是大多数 HTTP proxy servers 只能处理 HTTP request。如果它能处理任意端口的 HTTPS 连接，那么它也能被称作 HTTPS proxy server。换句话说，HTTPS proxy server 其实也就是 HTTP proxy server 的一种。   所以呢：      After that you will be able to add HTTP proxy server just like any other type of proxies. Once HTTP proxy server is added, make sure that you properly set the Proxification Rules. If you want to process HTTPS connections through this proxy also, you should add this proxy separately as HTTPS.    也就是说当要使用一个 proxy server 来同时处理 http 与 https 的连接时，应该把它们当作单独的 server 添加，如下：      同时，forwarding rules 也要添加两条：      使用 Charles 的同时使用 http/https proxy server   有时我们不希望设置一个全局的代理，而是希望针对某些 host:port 才使用代理。但是很多网站检测 ip 的功能可能放了在别的 host，所以 rule 里面要 match 好几个 host。这时，使用 charles 来监听访问一个网站时的 traffic 就特别方便，能轻易看到这个 request 会访问哪些 IP，然后可以逐个尝试把这些 IP 添加到 rule 里。但是 charles 本身就是一个（本地）代理。如何使用 Charles 时并且设置 external proxy server 呢？   这里比较复杂，要同时设置几个地方。首先，简单起见本地的 proxy server 直接设为 SOCKS:      其次，要支持 SSL 连接，必须设置 SSL proxy，简单起见，假设对于任意 host 任意端口都启用设置：      但是比较现代的浏览器会检测到 charles 发过来的根证书并不是要访问的网站的根证书，所以一般会提示并且出错，这是因为 (via)：      Charles can be used as a man-in-the-middle HTTPS proxy, enabling you to view in plain text the communication between web browser and SSL web server. Charles does this by becoming a man-in-the-middle. Instead of your browser seeing the server’s certificate, Charles dynamically generates a certificate for the server and signs it with its own root certificate (the Charles CA Certificate). Charles receives the server’s certificate, while your browser receives Charles’s certificate. Therefore you will see a security warning, indicating that the root authority is not trusted. If you add the Charles CA Certificate to your trusted certificates you will no longer see any warnings – see below for how to do this. Charles still communicates via SSL to the web server. The communication is SSL (encrypted) from web browser to Charles and also SSL (encrypted) from Charles to the web server.       Charles uses its own Root SSL certificate for SSL requests through Charles to hosts enabled for SSL Proxying. The Root certificate is generated automatically for each Charles installation. Because Charles has signed the Root certificate itself, it won’t be trusted by your browsers or applications. In order to use the SSL Proxying feature in Charles you therefore need to add the Root certificate for your copy of Charles to the trust-store on your OS, and perhaps in your browser. Use the options in the SSL submenu in the Help menu in Charles to help install the Root certificate. You can install the certificate on the current OS, or on remote devices or browsers. To install the certificate in Mozilla Firefox, first configure Firefox to use Charles as its proxy then browse to chls.pro/ssl.    参见这里设置添加信任 Charles 的根证书。   最后，我们要设置对应的 HTTP 以及 HTTPS server，原理类似 proxifier：      PS. 这位博主有一系列非常好的介绍文章，并且使用 nodejs 实现了 HTTP 与 HTTPS 代理。强烈推荐。   ","categories": ["network security"],
        "tags": ["notes","charles","proxy"],
        "url": "https://www.ivanxiao.com/2019/09/07/http-yu-https-dai-li-qu-bie-slash-proxifier-yu-charles-shi-yong-xin-de-xiao-jie",
        "teaser": "https://www.ivanxiao.com/images/post_http_proxy.jpg"
      },{
        "title": "推荐一个好用的理财网站: Personal Capital",
        "excerpt":"从小家里就给我灌输记账的习惯，一直没有养成，因为懒…… 直到来了美国之后，这个问题神奇地被解决了——因为信用卡普及光并且使用方便，基本上 eliminate 了我使用现金的需求（大多数都是去中国餐馆）。结合理财软件 mint，所有的消费一目了然。可惜自从 mint 被 intuit 收购之后，一直处于维护状态，很多功能停滞不前，很多时候也连不上银行。最近半年开始试用 Personal Capital，发觉非常好用。它更像是一个财富管理软件而不是记账软件。随着资产的积累，我发觉我不再经常 review 我的消费，更多的是管理我的资产，包括各个银行帐号，401k，IRA，brokerage，车贷房贷等。此篇就来介绍一下它以及自己的使用心得。     如果你想直接开始试用，推荐使用我的推广连接进行注册，这样双方都能得到 $20 的奖励，也算是对我这篇介绍的一点支持 🙂   Personal Capital 可以做些什么   大体来说，Personal Capital （以下简称 PC）有三个主要功能。一是财富管理，简单来说跟 mint 很像，就是可以让你 link 你所有的银行、投资账户，然后可以让你清晰地看到个人的 net worth 是多少，包括资产与负债；另外一个功能是，当它有了你这么多资产信息后，它为进行分析并且给你理财建议，甚至可以提供免费的真人咨询。最后，它自己也提供投资业务，这里基本上就是跟 roboadvisor 很接近了，比如托管 1million 的资产，expense 是 0.87% 这样子（当然具体我没用没有了解过）。这篇文章主要介绍的是前两个功能。下面来具体说说。   Dashboard / 主界面     登录后，映入眼帘的是你的帐号结余，以及一些 time series 等。其实我主要看的是左边，也就是帐号结余。我最大的收获就是当我把所有的资产负债加进去后，我大概知道我个人的资产、负债，净值 (net worth) 是多少。 用 mint 也能做到，但是不知道为什么它把这个放在所有帐号之后…… 而 Personal capital 左上角就是，很醒目。   值得注意的是对于每个账户，你能看到一个 time series，这里反应了你那个帐号的资产（负债）数目。用了一段时间后，积累了足够数据后，看着自己的净值噌蹭地往上涨还蛮有成就感的……   另外，mint 连接银行帐号总是有这种那种问题，而 personal capital 整个过程都很 smooth，也很智能地给我分类成现金、投资、credit等。很明显 mint 这么多年来从没改变过，用户体验一直很一般，白白地失去（或迟早失去）了自己江湖老大的地位。至少我作为一个大概8年的老用户，didn’t look back。   Budgeting / 预算分析，cashflow / 现金流   这个功能 mint 相对来说强大点，可以设置不同的预算目标，我之前也用了一段时间，可以看大概每个月开销。Personal Capital 只提供了最基本的一个 goal，所以用得不是特别多。      对于现金流，对于单一收入的职工家庭来说似乎意义也不大（哎）。      Portfolio analysis 投资组合分析   好了，上面这些基本上就是跟 mint 重合很大的“记账”功能了。如果只是使用它们，可能的确不会感觉区别特别大。但是当你有多个账户，特别是投资账户后，它的投资分析功能便显得特别强大了。   Portfolio Holdings会算出你所有资产组合的增值率，并且跟不同的 index 相比：      这个功能特别有意思，因为我的确有很多不同的投资账户，比如股票账户，401k，IRA，甚至是不同银行的 CD 等，涵盖了 schwab, fidelity, Vanguard, wealthfront, Ally 等。它们每一个都只能看到自己的盈利而没有一个 big picture。当你把所有帐号 aggregate 到 personal capital 后，自己的 portfolio performance 如何一目了然（题外话，过去一年 US bond 都涨疯了）。   另外一个有意思的 view 是 Allocation。这里可以看到自己的资产都放在了什么地方。      这里面可以一直点进去细分，看到具体是哪些账户的钱放了在里面。   Investment Checkup / 投资检查   这个功能应该是对于没有阅读过太多投资知识的人来说非常强大的功能。基本上符合 Vanguard 的 lazy portfolio theory，也就是被动投资的始祖，Vanguard 创始人 John Bogle 大力提倡的投资理论。      首先，Personal capital 会根据你的年龄、risk tolerance 等为你生成一个 target allocation。然后，它会把你现在的资产配置进行一个比较，你就可以很直观地看到它们之间的 gap 在哪里。      同时比较 1992 年至今的两种 allocation 的回报看起来是怎样的：      Efficient fronier 可以看到 risk &amp; return 的比较，也就是高风险高回报。图中 X 点表示现在的分配，而绿点表示是推荐的分配。100% stock 对应的是黄点，可以看到回报很高风险也很高：      当然，说了这么多都是希望你能用它们的付费服务…… 如果你有闲心打理自己帐号的话，不难做到。其实很多文章都会提到，难在执行。比如基金定投，如何坚定地在市场上涨、下跌时都能做到淡定并且执行是很重要的。   Personal Capital 有很详细的 whitepaper 来介绍它们的投资策略。   Planning / 理财计划   Personal Capital 的理财计划是我非常喜欢的功能之一！在它的 planning section，有以下四项，都提供了非常强大的功能：      Retirement planner   Savings planner   Retirement fee analyzer   Investment checkup （上面已经单独拿出来讲过）   Retirement Planner / 退休计划 很多理财软件以及网站都有教导你如何做退休的规划，而 personal capital 把这一点做得非常细致。最让我称道的一点是，它支持多种 scenario （情景）。比如说，我可以设置不同的退休时间/去世时间，来看我的资产增值与消耗。我也可以模拟一个场景，比如我或者我配偶其中一个失业了，看起来会是怎样的。我觉得最符合大多数人现状的，可能是设置不抚养子女、抚养一个子女、两个子女等的情景。      我们来看看它的功能吧。根据你现有的资产以及消费，PC 会根据一些假设来模拟你退休后的资产价值。大概的策略是，PC 会根据你现有的现金流， X% 的资产增值 和 Y% 的方差来模拟 5000 次。这里面的 X 与 Y 会根据你现有的投资策略来大概算出。模拟完后，会显示出 median 以及 10th percentile 你的资产会剩余什么价值。   这里方便的是，如果你使用 PC 已经一年以上了，那么它还会根据你的收支给你一些预设的值（比如现金流等），省去了一些你自己做功课的时间。   同时，你也可以很方便地加入自己的计算来设置一些收入、消费等事件。比如，假设你 67 岁退休，根据工资水平，可以得到不同程度的社会保险。再如下面的例子就是设置了退休后每年花多少钱，花多少钱抚养小孩，教育，度假等。      Savings Planner / 储蓄计划 这个功能基本上是结合退休计划一起使用的。它可以告诉你截至今天为止一共存了多少钱，跟退休计划里面的目标关联；它也可以告诉你以90%的成功率达到你的退休目标你需要存多少钱。   Personal capital 安全吗   不得不承认这也许是很多人使用类似服务的一个很大的心理障碍，毕竟要跟各种金融机构、银行互联需要显式地登录、授权。   类似 PC 这种 financial aggregator，顾名思义，都会到你的各个银行抓取你的 transaction，才能获取你的财产信息。理论上如果银行开放类似于 api 的接口让这种第三方机构可以获得读权限（也就是能获取你的 transaction，而不能进行转账等操作），是比较安全的，毕竟你最大的损失也只是泄漏了你的信息而不是财产本身。不过，我暂时还没听说过有这种实践存在。所以，你不得不把你的帐号密码给 PC，而 PC 代你进行登录。跟你直接到银行网站登录是一个道理。   那么问题来了，PC 代你登录肯定是需要有你的密码明文的，这个没办法绕过去（again，除非银行有 API）。根据 PC 官网信息，它们自己并不存这个信息，而是通过一个第三方机构 Yodlee，来代为存取。所以这个安全信息相当于是 decouple 了，并且最主要的安全问题是在于 Yodlee 本身，因为理论上 PC 自己是完全没办法 access 到这些密码的——除非在用户连接账户时那一步就被截取了密码。包括 mint 在内的服务也是使用了 Yodlee。至于它有多安全，完全能展开再写一大段。网上不少参考文章都有介绍，大家可以去参考以决定是否完全信任这些服务。比如这篇文章谈到，”Yodlee has no security breaches to date.”      Benefit from our partnership with Yodlee, a financial technology industry veteran, to facilitate aggregation of your accounts. With over a decade of experience connecting with financial institutions, Yodlee provides an added layer of safety between your data and anyone who would want to access your account information. Your bank and brokerage credentials are only stored at Yodlee, not in Personal Capital’s database.    当然没有 security breach 不代表将来不会出现，而日常生活中正常的网站登录也伴随有一定的风险（比如电脑被装了恶意软件收集密码之类的）。所以对于一般人，做好功课了解 PC 的工作原理，做一个 informed decision，可能也就足够好了。   结语   结合我将近 10 年使用 mint 的经验，以及将近一年使用 PC 的经验，我认为 PC 还是非常值得一用的。它的 mobile app 做得也不错，而且它本身提供类似 wealthfront, betterment 的投资服务，说明是有自己的资金来源；比起半死不活的mint应该是更有前景。   如果你读完我的文章觉得有兴趣尝试，不妨使用我的推广连接进行注册，这样你我都能得到 $20 的奖励，也算是对我这篇介绍的一点支持 🙂   ","categories": ["personal finance"],
        "tags": ["personal capital","mint","investing"],
        "url": "https://www.ivanxiao.com/2020/05/26/tui-jian-yi-ge-hao-yong-de-li-cai-wang-zhan-personal-capital",
        "teaser": "https://www.ivanxiao.com/images/post_personal_capital.jpg"
      },{
        "title": "The White Coat Investor 读书笔记",
        "excerpt":" 最近开始阅读 The White Coat Investor。 之前在网上搜投资理财时已经知道这个作者，并且大致看过他的网站，印象不错。最近准备在 Books 上看一些书，刚好给我推荐了这本。总的来说，这本书算是一本投资理财的入门书，里面提到的东西基本上我都零散地学习过。如今能系统地学习，索性写下一些笔记来加深理解。     这本书最吸引我的是：      虽然这本书是为医生 (Physician) 职业量身订造，但其实里面的建议，基本上符合高收入人士的情况。在网上很多的文章基本上都是针对一般意义上的中产阶级而写，有很多东西存在局限性。比如说很多文章都会让你 每年 max out IRA or Roth IRA，但是对于高收入人群，如果你收入超过 $140,000，那么就不能享受 IRA 免税的优惠了。而这个收入，对于医生或者湾区科技从业人员来说，其实是很普遍的。而相对地，高收入人群应该利用 Backdoor IRA 的方法 —— 这一点我一直都知道并且有实践，但是也是经过了不少学习才总结出来的。这本书穿插了不少针对特定人群的建议，比一般网上的文章实用。   书里很系统地介绍了不同 life stage 应该做的事。虽然基本是针对 physician 的，但是 principles 是通用的。比如书里反复强调的，医生在 residency 时要节省，不要一有收入马上买大房子，付低于 20% 的首付，买一手豪车等等。这些建议其实基本上就是告诉我们要 live below your means。我个人最喜欢的一点是，早期收入低时，建议存 Roth 401(K)；后期收入高时，建议 Traditional 401(K)。针对这个有不同的观点，但基本上跟我的理念与感受吻合，good to have some confirmation here。   每章最后有提纲挈领的总结，并且有一些延伸的阅读。书本身很短，只有不到 200 页，但是提供了很清晰的索引让人可以自行寻找感兴趣的话题与资料，加深理解。   下面是一些感悟：      书中 Acknowledgements 提到 “Rick Ferri sat next to me in that class“。这给我阅读本书提供了很大的信心。Rick Ferri 是 Bogleheads 里面大家都比较推崇的一个 expert，他提出的 Three-fund, Core-Four portfolio 基本上是我自己的投资策略。   Saving money is easier than earning money - a dollar saved is worth a dollar, while a dollar earned is only worth about fifty cents due to taxes!   第三章            平时说的“百万富翁”，考虑到 inflation， 在今天已经不算什么。比如 1914年的 1 million，2013 年其实已经相当于 23 million 了。       4 % rule。退休后如果每年花费 4%，那么你的退休储蓄可以用到你去世那天为止。该项研究计算了 1926-2009 年 S&amp;P500 的受益，不同的  portfolio （比如 75 stocks / 25 bonds vs 25/75），在不同 withdrawl rate 下，能持续到退休后 15/20/25/30 年的成功率。基本上 4% 是一个分水岭，除非是特别保守 (100% bonds) 的情况。注意很多文章都提出 4% 可能过时了，要更保守才行。但我们的 take away 应该是，退休后的 withdrawl rate 不能是 6%, 8% 之类的。       按照 4% rule，如果退休只有 1 mil，那么每年税前收入也只有 $40,000 或更少。           第四-六章基本上说的就是，读 Medical school 有多贵，residency 收入比较低，要节省。总的来说就是开源节流。我觉得对于大多数普通家庭出身的中国人来说，都能自然地做到。   第七章            建议 save 20% income for retirement。我个人认为如果可以的话，要更多。当资产还比较少时，每年 compound 的速度增长一倍，是没有 savings rate 增长一倍攒的钱多的。       对投资增长的期望要保守，考虑到 tax，inflation，expense 后，实际能达到 3%-7% 就不错了。Rule of 72: interest rate / 72 = number of years it takes money to double。所以如果 7.2% 的话，需要 10 年才能 double。       很多文章提到退休后大概需要维持 80% pre-retirement income。但是对于高收入者，其实只需要 25% - 50%，因为他们的 income 基数大 🙂 具体来说：高收入者税收也高，退休后就会降下来；ideally，退休后房贷，子女教育等的 expense 也会降低，甚至可以搬到 living cost 比较低的州，downsize house 以获得较低的 property tax，以及只开一辆车。同时，social security 也会带来大概 $30000 的 income。书里给出了一个具体的 table 来比较 working / retired 的开支来证明为何退休后需要的收入会大幅减少。       顺带温习一下 Social Security：大致要求为 PR / Citizen 身份在美工作并交税，积攒 40 个 credit（每年最多4个，所以需要10年），即可享受社会福利。具体的 monthly pay 可以到 ssa 里用它的 calculator 来算，大概是收入最高的 15 年然后 up to a maximum。我们这一代人的正常退休年龄是 67 岁，但最早可以 62 岁开始拿 reduced benefit，相应地，推迟领取福利也可以增加 monthly pay，但最多推迟到 70 岁。可以 login 到 my SSN 查看已经积攒了多少 credit。           第八章：motorway to Dublin / 条条大路通罗马。这张基本上就是我最熟悉的内容，核心思想就是 low cost, diversified, passive investing。这点在 Bogleheads 的网站有最详细的介绍。   第九章介绍了非常规的一些投资手段，包括:            real estate. Pros: 多种办法增值，包括 appreciation, rent, tax breaks, amortization。Cons：需要花费大量时间精力去管理，对于高收入人群，一般他们都是“技工”类型，靠磨练自己手艺和出卖时间来赚钱。REIT 也许是更方便，风险更低的一种投资房地产的方法。       insurance. 基本上除了 term life 之外的保险，比如 whole life，都不划算。这些保险的条款都非常复杂。这篇和这篇文章说得都很不错。       private investment. 比如参与投资兴建某个 medical center 等。这些都需要花费大量时间去学习。       other investments. 比如 p2p, bitcoins, arts, options, currencies, precious metals 等。这些也是非常 risky 然后需要很好的 skill 和经验去支持。           同时，作者很深刻地指出，不应该把 Home 看作投资，而应该是消费，因为 utility，insurance，maintainence， tax 等其实很花钱。他建议如果不能支付 20% down for a 15-yr mortgage，就不要考虑买房；并且如果 mortgage 超过两倍 income，也是不建议买。这点在 2021 年对于湾区码农家里没矿的话估计很难，考虑到硅谷不太破的，交通尚可，不太破的 4b2b SFH，起码也得个 1.5M - 2M，20% down 也要贷个  1.2M - 1.6M，如果按照这个原则，对应每年收入应该是 600K - 800K，这个基本上只有大厂高阶双码工能比较容易达到。这么说起来，也许东湾的确是一个更适合的地方，估计 1M - 1.5M 就可以了，然后对应 400K - 600K 的收入，安全得多。Fremont 买房我写过一篇相关的文章。   第十章介绍的是花钱买 financial advice:            要找到一个好的 financial advisor 很难 —— 它们的入门门槛并不高，并且需要很多经验。最悲剧的是，如果你自己没有好的 financial sense，你很难知道他们的建议是不是有用的。而如果你自己已经有了这些知识，似乎也没有请他们的必要了。       如果真的要请的话，一定要找一个 fee only 的。这样可以保证他们没有 conflict of interest。比如有些 advisor 是抽成的，那么他们就会有 incentive 让你买他们推荐的股票，而这些股票、基金往往不一定是最好的。       并且，一定要请至少有某些 top tier certificate 的。CFA 最好 （但是其实也只需要一年学习跟一些工作经验），CFP 次之，还有 ChFC, JD, CAP, MBA 等。       Advisor 提供的服务有很多种，最常用的有 financial planning 和 asset management。建议分开这两种服务来购买，并且是 fee only 的。           第十一章讲的是 asset protection 这个话题。这个话题比起赚钱来说枯燥了点，但是总是需要做好保护自己的准备。            即使对于高收入人群，也不需要一个特别复杂的 plan。基本上作者总结出来的几点我觉得就特别好                    给 auto, home 买足够的 personal liability 保险，并且额外地买一个 1m-5m 的 umbrella。           给自己的房产上合适的 title。比如说是夫妻两人共同持有，那么如果一方破产，另外一方因为也拥有房产，那么讨债的无法直接把房产搞走。           存买退休账户，特别是 401k。大多数退休账户是有破产保护的。           根据不同州的法律，房产有一定程度是收到破产保护。因此可以根据州规来决定还房贷的速度从而达到保护。比如加州是 $75,000 的房产价值收到保护，夫妻双人是 $100,000。           尽量避免拥有危险的资产（比如越野车）。如果有，建议单独开一个公司，然后把这些资产放到名下。比如 rental property，出事了最多是那个公司破产，而不影响到个人财产。           婚前协议(!)                       UGMA/UTMA accounts。基本上就是给自己未成年子女开的账户并且把资产送给子女。那么这些资产理论上就不属于自己了，所以可以 protect from creditors，它还有一定的 tax benefits。但是坏处就是当子女成年，这些资产就没法控制了。因此理论上这些账户是给自己教育用的，但也不排除子女拿来做别的事。介绍。           第十二章也是相对陌生的话题：estate planning （遗产规划）。            遗产规划的主要意义在于合法避税以及避免 probate （遗嘱检验）。       对于大多数类似医生收入水平的人群来说，资产可能达不到遗产税起征点。由于 2018 年的 TCJA，2018-2025 的遗产税起征点很高。2021 年是 11.7 million / person，夫妻 double。但是法案过期后如果没更改会还原到 5 million / 10 million。其实如果厉害一点的人，也还是会达到。       州也有自己的遗产税规定。我查了一下，加州是不收遗产税的。       对于大多数人，资产应该都在 banking, investment, retirement accounts 里。如果设置了受益人，那么资产可以直接转到受益人名下而无需 probate。       避免遗产税最简单的方法就是——送人。每个家庭每年可以 gift $14,000 而不算入遗产税征收范围。所以一个方法是在老年时候开始把钱转移给子女（甚至是任何人）。       will 遗嘱。每个人都需要设立 will，意义是确保去世时财产可以依照自己意图到达合适的人手里。           第十三章是我很感兴趣的话题 —— 税，但是却写得很简短且浅显，有点失望。基本上只传达了一个观点：建议自己报税，虽然一开始会花一些时间，但是之后每年只需要学一些新东西，并且可以省下 CPA 的花费以及对可以合理避税的地方。更重要的是，让自己有 tax awareness，那么以后在做财务决定时会更有 sense。这点我很赞同，比如说我们公司每年有 $720 的 wellness benefit，大致的用途是购买 gym pass 等。很多人为了不浪费这笔钱可能会买一些不怎么用到的东西。但如果你知道这个是 taxable income，那么你没有非必要支出其实不使用它而避免交税是更明智的选择。   第十四章应该是码农没那么熟悉的话题 —— 选择一个好的商业结构，也就是 Sole proprietor, Individual Contributor, LLC, S corp, C corp 之类的。对于美国的医生来说，应该是一个比较容易接触到的话题，因为有大约一半的医生会选择开私人诊所，也就是相当于自己做生意，那么必然要选一个合适的 incorporation structure，来处理 payroll, tax, 以及 liability 的问题。自己开公司的好处不言而喻，有很多避税的方法。比如作者提到 internet，cell phone，computer 都可以当 business expense deduct。同时如果是 self-employed，可以存 Solo 401(k)，2020 年可以存 $57,000！这几种方式的主要区别在于怎么处理 liability 以及报税问题上。根据我自己的 study，自己做个 side kick，比如 rental property，consulting 之类的，LLC 理论上是比较好的。   十五和十六章没什么实质内容，只是鼓励大家只要按照书中原则去做，很大机率能得到一个好的结果，享受生活。总的来说是先苦后甜，把退休的资金都plan好了再买 fancy car~   总结：作为一本了解美国高收入人群财务管理规划的入门索引，是不错的书籍。  ","categories": ["personal finance"],
        "tags": ["notes"],
        "url": "https://www.ivanxiao.com/2021/01/12/the-white-coat-investor-du-shu-bi-ji",
        "teaser": "https://www.ivanxiao.com/images/post_white_coat_investor.jpg"
      },{
        "title": "在湾区工作一年大概能攒下来多少钱?",
        "excerpt":"本文首发于知乎，作为《在湾区工作一年大概能攒下来多少钱？》的回答。里面的意见仅代表本人，我不是 CPA 也不是律师，里面的建议不能作为任何报税、投资的客观指导。     从税的角度来说说吧。工作后每年都被报税震惊，于是前一段时间大概 plot 了一下看看在加州生活的有效税率 (effective tax rate, ETR) 究竟是多少。   Disclaimer: 非会计师/CPA，下面的只是粗略估计，没有任何指导意义~   Single   先来个单身狗情况，根据这个回答，工资的范围就假设到 600K 吧。这里敲黑板划个重点：这里只是设置一个范围让大家看看工资和税的增长是什么样的，没有实际意义，也就是一个 plot；另外也是可以让大家可以根据自己的工资水平参考一下将来收入的曲线。假设你都用的 standard deduction，欢迎评论或者私信我看看这个估算符不符合你自己的情况。 首先是联邦税，这个好办，累进税制。2018 大概长这样子：      具体例子就不举了。假设每年都存满 401k (美国一种养老金计划，存入部分除去不算 taxable income，2018年最多存 $18500），并且使用 standard deduction （每个人的免税额，2018 是 $12000），那么看起来是这样子的：      20万收入的话，大概 ETR 是 17% 这样子。 但是我们还有 FICA tax，其中包括：      social security tax 税率是 6.2%，工资收入直到 $128700 都要算税（封顶）   medicare 税率是 1.45%，不封顶！   你要交的 FICA 看起来是这样子的：      但是我们可是在万税的加州啊！上面联邦那一套累进税阶还要来一次。2018年看起来是这样的：      这里很复杂，还要根据联邦税的情况来做调节，这里就不做太精细的计算了，假设我们 take 一个 standard deduction， 2018年个人只有少得可怜的 $4401 免税额。 它对应的 ETR 是这样子的：      你以为这样子就完了？IRS 笑了。咱还有大招没出呢：      additional medicare，当个人收入超过 $200000 （2018年），多余部分盛惠 0.9% 的税。   net investment income tax (niit)，当个人收入超过 $200000 （2018年），min(多余部分,投资利得)盛惠 3.8% 的税。   在加州做码工的，大多数都有一定的投资收入，这里假设 1/10 的收入来自投资所得。 它们看起来是这样的：      （感谢 @潘龙·迦门楠  指正，我还漏了 AMT。由于 AMT 太复杂，这里就不专门列出了。不过 2018 税改后，AMT 门槛大幅提高，很多人都不 hit amt 了。根据这篇文章说法，hit amt 的人 减少了96%。） 所以这里总结一下，你每年要交的税包括有：      federal tax   California tax   FICA tax   additional medicare/niit   你最终的税率是这样的：      也就是说，如果你达到了 60w 的收入，相应地你也要交大概 26w 的税，大概是 43% 的税率。你的最终收入看起来是这样的：      这里终于点题了，大家可以 ballpark 一下不同收入水平，不吃不喝，大概能剩多少钱。比如10w 收入，能存下大概 7.5w 这样子； 20w 收入，那么剩下大概 14w 左右吧。当你收入翻倍到了 40w，你的钱包却没有翻倍，只有大概 24w，当你登上一个很多人都可能短时间无法达到的高度， 60w 收入时，大概能剩 34w 这样子。 假设你非常非常节省，住在非宇宙中心的 apt，那么月租应该能压缩到 $2000 左右；再假设每天三餐都吃公司，只有周末出去吃吃，两天下来大概 $100-$120，一个月算它 $500。每个季度简单出去玩一下， $1000 x 4 = $4000；那么一年花费大概在 $3.4w 左右。所以即使如果你收入只有 10w，那么狠狠对自己还是能攒下大概 $4w 左右，只需要大概 6 年时间就能存下 120w 房子的 20% 首付了！这样的房子估计在东湾和santa clara选择比较多，每天上班 40-60min。怎么样，感觉人生还是很有希望的罢……   Married Filing Jointly   这里就不多说了，基本流程跟上面差不多，主要是 assumption 变为：      两个人都 max out 401k, 两份的 Standard Deduction   加州州税也是两人份的 Standard Deduction   联邦税：      这里可以看到比起一个人收入 600K，两个人收入同样多的税率其实是有下降的，因为这样相当于平均每个人收入 300K，所以其实要跟单人 300K 比才公平；或者两人同时收入 600K，总收入 120 K 比才是一个 fair comparison。 这里就不得不引进一个概念：Marriage Penalty/Bonus。大概意思就是，当两人没结婚时报税，比起结婚共同报税时，在美国这个诡异的税务系统下，得到的税很可能是不同的。当两人收入不平均时，这个现象更明显。 首先祭出这张图，来自： https://taxfoundation.org/tax-cuts-and-jobs-act-marriage-penalty-marriage-bonus/      可以看到，假设如果两人收入均等，共同收入在20K，或者在1M这样，会有一定程度的 penalty，也就是交的税比没结婚单独交税要高；如果总收入在 100K，但是收入很不平均，比如一个人收入 90K，另一个人 10K，这样就对应于上图 (10%, 100K)的位置，共同报税是会降低税率的。直观解释就是因为低收入的那个人分摊了高收入人的收入，使得那部分本来应该在更高税阶扣税的钱降低了税阶扣税。具体请参阅上述文章，这里不再赘述。 再提一句，还有一些坑爹的是，比如有一个抵扣项目 SALT (state and local tax)，一个人报税上限是 10K，两个人也是 10K；additional medicare 也是这个鸟样，2018 一个人超过200K部分要算税，两个人却不是 400K，而是 250K 开始就算。类似地，niit 也是这样。 扯远了。回到之前说的例子。假设夫妻每人都收入 600K，那根据上图，虽然超出了范围，不过根据曲线 extrapolate 大概可以知道会 suffer 一定的 penalty。如下：ETR 大概 30.07%，比起单人 29.4% 是高一点。(Again，不是 cpa，没有经过严谨的 review，不保证准确度)。      扯远了，继续贴。ca tax:      FICA 的话，上限就是单人上限的两倍：      两个surtax：      最后所有加起来：      net income 曲线：      ","categories": ["personal finance"],
        "tags": ["zhihu"],
        "url": "https://www.ivanxiao.com/2021/01/15/zai-wan-qu-gong-zuo-yi-nian-da-gai-neng-zan-xia-lai-duo-shao-qian",
        "teaser": "https://www.ivanxiao.com/images/Silicon_Valley_-_Google_Maps.png"
      },{
        "title": "Goodbye Octopress. Hello Jekyll.",
        "excerpt":"I have been using Octopress for blogging since 2013. Octopress is essentially a framework built on top of Jekyll, which offers vast convenient at the time, such as nice themes, automating in publishing to github, etc. I considered Jekyll a bit bare-bone and heavy back then. However, the ever-promised Octopress 3.0 is never coming officially, nor there is a friendly migration guide. The octopress github repo has stalled since 2015.  Each time I wrote a blog post, there’s always some plugin or things break as I am using ancient age gems. It’s a time for a change. My new site is hosted at here.     After doing some study, I’ve decided to migrate my blog to Jekyll, because:      Octopress is built on top of jekyll anyways. The migration is supposed to be a lot easier. Moreover, my blogging flow can more or less be preserved.   I’ve originally thought of migrating to a framework written in Python, which I am more familiar with. However, I didn’t really find a better choice (please let me know otherwise in the comments).   Jekyll has been actively maintained and has been integrated well with a lot of things, especially github pages, the host that I’ve been using so far.   The Migration   Since migration from octopress is not officially supported by jekyll, I spent some time working around the migration. Rough steps I’ve taken:         Create a jekyll site following the instructions.   Configure custom domain.   Install themes. I used so-simple.   Build and fix the errors. Most of them come from the plugins I had. For backward compatibility, I deprecated them gradually until my build passes. I am sure there are still some of them are causing error. But who cares for now 🙂   Configure the theme. This took the most time. A lot of new things to learn and include.   Use plugins such as jekyll-compose to help the publishing flow, e.g., create a new post and automatically open up my editor.   There are a few things that I didn’t solve quite nicely and on-going:      I attempted to migrate my old theme foxslide. But it’s way too time consuming to do it. I ended up just using the jekyll gem theme, which is nice as I don’t have to copy-paste the source to my own repo.   However, the so-simple theme itself poses some problem for me. The latest version as of writing is v3.2.0, which is released on Nov 5, 2019. There seems to be quite some new development going on and some nice improvements such as MathJax v3. Without a new release it seems no easy way to install with a breeze (as a gem).   I used to use “$” for MathJax’s inline math. However, I realized there are a lot of time I did need to use the dollar sign for what it’s for. I’ve decided to go back to use the recommended \\( ... \\) for inline math. But seems not working (rendering) properly. Even in the demo page of so-simple theme.   Separate the source and static files to a private and a public repo. I don’t like my markdown source to be easily copied in a public repo. I’ve always using a bitbucket repo to host the source privately (github did not offer free hosting at the time), and publish the generated sites to github. Octopress comes with some rake utils to handle the deploy and it handles it quite well. However, apparently I need to migrate that as well if I wish to continue to do that. Jekyll website offers some advices on automating the deployment process:            Apparently, GitHub actions are easiest and well-supported one. However, it has to publish to the same repo which means both source and site will be public.       Travis CI. This is a relatively “heavy” solution but it does work as what I wanted. With the help of this article, I was able to set up a Travis CI workflow to do. However, it does come with a cost: at the time of writing, the free plan only comes with 10,000 credits, and each time I deploy it will bill to it for the VM time usage. This means some time in the future, this will stop working for me. Meanwhile, CircleCI is offering 2500 free credits/week. I should probably explore this one.       Use Github pipelines. This articles introduces how to do it.              Conclusion   At the end, I was able to migrate my blog with the most part. There are still some weird or unexpected issues I need to address (e.g., images layout). I am definitely glad that this is doable within a relatively short time.  ","categories": ["blogging"],
        "tags": ["octopress","jekyll"],
        "url": "https://www.ivanxiao.com/2021/01/20/goodbye-octopress-hello-jekyll",
        "teaser": "https://www.ivanxiao.com/images/post_goodbye.jpg"
      },{
        "title": "A few gotchas in Kramdown with MathJax",
        "excerpt":"As I recently just migrated my blog to use jekyll + github pages, a lot of things have changed. Most notably, the markdown processor is kramdown by default. There are a few gotchas I ran into so I am writing down some notes here.      Pipe symbol \\(\\vert\\) does not work properly, as it is reserved for use in tables in the markdown syntax. Per the doc, you have to use \\vert. This is a bit annoying since \\(\\vert\\) has been used extensively in a lot of math, such as conditional probability, absolute value, and sets.   For both inline and display math, kramdown uses double dollar signs. When writing math in list, if the line contains only an inline math, it will be rendered as display math. My workaround is to add some wording to make it legit inline math. To make the MathJax default \\( ... \\) and \\[ ... \\] work, you have to escape it as \\\\( etc when writing markdown.   I’ve not yet encountered since I am still using MathJax v2, but I’ve seen quite a few places mentioned that it’s a hassle to upgrade from v2 to v3, and there are some hiccups with Kramdown. Fingers crossed it will get better soon.  ","categories": ["blogging"],
        "tags": ["mathjax","kramdown"],
        "url": "https://www.ivanxiao.com/2021/01/21/a-few-gotchas-in-kramdown-with-mathjax",
        "teaser": "https://www.ivanxiao.com/images/post_gotcha.jpg"
      },{
        "title": "Ray Dalio's thought on Bitcoin",
        "excerpt":"Ray Dalio shares what he thinks about BTC.   He thinks positively about bitcoin, as it is one of a kind invention. It is similar to gold that has limited supply.      I believe Bitcoin is one hell of an invention. To have invented a new type of money via a system that is programmed into a computer and that has worked for around 10 years and is rapidly gaining popularity as both a type of money and a storehold of wealth is an amazing accomplishment.       Because there aren’t many of these gold-like storehold of wealth assets that can be held in privacy and because the sizes of their markets are relatively small, there exists the possibility that Bitcoin and its competitors can fill that growing need.    However, he argues that the supply does not merely mean the quantity of bitcoin itself:      Although Bitcoin is limited in supply, digital currencies are not limited in supply because new ones have come along and will continue to come along to compete so the supply of Bitcoin-like assets should, and competition will, play a role in determining Bitcoin and other cryptocurrency prices.    The biggest risk is that how it plays well with goverment. Since it is unregulated like the government-backed currency, there is a risk that it’s privacy and use could be limited:      As an extension of Bitcoin being digital are the questions of how private it is and what the government will allow and not allow it to be   ","categories": ["personal finance"],
        "tags": ["bitcoin","Ray Dalio"],
        "url": "https://www.ivanxiao.com/2021/01/29/ray-dalio-s-thought-on-bitcoin",
        "teaser": "https://media-exp1.licdn.com/dms/image/C5612AQG3FuGzeVbOlg/article-cover_image-shrink_423_752/0/1611865595174?e=1617235200&v=beta&t=7kfL-jszrez4qg5ShKwrS419TAgQuSeYiWz-e3rHugc"
      }]
